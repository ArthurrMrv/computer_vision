{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V14 — Mixed Ensemble: DINO + CNN + EfficientNet\n",
    "\n",
    "**Goal:** maximize accuracy by training a diverse ensemble: 1 DINO, 1 CNN, and 2 EfficientNet models with optimized GPU batching.\n",
    "\n",
    "**Pipeline:**\n",
    "- Download HuggingFace dataset (`subinium/emojiimage-dataset`)\n",
    "- Map 11 vendor classes to 7 target classes\n",
    "- **Pre-train 1 CNN** (ConvNeXtV2-base) on HuggingFace dataset (seed=42)\n",
    "- **Pre-train 1 DINOv2** on HuggingFace dataset (seed=42)\n",
    "- **Pre-train 2 EfficientNet models** (b0 and b2) on HuggingFace dataset (seed=42)\n",
    "- Split target dataset train/val (stratified if possible)\n",
    "- **Fine-tune all models** on target dataset\n",
    "- **Final training** on combined train+val for CNN and EfficientNet models\n",
    "- For each image: compute deterministic TTA prob-vectors for **all models** + statistical features\n",
    "- Train **LightGBM** meta-model on top\n",
    "\n",
    "**Key Features:**\n",
    "- Diverse ensemble: DINO (transformer), CNN (ConvNeXtV2), EfficientNet (b0, b2)\n",
    "- Improved classifier heads (deeper for CNN/EfficientNet)\n",
    "- Model-specific learning rates and schedulers\n",
    "- Optimized feature extraction (pre-loading, single augmentation generation)\n",
    "- Final training on combined data\n",
    "- Better LightGBM hyperparameters\n",
    "\n",
    "**Classes:** apple, google, whatsapp, facebook, samsung, mozilla, messenger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.9.1)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.24.1)\n",
      "Requirement already satisfied: torchaudio>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (2.9.1)\n",
      "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (4.57.3)\n",
      "Requirement already satisfied: accelerate>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.4.0)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (2.3.3)\n",
      "Requirement already satisfied: pillow>=9.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (12.1.0)\n",
      "Requirement already satisfied: datasets>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (4.4.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (1.8.0)\n",
      "Requirement already satisfied: xgboost>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (3.1.2)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (4.6.0)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (3.10.8)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (4.67.1)\n",
      "Requirement already satisfied: kagglehub>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 30)) (0.3.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (3.1.3)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.6.0->-r requirements.txt (line 5)) (3.5.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 8)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 8)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 8)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 8)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 8)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 8)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 8)) (0.7.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.0->-r requirements.txt (line 9)) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 13)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 13)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 13)) (2025.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 15)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 15)) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 15)) (0.27.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 15)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 15)) (0.70.18)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 18)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 18)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 18)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 23)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 23)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 23)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 23)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 23)) (3.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 15)) (3.13.3)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 15)) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 15)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 15)) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 15)) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 15)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 15)) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 13)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 8)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 8)) (2.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=2.6.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.6.0->-r requirements.txt (line 5)) (2.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 15)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 15)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 15)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 15)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 15)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 15)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 15)) (1.22.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except Exception as e:\n",
    "    HAS_LGB = False\n",
    "    raise Exception('LightGBM import failed:', e)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_MODELS: ['cnn_base (facebook/convnextv2-base-22k-224)', 'cnn_base_1k (facebook/convnext-base-224-22k-1k)', 'cnn_large (facebook/convnextv2-large-22k-224)'] (3 models)\n",
      "CNN_SEED: 42\n",
      "DINO_SEEDS: [42] (1 models)\n",
      "CNN_LEARNING_RATE: 1e-05\n",
      "DINO_LEARNING_RATE: 1e-05\n",
      "NUM_EPOCHS: 30 patience: 7\n",
      "FINAL_TRAIN_EPOCHS: 3\n",
      "NUM_TTA_AUGS: 10\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "SECOND_DATASET_BASE_PATH = '.'\n",
    "SECOND_DATASET_TRAIN_DIR = Path(SECOND_DATASET_BASE_PATH) / 'train'\n",
    "SECOND_DATASET_CSV_PATH = Path(SECOND_DATASET_BASE_PATH) / 'train_labels.csv'\n",
    "SECOND_DATASET_TEST_DIR = Path(SECOND_DATASET_BASE_PATH) / 'test'\n",
    "\n",
    "# HuggingFace Dataset\n",
    "HF_DATASET_ID = 'subinium/emojiimage-dataset'\n",
    "\n",
    "# Models - Mixed ensemble: 1 DINO, 1 CNN, 2 EfficientNet\n",
    "DINO_MODEL_ID = 'facebook/dinov2-base'\n",
    "CNN_MODEL_ID = 'facebook/convnextv2-base-22k-224'\n",
    "CNN_TAG = 'cnn_base'\n",
    "EFFICIENTNET_MODELS = [\n",
    "    ('google/efficientnet-b0', 'effnet_b0'),\n",
    "    ('google/efficientnet-b2', 'effnet_b2'),\n",
    "]\n",
    "\n",
    "CNN_SEED = 42\n",
    "DINO_SEED = 42\n",
    "EFFNET_SEED = 42\n",
    "\n",
    "# Train\n",
    "VAL_SIZE = 0.10\n",
    "RANDOM_STATE = 42\n",
    "NUM_EPOCHS = 30\n",
    "EARLY_STOPPING_PATIENCE = 7\n",
    "LEARNING_RATE = 1e-5  # Default\n",
    "CNN_LEARNING_RATE = 1e-5  # Lower LR for CNN (more careful fine-tuning)\n",
    "DINO_LEARNING_RATE = 1e-5  # Keep DINO LR as is\n",
    "EFFNET_LEARNING_RATE = 1e-5  # Learning rate for EfficientNet\n",
    "FINAL_TRAIN_EPOCHS = 3  # Final training on combined data\n",
    "FINAL_TRAIN_LR_MULT = 0.5  # Lower LR for final training\n",
    "BATCH_SIZE_CUDA = 16\n",
    "BATCH_SIZE_CPU = 4\n",
    "NUM_WORKERS = 2\n",
    "LABEL_SMOOTHING = 0.05\n",
    "CNN_LABEL_SMOOTHING = 0.1  # More smoothing for CNN\n",
    "EFFNET_LABEL_SMOOTHING = 0.1  # Label smoothing for EfficientNet\n",
    "\n",
    "# TTA\n",
    "NUM_TTA_AUGS = 10\n",
    "TQDM_MININTERVAL = 10\n",
    "SHOW_PROGRESS = True  # set False to reduce output\n",
    "FEATURE_BATCH_SIZE = 16  # images per batch for feature extraction\n",
    "\n",
    "# Meta-model\n",
    "USE_LIGHTGBM = True\n",
    "LGB_PARAMS = {\n",
    "    'n_estimators': 1500,  # More trees\n",
    "    'learning_rate': 0.02,  # Lower LR for better convergence\n",
    "    'num_leaves': 127,  # More leaves\n",
    "    'subsample': 0.8,\n",
    "#    'colsample_bytree': 0.8,\n",
    "    'bagging_freq' : 1,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'verbosity': -1,  # Suppress warnings\n",
    "    'feature_fraction': 0.9,  # Use 90% of features\n",
    "    'min_child_samples': 20  # Prevent overfitting\n",
    "}\n",
    "\n",
    "# Output\n",
    "PREDICTIONS_OUTPUT_FILE = 'predictions_V14.csv'\n",
    "\n",
    "print('CNN_MODEL:', f'{CNN_TAG} ({CNN_MODEL_ID})')\n",
    "print('DINO_MODEL:', DINO_MODEL_ID)\n",
    "print('EFFICIENTNET_MODELS:', [f'{tag} ({model_id})' for model_id, tag in EFFICIENTNET_MODELS], f'({len(EFFICIENTNET_MODELS)} models)')\n",
    "print('CNN_SEED:', CNN_SEED, 'DINO_SEED:', DINO_SEED, 'EFFNET_SEED:', EFFNET_SEED)\n",
    "print('CNN_LEARNING_RATE:', CNN_LEARNING_RATE)\n",
    "print('DINO_LEARNING_RATE:', DINO_LEARNING_RATE)\n",
    "print('EFFNET_LEARNING_RATE:', EFFNET_LEARNING_RATE)\n",
    "print('NUM_EPOCHS:', NUM_EPOCHS, 'patience:', EARLY_STOPPING_PATIENCE)\n",
    "print('FINAL_TRAIN_EPOCHS:', FINAL_TRAIN_EPOCHS)\n",
    "print('NUM_TTA_AUGS:', NUM_TTA_AUGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VENDOR_CLASSES: ['apple', 'google', 'whatsapp', 'facebook', 'samsung', 'mozilla', 'messenger']\n",
      "HF_TO_V11_MAPPING: {'Apple': 'apple', 'Google': 'google', 'Gmail': 'google', 'Mozilla': 'google', 'Facebook': 'facebook', 'Samsung': 'samsung', 'WhatsApp': 'whatsapp', 'Messenger': 'messenger', 'DoCoMo': 'apple', 'JoyPixels': 'apple', 'KDDI': 'apple', 'SoftBank': 'apple', 'Twitter': 'google', 'Windows': 'google'}\n"
     ]
    }
   ],
   "source": [
    "VENDOR_CLASSES = ['apple','google','whatsapp','facebook','samsung','mozilla','messenger']\n",
    "VENDOR_TO_IDX = {v:i for i,v in enumerate(VENDOR_CLASSES)}\n",
    "IDX_TO_VENDOR = {i:v for v,i in VENDOR_TO_IDX.items()}\n",
    "\n",
    "# Label mapping from HuggingFace dataset (11 classes) to target dataset (7 classes)\n",
    "HF_TO_V11_MAPPING = {\n",
    "    'Apple': 'apple',\n",
    "    'Google': 'google', 'Gmail': 'google', 'Mozilla': 'google',\n",
    "    'Facebook': 'facebook',\n",
    "    'Samsung': 'samsung',\n",
    "    'WhatsApp': 'whatsapp',  # if exists in HF dataset\n",
    "    'Messenger': 'messenger',  # if exists in HF dataset\n",
    "    'DoCoMo': 'apple', 'JoyPixels': 'apple', 'KDDI': 'apple', 'SoftBank': 'apple',\n",
    "    'Twitter': 'google', 'Windows': 'google'\n",
    "}\n",
    "\n",
    "print('VENDOR_CLASSES:', VENDOR_CLASSES)\n",
    "print('HF_TO_V11_MAPPING:', HF_TO_V11_MAPPING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Augmentation (Predictable TTA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical features (incl. original_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_properties(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        mode_mapping = {'L':0,'LA':1,'P':2,'RGB':3,'RGBA':4}\n",
    "        original_mode = float(mode_mapping.get(img.mode, 3))\n",
    "        # Normalize image to RGB for pixel stats\n",
    "        if img.mode == 'P':\n",
    "            img = img.convert('RGBA')\n",
    "        if img.mode == 'RGBA':\n",
    "            bg = Image.new('RGB', img.size, (255,255,255))\n",
    "            bg.paste(img, mask=img.split()[3])\n",
    "            img = bg\n",
    "        elif img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        w,h = img.size\n",
    "        ar = w / h if h else 0.0\n",
    "        pix = float(w*h)\n",
    "        arr = np.array(img)\n",
    "        mean_r = float(arr[:,:,0].mean()); mean_g = float(arr[:,:,1].mean()); mean_b = float(arr[:,:,2].mean())\n",
    "        std_r = float(arr[:,:,0].std());  std_g  = float(arr[:,:,1].std());  std_b  = float(arr[:,:,2].std())\n",
    "        brightness = float((mean_r+mean_g+mean_b)/3.0)\n",
    "        is_mostly_white = float(brightness > 200)\n",
    "        return {\n",
    "            'width': float(w), 'height': float(h), 'aspect_ratio': float(ar), 'pixel_count': pix,\n",
    "            'mean_r': mean_r, 'mean_g': mean_g, 'mean_b': mean_b,\n",
    "            'std_r': std_r, 'std_g': std_g, 'std_b': std_b,\n",
    "            'brightness': brightness, 'is_mostly_white': is_mostly_white,\n",
    "            'original_mode': original_mode\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'width':224.0,'height':224.0,'aspect_ratio':1.0,'pixel_count':50176.0,\n",
    "            'mean_r':128.0,'mean_g':128.0,'mean_b':128.0,\n",
    "            'std_r':50.0,'std_g':50.0,'std_b':50.0,\n",
    "            'brightness':128.0,'is_mostly_white':0.0,'original_mode':3.0\n",
    "        }\n",
    "\n",
    "STAT_COLS = ['width','height','aspect_ratio','pixel_count','mean_r','mean_g','mean_b','std_r','std_g','std_b','brightness','is_mostly_white','original_mode']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic augmentation ready.\n"
     ]
    }
   ],
   "source": [
    "class DeterministicAugmentation:\n",
    "    def __init__(self, image_size=224, seed=42):\n",
    "        self.image_size = image_size\n",
    "        self.seed = seed\n",
    "        self.rotation_angles = [-10, -5, 5, 10]\n",
    "        self.crop_ratios = [0.75, 0.85, 0.9, 0.95]\n",
    "        self.color_jitter_params = {'brightness':0.3,'contrast':0.3,'saturation':0.3,'hue':0.1}\n",
    "        self.translate_range = (0.1, 0.1)\n",
    "        self.blur_sigma = (0.1, 0.5)\n",
    "\n",
    "    def _get_deterministic_seed(self, image_or_hash):\n",
    "        if isinstance(image_or_hash, Image.Image):\n",
    "            img_bytes = image_or_hash.tobytes()\n",
    "            return int(hashlib.md5(img_bytes).hexdigest()[:8], 16)\n",
    "        return hash(str(image_or_hash)) & 0xFFFFFFFF\n",
    "\n",
    "    def horizontal_flip(self, image):\n",
    "        return F.hflip(image)\n",
    "\n",
    "    def rotation(self, image, angle):\n",
    "        return F.rotate(image, angle)\n",
    "\n",
    "    def center_crop(self, image, crop_ratio=0.9):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        return F.center_crop(image, [crop,crop])\n",
    "\n",
    "    def corner_crop(self, image, crop_ratio=0.9, position='tl'):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        if position=='tl': return F.crop(image, 0, 0, crop, crop)\n",
    "        if position=='tr': return F.crop(image, 0, w-crop, crop, crop)\n",
    "        if position=='bl': return F.crop(image, h-crop, 0, crop, crop)\n",
    "        if position=='br': return F.crop(image, h-crop, w-crop, crop, crop)\n",
    "        return image\n",
    "\n",
    "    def resized_crop(self, image, crop_ratio=0.85):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        cropped = F.center_crop(image, [crop,crop])\n",
    "        return cropped.resize((self.image_size,self.image_size), Image.BILINEAR)\n",
    "\n",
    "    def color_jitter(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        b = 1.0 + np.random.uniform(-self.color_jitter_params['brightness'], self.color_jitter_params['brightness'])\n",
    "        c = 1.0 + np.random.uniform(-self.color_jitter_params['contrast'], self.color_jitter_params['contrast'])\n",
    "        s = 1.0 + np.random.uniform(-self.color_jitter_params['saturation'], self.color_jitter_params['saturation'])\n",
    "        h = np.random.uniform(-self.color_jitter_params['hue'], self.color_jitter_params['hue'])\n",
    "        img = F.adjust_brightness(image, b)\n",
    "        img = F.adjust_contrast(img, c)\n",
    "        img = F.adjust_saturation(img, s)\n",
    "        img = F.adjust_hue(img, h)\n",
    "        return img\n",
    "\n",
    "    def affine_transform(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        tx = np.random.uniform(-self.translate_range[0], self.translate_range[0])\n",
    "        ty = np.random.uniform(-self.translate_range[1], self.translate_range[1])\n",
    "        return F.affine(image, angle=0, translate=(tx*image.width, ty*image.height), scale=1.0, shear=0.0)\n",
    "\n",
    "    def gaussian_blur(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        sigma = np.random.uniform(self.blur_sigma[0], self.blur_sigma[1])\n",
    "        return F.gaussian_blur(image, kernel_size=3, sigma=[sigma,sigma])\n",
    "\n",
    "    def get_augmentations(self, image, num_augmentations=10, seed_source=None):\n",
    "        if seed_source is None:\n",
    "            seed_val = self._get_deterministic_seed(image)\n",
    "        elif isinstance(seed_source, str):\n",
    "            seed_val = self._get_deterministic_seed(seed_source)\n",
    "        else:\n",
    "            seed_val = int(seed_source)\n",
    "        augs = []\n",
    "        augs.append(image.resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        augs.append(self.horizontal_flip(image).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        for angle in self.rotation_angles[:max(0, min(4, num_augmentations-len(augs)))]:\n",
    "            augs.append(self.rotation(image, angle).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        corners = ['tl','tr','bl','br']\n",
    "        for cpos in corners[:max(0, min(4, num_augmentations-len(augs)))]:\n",
    "            augs.append(self.corner_crop(image, 0.9, cpos).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.center_crop(image, 0.9).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.resized_crop(image, 0.85))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.color_jitter(image, seed_val).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.affine_transform(image, seed_val+1).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.gaussian_blur(image, seed_val+2).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        return augs[:num_augmentations]\n",
    "    \n",
    "    def apply_training_augmentation(self, image, seed_source=None):\n",
    "        if seed_source is None:\n",
    "            seed_val = self._get_deterministic_seed(image)\n",
    "        elif isinstance(seed_source, str):\n",
    "            seed_val = self._get_deterministic_seed(seed_source)\n",
    "        else:\n",
    "            seed_val = int(seed_source)\n",
    "        \n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        \n",
    "        if (seed_val % 2 == 0):\n",
    "            image = self.horizontal_flip(image)\n",
    "        \n",
    "        angle_idx = (seed_val // 2) % len(self.rotation_angles)\n",
    "        angle = self.rotation_angles[angle_idx]\n",
    "        image = self.rotation(image, angle)\n",
    "        \n",
    "        crop_idx = (seed_val // 10) % len(self.crop_ratios)\n",
    "        crop_ratio = self.crop_ratios[crop_idx]\n",
    "        w, h = image.size\n",
    "        crop_size = int(min(w, h) * crop_ratio)\n",
    "        image = F.center_crop(image, [crop_size, crop_size])\n",
    "        \n",
    "        image = self.color_jitter(image, seed_val)\n",
    "        \n",
    "        if (seed_val // 3) % 2 == 0:\n",
    "            image = self.affine_transform(image, seed_val + 1)\n",
    "        \n",
    "        if (seed_val // 5) % 5 == 0:\n",
    "            image = self.gaussian_blur(image, seed_val + 2)\n",
    "        \n",
    "        image = image.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        \n",
    "        return image\n",
    "\n",
    "tta_aug = DeterministicAugmentation(image_size=224, seed=42)\n",
    "print('Deterministic augmentation ready.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_hf_dataset_with_mapping(dataset_path):\n",
    "    \"\"\"\n",
    "    Prepare HuggingFace dataset by finding all images and mapping vendor labels.\n",
    "    Maps 11 HF classes to 7 target classes using HF_TO_V11_MAPPING.\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    dataset_path = Path(dataset_path)\n",
    "    image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
    "    \n",
    "    # HF dataset has vendor folders (Apple, Google, Facebook, etc.)\n",
    "    # Scan each vendor folder and map to target classes\n",
    "    for hf_vendor, target_vendor in HF_TO_V11_MAPPING.items():\n",
    "        if target_vendor not in VENDOR_TO_IDX:\n",
    "            continue  # Skip if target vendor not in our classes\n",
    "        \n",
    "        vendor_dir = dataset_path / hf_vendor\n",
    "        if vendor_dir.exists() and vendor_dir.is_dir():\n",
    "            for ext in image_extensions:\n",
    "                images = list(vendor_dir.glob(f\"*{ext}\"))\n",
    "                for img_path in images:\n",
    "                    image_paths.append(str(img_path))\n",
    "                    labels.append(VENDOR_TO_IDX[target_vendor])\n",
    "    \n",
    "    # Fallback: if no images found via vendor folders, try scanning all images\n",
    "    if len(image_paths) == 0:\n",
    "        for ext in image_extensions:\n",
    "            all_images = list(dataset_path.rglob(f\"*{ext}\"))\n",
    "            for img_path in all_images:\n",
    "                filename = img_path.name.lower()\n",
    "                parent_dir = img_path.parent.name\n",
    "                # Try to match vendor from filename or parent directory\n",
    "                for hf_vendor, target_vendor in HF_TO_V11_MAPPING.items():\n",
    "                    if target_vendor not in VENDOR_TO_IDX:\n",
    "                        continue\n",
    "                    if hf_vendor.lower() in filename or hf_vendor.lower() in parent_dir.lower():\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(VENDOR_TO_IDX[target_vendor])\n",
    "                        break\n",
    "    \n",
    "    print(f'Loaded {len(image_paths)} images from HuggingFace dataset')\n",
    "    if len(labels) > 0:\n",
    "        label_counts = np.bincount(np.array(labels), minlength=len(VENDOR_CLASSES))\n",
    "        print(f'Label distribution: {label_counts}')\n",
    "        print(f'Label names: {[VENDOR_CLASSES[i] for i in range(len(VENDOR_CLASSES))]}')\n",
    "    \n",
    "    return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading (CSV labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_from_csv(train_dir, csv_path):\n",
    "    train_dir = Path(train_dir); csv_path = Path(csv_path)\n",
    "    if not train_dir.exists() or not csv_path.exists():\n",
    "        raise FileNotFoundError(f'Missing train_dir or csv: {train_dir} / {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    label_map = {v: VENDOR_TO_IDX[v] for v in VENDOR_CLASSES}\n",
    "    img_paths=[]; labels=[]\n",
    "    missing=0; unmapped=0\n",
    "    for _, r in df.iterrows():\n",
    "        img_id = str(r['Id']).zfill(5)\n",
    "        lab = str(r['Label']).lower()\n",
    "        if lab not in label_map:\n",
    "            unmapped += 1\n",
    "            continue\n",
    "        found = None\n",
    "        for ext in ('.png','.jpg','.jpeg','.PNG','.JPG','.JPEG'):\n",
    "            p = train_dir / f'{img_id}{ext}'\n",
    "            if p.exists():\n",
    "                found = str(p)\n",
    "                break\n",
    "        if found is None:\n",
    "            missing += 1\n",
    "            continue\n",
    "        img_paths.append(found)\n",
    "        labels.append(int(label_map[lab]))\n",
    "    print('Loaded:', len(img_paths), 'images')\n",
    "    print('Unmapped labels skipped:', unmapped, 'Missing files skipped:', missing)\n",
    "    if labels:\n",
    "        print('Label distribution:', np.bincount(np.array(labels), minlength=len(VENDOR_CLASSES)))\n",
    "    return img_paths, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_rgb(path):\n",
    "    img = Image.open(path)\n",
    "    if img.mode == 'P':\n",
    "        img = img.convert('RGBA')\n",
    "    if img.mode == 'RGBA':\n",
    "        bg = Image.new('RGB', img.size, (255,255,255))\n",
    "        bg.paste(img, mask=img.split()[3])\n",
    "        img = bg\n",
    "    elif img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, processor, use_augmentation=False):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.labels = list(labels)\n",
    "        self.processor = processor\n",
    "        self.use_augmentation = use_augmentation\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_paths[idx]\n",
    "        y = int(self.labels[idx])\n",
    "        img = load_image_rgb(p)\n",
    "        \n",
    "        # Apply training augmentation if enabled (deterministic based on image path)\n",
    "        if self.use_augmentation:\n",
    "            img = tta_aug.apply_training_augmentation(img, seed_source=str(p))\n",
    "        \n",
    "        inputs = self.processor(img, return_tensors='pt')\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        y = int(max(0, min(y, len(VENDOR_CLASSES)-1)))\n",
    "        return {'pixel_values': pixel_values, 'labels': torch.tensor(y, dtype=torch.long)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model wrapper (DINOv2, ConvNeXtV2, and EfficientNet backbones → 7 classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtV2ForEmojiClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_labels, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_labels = num_labels\n",
    "        self.label_smoothing = label_smoothing\n",
    "        # ConvNeXtV2 hidden size is in config.hidden_sizes\n",
    "        hidden = getattr(getattr(base_model, 'config', None), 'hidden_sizes', [1024])[-1]\n",
    "        \n",
    "        # Improved classifier with more capacity (deeper, better for CNN)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden // 2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden // 2, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        # ConvNeXtV2 backbone feature map\n",
    "        out = self.base_model.convnextv2(pixel_values)\n",
    "        feats = out.last_hidden_state\n",
    "        if len(feats.shape) == 4:\n",
    "            pooled = feats.mean(dim=[2,3])\n",
    "        else:\n",
    "            pooled = feats\n",
    "        logits = self.classifier(pooled)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "            loss = nn.CrossEntropyLoss(label_smoothing=self.label_smoothing)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "class DINOv2ForEmojiClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_labels = num_labels\n",
    "        hidden = getattr(base_model.config, 'hidden_size', 1024)\n",
    "        \n",
    "        # Simpler head: less capacity, less overfitting risk\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(0.1),  # Lower dropout\n",
    "            nn.Linear(hidden, num_labels)  # Direct projection\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        out = self.base_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "        if hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
    "            pooled = out.pooler_output\n",
    "        else:\n",
    "            pooled = out.hidden_states[-1][:,0,:]\n",
    "        logits = self.classifier(pooled)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "            loss = nn.CrossEntropyLoss(label_smoothing=0.1)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "class EfficientNetForEmojiClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_labels, label_smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_labels = num_labels\n",
    "        self.label_smoothing = label_smoothing\n",
    "        \n",
    "        # EfficientNet models in transformers have different structures\n",
    "        # Try to get hidden size from config or model\n",
    "        hidden = None\n",
    "        if hasattr(base_model, 'config'):\n",
    "            if hasattr(base_model.config, 'hidden_size'):\n",
    "                hidden = base_model.config.hidden_size\n",
    "            elif hasattr(base_model.config, 'hidden_dim'):\n",
    "                hidden = base_model.config.hidden_dim\n",
    "            elif hasattr(base_model.config, 'num_channels'):\n",
    "                # Some EfficientNet models use num_channels\n",
    "                hidden = getattr(base_model.config, 'hidden_size', 1280)\n",
    "        \n",
    "        # Fallback: try to get from model directly\n",
    "        if hidden is None:\n",
    "            if hasattr(base_model, 'classifier'):\n",
    "                # Try to infer from classifier\n",
    "                if isinstance(base_model.classifier, nn.Linear):\n",
    "                    hidden = base_model.classifier.in_features\n",
    "                elif isinstance(base_model.classifier, nn.Sequential):\n",
    "                    for layer in base_model.classifier:\n",
    "                        if isinstance(layer, nn.Linear):\n",
    "                            hidden = layer.in_features\n",
    "                            break\n",
    "        \n",
    "        # Final fallback\n",
    "        if hidden is None:\n",
    "            hidden = 1280  # Default for EfficientNet\n",
    "        \n",
    "        # Improved classifier similar to CNN (deeper, more capacity)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden // 2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden // 2, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        # EfficientNet models may have different forward signatures\n",
    "        # Try standard forward\n",
    "        out = self.base_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "        \n",
    "        # Extract features\n",
    "        if hasattr(out, 'last_hidden_state'):\n",
    "            feats = out.last_hidden_state\n",
    "        elif hasattr(out, 'hidden_states') and out.hidden_states is not None:\n",
    "            feats = out.hidden_states[-1]\n",
    "        elif hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
    "            pooled = out.pooler_output\n",
    "            logits = self.classifier(pooled)\n",
    "            loss = None\n",
    "            if labels is not None:\n",
    "                labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "                loss = nn.CrossEntropyLoss(label_smoothing=self.label_smoothing)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            return ImageClassifierOutput(loss=loss, logits=logits)\n",
    "        else:\n",
    "            # Fallback: try to get logits directly\n",
    "            if hasattr(out, 'logits'):\n",
    "                pooled = out.logits\n",
    "                # Use base model's pooling if available\n",
    "                if hasattr(self.base_model, 'global_pool'):\n",
    "                    pooled = self.base_model.global_pool(pixel_values)\n",
    "                else:\n",
    "                    pooled = out.logits.mean(dim=1) if len(out.logits.shape) > 2 else out.logits\n",
    "            else:\n",
    "                raise ValueError(\"Could not extract features from EfficientNet model\")\n",
    "        \n",
    "        # Global average pooling if needed\n",
    "        if len(feats.shape) == 4:\n",
    "            pooled = feats.mean(dim=[2, 3])\n",
    "        elif len(feats.shape) == 3:\n",
    "            pooled = feats.mean(dim=1)\n",
    "        else:\n",
    "            pooled = feats\n",
    "        \n",
    "        logits = self.classifier(pooled)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "            loss = nn.CrossEntropyLoss(label_smoothing=self.label_smoothing)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return ImageClassifierOutput(loss=loss, logits=logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Validate loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss=0.0; correct=0; total=0\n",
    "    use_amp = (device.type=='cuda')\n",
    "    for batch in tqdm(loader, desc='Training', mininterval=TQDM_MININTERVAL):\n",
    "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        y = batch['labels'].to(device, non_blocking=True)\n",
    "        y = torch.clamp(y, 0, model.num_labels-1)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(pixel_values=x, labels=y)\n",
    "            loss = out.loss\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += float(loss.item())\n",
    "        pred = torch.argmax(out.logits, dim=1)\n",
    "        correct += int((pred==y).sum().item())\n",
    "        total += int(y.size(0))\n",
    "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss=0.0; correct=0; total=0\n",
    "    preds=[]; labels=[]\n",
    "    use_amp = (device.type=='cuda')\n",
    "    for batch in tqdm(loader, desc='Validation', mininterval=TQDM_MININTERVAL):\n",
    "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        y = batch['labels'].to(device, non_blocking=True)\n",
    "        y = torch.clamp(y, 0, model.num_labels-1)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(pixel_values=x, labels=y)\n",
    "            loss = out.loss\n",
    "        total_loss += float(loss.item())\n",
    "        pred = torch.argmax(out.logits, dim=1)\n",
    "        pred = torch.clamp(pred, 0, model.num_labels-1)\n",
    "        correct += int((pred==y).sum().item())\n",
    "        total += int(y.size(0))\n",
    "        preds.extend(pred.cpu().numpy().tolist())\n",
    "        labels.extend(y.cpu().numpy().tolist())\n",
    "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total), preds, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training on HuggingFace Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_on_hf_dataset(model_kind, model_id, seed, tag=None, learning_rate=None, label_smoothing=None):\n",
    "    \"\"\"\n",
    "    Pre-train model on HuggingFace dataset with label mapping.\n",
    "    Returns model, processor, checkpoint_path, best_acc\n",
    "    \n",
    "    Args:\n",
    "        model_kind: 'cnn' or 'dino'\n",
    "        model_id: HuggingFace model identifier\n",
    "        seed: Random seed\n",
    "        tag: Optional tag for checkpoint naming (e.g., 'cnn_base', 'cnn_tiny')\n",
    "    \"\"\"\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # Use model-specific learning rate and label smoothing\n",
    "    if learning_rate is None:\n",
    "        if model_kind == 'cnn':\n",
    "            learning_rate = CNN_LEARNING_RATE\n",
    "        elif model_kind == 'efficientnet':\n",
    "            learning_rate = EFFNET_LEARNING_RATE\n",
    "        else:\n",
    "            learning_rate = DINO_LEARNING_RATE\n",
    "    if label_smoothing is None:\n",
    "        if model_kind == 'cnn':\n",
    "            label_smoothing = CNN_LABEL_SMOOTHING\n",
    "        elif model_kind == 'efficientnet':\n",
    "            label_smoothing = EFFNET_LABEL_SMOOTHING\n",
    "        else:\n",
    "            label_smoothing = LABEL_SMOOTHING\n",
    "    \n",
    "    # Download and load HuggingFace dataset\n",
    "    print(f'\\n=== Pre-training {model_kind.upper()} (seed={seed}) on HuggingFace dataset ===')\n",
    "    hf_path = kagglehub.dataset_download(HF_DATASET_ID)\n",
    "    print(f'HuggingFace dataset path: {hf_path}')\n",
    "    \n",
    "    hf_paths, hf_labels = prepare_hf_dataset_with_mapping(hf_path)\n",
    "    \n",
    "    if len(hf_paths) == 0:\n",
    "        raise ValueError('No images found in HuggingFace dataset')\n",
    "    \n",
    "    # Split HF dataset into train/val\n",
    "    labels_arr = np.array(hf_labels)\n",
    "    min_count = np.bincount(labels_arr, minlength=len(VENDOR_CLASSES)).min() if len(labels_arr) else 0\n",
    "    can_stratify = (min_count >= 2)\n",
    "    \n",
    "    hf_train_paths, hf_val_paths, hf_train_y, hf_val_y = train_test_split(\n",
    "        list(hf_paths), list(hf_labels),\n",
    "        test_size=VAL_SIZE, random_state=seed,\n",
    "        stratify=list(hf_labels) if can_stratify else None\n",
    "    )\n",
    "    \n",
    "    print(f'HF Train: {len(hf_train_paths)}, HF Val: {len(hf_val_paths)}')\n",
    "    \n",
    "    # Load model and processor\n",
    "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "    backbone = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
    "    if model_kind == 'dino':\n",
    "        model = DINOv2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
    "    elif model_kind == 'cnn':\n",
    "        model = ConvNeXtV2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES), label_smoothing=label_smoothing).to(device)\n",
    "    elif model_kind == 'efficientnet':\n",
    "        model = EfficientNetForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES), label_smoothing=label_smoothing).to(device)\n",
    "    else:\n",
    "        raise ValueError('Unknown model_kind: ' + str(model_kind))\n",
    "    \n",
    "    # Create datasets and loaders\n",
    "    hf_train_ds = EmojiDataset(hf_train_paths, hf_train_y, processor, use_augmentation=True)\n",
    "    hf_val_ds = EmojiDataset(hf_val_paths, hf_val_y, processor, use_augmentation=False)\n",
    "    bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
    "    hf_train_loader = DataLoader(hf_train_ds, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    hf_val_loader = DataLoader(hf_val_ds, batch_size=bs, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    \n",
    "    # Training setup\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Use model-specific scheduler: CosineAnnealingWarmRestarts for CNN, ReduceLROnPlateau for DINO\n",
    "    # DINO: ReduceLROnPlateau (works well for transformers)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7, cooldown=1\n",
    "    )\n",
    "    use_plateau_scheduler = True\n",
    "    print(f'Using ReduceLROnPlateau scheduler')\n",
    "    \n",
    "    scaler = None\n",
    "    if torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()):\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    # Training loop\n",
    "    best_acc = -1.0\n",
    "    # Use tag in filename if provided, otherwise use seed\n",
    "    if tag:\n",
    "        best_path = f'pretrained_{model_kind}_{tag}_hf.pt'\n",
    "    else:\n",
    "        best_path = f'pretrained_{model_kind}_seed{seed}_hf.pt'\n",
    "    bad = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f'\\n[HF Pre-train {model_kind} seed={seed}] epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        tr_loss, tr_acc = train_epoch(model, hf_train_loader, optimizer, device, scaler)\n",
    "        va_loss, va_acc, va_pred, va_true = validate(model, hf_val_loader, device)\n",
    "        \n",
    "        # Step scheduler: ReduceLROnPlateau needs metric, others don't\n",
    "        if use_plateau_scheduler:\n",
    "            scheduler.step(va_acc)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        print(f'Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% | Val: loss={va_loss:.4f} acc={va_acc:.2f}%')\n",
    "        if va_acc > best_acc + 1e-6:\n",
    "            best_acc = va_acc\n",
    "            bad = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print('✓ saved', best_path)\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= EARLY_STOPPING_PATIENCE:\n",
    "                print('Early stopping: no improvement for', EARLY_STOPPING_PATIENCE, 'epochs')\n",
    "                break\n",
    "    \n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    print(f'✓ Pre-training completed! Best HF validation accuracy: {best_acc:.2f}%')\n",
    "    return model, processor, best_path, best_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning on Target Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_model(model_kind, model_id, seed, stage_tag, train_paths_s, train_y_s, val_paths_s, val_y_s, \n",
    "                   pretrained_checkpoint=None, learning_rate=None, label_smoothing=None):\n",
    "    \"\"\"\n",
    "    Fine-tune model on target dataset.\n",
    "    If pretrained_checkpoint is provided, loads weights from pre-training.\n",
    "    \"\"\"\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    # Use model-specific learning rate and label smoothing\n",
    "    if learning_rate is None:\n",
    "        if model_kind == 'cnn':\n",
    "            learning_rate = CNN_LEARNING_RATE\n",
    "        elif model_kind == 'efficientnet':\n",
    "            learning_rate = EFFNET_LEARNING_RATE\n",
    "        else:\n",
    "            learning_rate = DINO_LEARNING_RATE\n",
    "    if label_smoothing is None:\n",
    "        if model_kind == 'cnn':\n",
    "            label_smoothing = CNN_LABEL_SMOOTHING\n",
    "        elif model_kind == 'efficientnet':\n",
    "            label_smoothing = EFFNET_LABEL_SMOOTHING\n",
    "        else:\n",
    "            label_smoothing = LABEL_SMOOTHING\n",
    "    \n",
    "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "    backbone = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
    "    if model_kind == 'dino':\n",
    "        model = DINOv2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
    "    elif model_kind == 'cnn':\n",
    "        model = ConvNeXtV2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES), label_smoothing=label_smoothing).to(device)\n",
    "    elif model_kind == 'efficientnet':\n",
    "        model = EfficientNetForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES), label_smoothing=label_smoothing).to(device)\n",
    "    else:\n",
    "        raise ValueError('Unknown model_kind: ' + str(model_kind))\n",
    "    \n",
    "    # Load pre-trained weights if provided\n",
    "    if pretrained_checkpoint is not None and os.path.exists(pretrained_checkpoint):\n",
    "        model.load_state_dict(torch.load(pretrained_checkpoint, map_location=device))\n",
    "        print(f'✓ Loaded pre-trained weights from {pretrained_checkpoint}')\n",
    "    else:\n",
    "        print('Starting from scratch (no pre-trained checkpoint)')\n",
    "\n",
    "    train_ds = EmojiDataset(train_paths_s, train_y_s, processor, use_augmentation=True)\n",
    "    val_ds = EmojiDataset(val_paths_s, val_y_s, processor, use_augmentation=False)\n",
    "    bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Use model-specific scheduler: CosineAnnealingWarmRestarts for CNN, ReduceLROnPlateau for DINO\n",
    "    # DINO: ReduceLROnPlateau (works well for transformers)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7, cooldown=1\n",
    "    )\n",
    "    use_plateau_scheduler = True\n",
    "    print(f'Using ReduceLROnPlateau scheduler')\n",
    "    \n",
    "    scaler = None\n",
    "    if torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()):\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    best_acc = -1.0\n",
    "    best_path = f'best_{stage_tag}.pt'\n",
    "    bad = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f'\\n[{stage_tag}] epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, device, scaler)\n",
    "        va_loss, va_acc, va_pred, va_true = validate(model, val_loader, device)\n",
    "        \n",
    "        # Step scheduler: ReduceLROnPlateau needs metric, others don't\n",
    "        if use_plateau_scheduler:\n",
    "            scheduler.step(va_acc)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        print(f'Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% | Val: loss={va_loss:.4f} acc={va_acc:.2f}%')\n",
    "        if va_acc > best_acc + 1e-6:\n",
    "            best_acc = va_acc\n",
    "            bad = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print('✓ saved', best_path)\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= EARLY_STOPPING_PATIENCE:\n",
    "                print('Early stopping: no improvement for', EARLY_STOPPING_PATIENCE, 'epochs')\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    return model, processor, best_path, best_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Target Dataset and Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 9879 images\n",
      "Unmapped labels skipped: 0 Missing files skipped: 0\n",
      "Label distribution: [1924 1877 1644 1667 1790  397  580]\n",
      "Min class count: 397 Stratify: True\n",
      "Train: 8891 Val: 988\n",
      "Train dist: [1732 1689 1480 1500 1611  357  522]\n",
      "Val   dist: [192 188 164 167 179  40  58]\n"
     ]
    }
   ],
   "source": [
    "# Load target dataset\n",
    "all_paths, all_labels = prepare_dataset_from_csv(SECOND_DATASET_TRAIN_DIR, SECOND_DATASET_CSV_PATH)\n",
    "\n",
    "# Split target dataset\n",
    "min_count = np.bincount(np.array(all_labels), minlength=len(VENDOR_CLASSES)).min()\n",
    "can_stratify = (min_count >= 2)\n",
    "print('Min class count:', int(min_count), 'Stratify:', can_stratify)\n",
    "train_paths, val_paths, train_y, val_y = train_test_split(\n",
    "    all_paths, all_labels, test_size=VAL_SIZE, random_state=RANDOM_STATE,\n",
    "    stratify=all_labels if can_stratify else None\n",
    ")\n",
    "print('Train:', len(train_paths), 'Val:', len(val_paths))\n",
    "print('Train dist:', np.bincount(np.array(train_y), minlength=len(VENDOR_CLASSES)))\n",
    "print('Val   dist:', np.bincount(np.array(val_y), minlength=len(VENDOR_CLASSES)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline: Pre-train + Fine-tune + Final Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Pre-training on HuggingFace dataset ===\n",
      "\n",
      "=== Pre-training CNN (seed=42) on HuggingFace dataset ===\n",
      "HuggingFace dataset path: /root/.cache/kagglehub/datasets/subinium/emojiimage-dataset/versions/2\n",
      "Loaded 14253 images from HuggingFace dataset\n",
      "Label distribution: [4993 5809    0 1727 1724    0    0]\n",
      "Label names: ['apple', 'google', 'whatsapp', 'facebook', 'samsung', 'mozilla', 'messenger']\n",
      "HF Train: 12827, HF Val: 1426\n",
      "Using ReduceLROnPlateau scheduler\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [01:03<00:00, 12.59it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 50.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=1.0592 acc=67.36% | Val: loss=1.1991 acc=66.97%\n",
      "✓ saved pretrained_cnn_cnn_base_hf.pt\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:58<00:00, 13.62it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 50.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.7439 acc=86.11% | Val: loss=0.9691 acc=74.68%\n",
      "✓ saved pretrained_cnn_cnn_base_hf.pt\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 3/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [01:02<00:00, 12.89it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 50.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.6277 acc=92.08% | Val: loss=0.9530 acc=77.00%\n",
      "✓ saved pretrained_cnn_cnn_base_hf.pt\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 4/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [01:03<00:00, 12.68it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 46.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.5570 acc=95.62% | Val: loss=1.0125 acc=76.65%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 5/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:57<00:00, 13.90it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 50.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.5111 acc=97.92% | Val: loss=1.0403 acc=75.95%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 6/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:58<00:00, 13.73it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 47.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4931 acc=98.54% | Val: loss=1.0765 acc=75.81%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 7/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [01:01<00:00, 12.98it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 49.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4802 acc=99.13% | Val: loss=1.0430 acc=76.79%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 8/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [01:00<00:00, 13.32it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 49.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4628 acc=99.76% | Val: loss=1.0105 acc=78.54%\n",
      "✓ saved pretrained_cnn_cnn_base_hf.pt\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 9/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:58<00:00, 13.79it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 48.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4582 acc=99.88% | Val: loss=1.0268 acc=78.89%\n",
      "✓ saved pretrained_cnn_cnn_base_hf.pt\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 10/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:58<00:00, 13.62it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 48.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4564 acc=99.89% | Val: loss=0.9834 acc=80.08%\n",
      "✓ saved pretrained_cnn_cnn_base_hf.pt\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 11/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [01:01<00:00, 13.00it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 50.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4607 acc=99.73% | Val: loss=1.0572 acc=77.91%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 12/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:59<00:00, 13.49it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 50.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4578 acc=99.77% | Val: loss=1.0936 acc=77.00%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 13/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:59<00:00, 13.56it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 49.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4543 acc=99.92% | Val: loss=1.0320 acc=79.45%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 14/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:57<00:00, 14.00it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 51.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4593 acc=99.67% | Val: loss=1.0568 acc=78.19%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 15/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:58<00:00, 13.74it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 49.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4531 acc=99.93% | Val: loss=1.0452 acc=79.17%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 16/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [00:59<00:00, 13.39it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 50.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4518 acc=99.97% | Val: loss=1.0407 acc=79.17%\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 17/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 802/802 [01:03<00:00, 12.58it/s]\n",
      "Validation: 100%|██████████| 90/90 [00:01<00:00, 52.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4515 acc=99.97% | Val: loss=1.0460 acc=79.38%\n",
      "Early stopping: no improvement for 7 epochs\n",
      "✓ Pre-training completed! Best HF validation accuracy: 80.08%\n",
      "\n",
      "=== Pre-training CNN (seed=42) on HuggingFace dataset ===\n",
      "HuggingFace dataset path: /root/.cache/kagglehub/datasets/subinium/emojiimage-dataset/versions/2\n",
      "Loaded 14253 images from HuggingFace dataset\n",
      "Label distribution: [4993 5809    0 1727 1724    0    0]\n",
      "Label names: ['apple', 'google', 'whatsapp', 'facebook', 'samsung', 'mozilla', 'messenger']\n",
      "HF Train: 12827, HF Val: 1426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85601c754b84571a73dfcc48b009dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf9d5af741d4196b8eefa703ab6b47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c9b64d3bdf41b7ac57c39f855250ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/354M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ReduceLROnPlateau scheduler\n",
      "\n",
      "[HF Pre-train cnn seed=42] epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/802 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConvNextForImageClassification' object has no attribute 'convnextv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m cnn_pretrained_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cnn_model_id, cnn_tag \u001b[38;5;129;01min\u001b[39;00m CNN_MODELS:\n\u001b[0;32m----> 6\u001b[0m     cnn_pretrained, cnn_proc, cnn_hf_ckpt, cnn_hf_best \u001b[38;5;241m=\u001b[39m \u001b[43mpretrain_on_hf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcnn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcnn_model_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCNN_SEED\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcnn_tag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     cnn_pretrained_list\u001b[38;5;241m.\u001b[39mappend((cnn_pretrained, cnn_proc, cnn_hf_ckpt, cnn_tag))\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Move to CPU to save memory\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[58], line 85\u001b[0m, in \u001b[0;36mpretrain_on_hf_dataset\u001b[0;34m(model_kind, model_id, seed, tag, learning_rate, label_smoothing)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[HF Pre-train \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_kind\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seed=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m     tr_loss, tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     va_loss, va_acc, va_pred, va_true \u001b[38;5;241m=\u001b[39m validate(model, hf_val_loader, device)\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;66;03m# Step scheduler: ReduceLROnPlateau needs metric, others don't\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[57], line 19\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, loader, optimizer, device, scaler)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, enabled\u001b[38;5;241m=\u001b[39muse_amp):\n\u001b[0;32m---> 19\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     loss \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[56], line 23\u001b[0m, in \u001b[0;36mConvNeXtV2ForEmojiClassification.forward\u001b[0;34m(self, pixel_values, labels)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# ConvNeXtV2 backbone feature map\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvnextv2\u001b[49m(pixel_values)\n\u001b[1;32m     24\u001b[0m     feats \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feats\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1964\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1962\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1963\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1964\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1966\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConvNextForImageClassification' object has no attribute 'convnextv2'"
     ]
    }
   ],
   "source": [
    "print('=== Pre-training on HuggingFace dataset ===')\n",
    "\n",
    "# Pre-train 1 CNN model\n",
    "print('\\n--- Pre-training CNN ---')\n",
    "cnn_pretrained, cnn_proc, cnn_hf_ckpt, cnn_hf_best = pretrain_on_hf_dataset('cnn', CNN_MODEL_ID, CNN_SEED, tag=CNN_TAG)\n",
    "if device.type == 'cuda':\n",
    "    cnn_pretrained = cnn_pretrained.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Pre-train 1 DINO model\n",
    "print('\\n--- Pre-training DINO ---')\n",
    "dino_tag = f'dino_seed{DINO_SEED}'\n",
    "dino_pretrained, dino_proc, dino_hf_ckpt, dino_hf_best = pretrain_on_hf_dataset('dino', DINO_MODEL_ID, DINO_SEED, tag=dino_tag)\n",
    "if device.type == 'cuda':\n",
    "    dino_pretrained = dino_pretrained.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Pre-train 2 EfficientNet models\n",
    "print('\\n--- Pre-training EfficientNet models ---')\n",
    "effnet_pretrained_list = []\n",
    "for effnet_model_id, effnet_tag in EFFICIENTNET_MODELS:\n",
    "    effnet_pretrained, effnet_proc, effnet_hf_ckpt, effnet_hf_best = pretrain_on_hf_dataset('efficientnet', effnet_model_id, EFFNET_SEED, tag=effnet_tag)\n",
    "    effnet_pretrained_list.append((effnet_pretrained, effnet_proc, effnet_hf_ckpt, effnet_tag))\n",
    "    if device.type == 'cuda':\n",
    "        effnet_pretrained = effnet_pretrained.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n=== Fine-tuning on target dataset ===')\n",
    "\n",
    "# Fine-tune CNN\n",
    "print('\\n--- Fine-tuning CNN ---')\n",
    "cnn_final, cnn_proc, cnn_ckpt, cnn_best = finetune_model(\n",
    "    'cnn', CNN_MODEL_ID, CNN_SEED, f'cnn_finetuned_{CNN_TAG}',\n",
    "    train_paths, train_y, val_paths, val_y,\n",
    "    pretrained_checkpoint=cnn_hf_ckpt\n",
    ")\n",
    "if device.type == 'cuda':\n",
    "    cnn_final = cnn_final.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Fine-tune DINO\n",
    "print('\\n--- Fine-tuning DINO ---')\n",
    "dino_final, dino_proc, dino_ckpt, dino_best = finetune_model(\n",
    "    'dino', DINO_MODEL_ID, DINO_SEED, f'dino_finetuned_seed{DINO_SEED}',\n",
    "    train_paths, train_y, val_paths, val_y,\n",
    "    pretrained_checkpoint=dino_hf_ckpt\n",
    ")\n",
    "if device.type == 'cuda':\n",
    "    dino_final = dino_final.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Fine-tune EfficientNet models\n",
    "print('\\n--- Fine-tuning EfficientNet models ---')\n",
    "effnet_final_list = []\n",
    "for effnet_pretrained, effnet_proc, effnet_hf_ckpt, effnet_tag in effnet_pretrained_list:\n",
    "    effnet_model_id = next(model_id for model_id, tag in EFFICIENTNET_MODELS if tag == effnet_tag)\n",
    "    effnet_final, effnet_proc, effnet_ckpt, effnet_best = finetune_model(\n",
    "        'efficientnet', effnet_model_id, EFFNET_SEED, f'effnet_finetuned_{effnet_tag}',\n",
    "        train_paths, train_y, val_paths, val_y,\n",
    "        pretrained_checkpoint=effnet_hf_ckpt\n",
    "    )\n",
    "    effnet_final_list.append((effnet_final, effnet_proc, effnet_ckpt, effnet_tag))\n",
    "    if device.type == 'cuda':\n",
    "        effnet_final = effnet_final.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print('\\n=== Final training on combined train+val (CNN + EfficientNet) ===')\n",
    "\n",
    "# Final training on combined data for CNN and EfficientNet models\n",
    "combined_train_paths = train_paths + val_paths\n",
    "combined_train_labels = train_y + val_y\n",
    "\n",
    "# Final training for CNN\n",
    "print(f'\\n--- Final training CNN {CNN_TAG} ---')\n",
    "cnn_model = cnn_final.to(device)\n",
    "combined_ds = EmojiDataset(combined_train_paths, combined_train_labels, cnn_proc, use_augmentation=True)\n",
    "bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
    "combined_loader = DataLoader(combined_ds, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "final_lr = CNN_LEARNING_RATE * FINAL_TRAIN_LR_MULT\n",
    "optimizer = torch.optim.AdamW(cnn_model.parameters(), lr=final_lr, weight_decay=0.01)\n",
    "scaler = None\n",
    "if torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()):\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "for epoch in range(FINAL_TRAIN_EPOCHS):\n",
    "    print(f'  Epoch {epoch+1}/{FINAL_TRAIN_EPOCHS}')\n",
    "    tr_loss, tr_acc = train_epoch(cnn_model, combined_loader, optimizer, device, scaler)\n",
    "    print(f'  Train: loss={tr_loss:.4f} acc={tr_acc:.2f}%')\n",
    "final_cnn_ckpt = f'final_cnn_{CNN_TAG}.pt'\n",
    "torch.save(cnn_model.state_dict(), final_cnn_ckpt)\n",
    "if device.type == 'cuda':\n",
    "    cnn_model = cnn_model.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Final training for EfficientNet models\n",
    "effnet_final_trained_list = []\n",
    "for effnet_final, effnet_proc, effnet_ckpt, effnet_tag in effnet_final_list:\n",
    "    print(f'\\n--- Final training EfficientNet {effnet_tag} ---')\n",
    "    effnet_model = effnet_final.to(device)\n",
    "    combined_ds = EmojiDataset(combined_train_paths, combined_train_labels, effnet_proc, use_augmentation=True)\n",
    "    combined_loader = DataLoader(combined_ds, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    final_lr = EFFNET_LEARNING_RATE * FINAL_TRAIN_LR_MULT\n",
    "    optimizer = torch.optim.AdamW(effnet_model.parameters(), lr=final_lr, weight_decay=0.01)\n",
    "    scaler = None\n",
    "    if torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()):\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    for epoch in range(FINAL_TRAIN_EPOCHS):\n",
    "        print(f'  Epoch {epoch+1}/{FINAL_TRAIN_EPOCHS}')\n",
    "        tr_loss, tr_acc = train_epoch(effnet_model, combined_loader, optimizer, device, scaler)\n",
    "        print(f'  Train: loss={tr_loss:.4f} acc={tr_acc:.2f}%')\n",
    "    final_effnet_ckpt = f'final_effnet_{effnet_tag}.pt'\n",
    "    torch.save(effnet_model.state_dict(), final_effnet_ckpt)\n",
    "    effnet_final_trained_list.append((effnet_model, effnet_proc, final_effnet_ckpt, effnet_tag))\n",
    "    if device.type == 'cuda':\n",
    "        effnet_model = effnet_model.to('cpu')\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# Build trained_members list\n",
    "trained_members = []\n",
    "# Load final CNN model\n",
    "cnn_model.load_state_dict(torch.load(final_cnn_ckpt, map_location=device))\n",
    "trained_members.append((cnn_model, cnn_proc, CNN_TAG))\n",
    "# Add DINO (no final training)\n",
    "trained_members.append((dino_final, dino_proc, dino_tag))\n",
    "# Add EfficientNet models\n",
    "for effnet_model, effnet_proc, effnet_ckpt, effnet_tag in effnet_final_trained_list:\n",
    "    trained_members.append((effnet_model, effnet_proc, effnet_tag))\n",
    "\n",
    "print('\\n✓ Trained members:', [t for _,_,t in trained_members])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature matrix: stats + (A×C) prob vectors (Optimized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prob_cols_for_members(members, num_augmentations):\n",
    "    cols = []\n",
    "    for _, _, tag in members:\n",
    "        for i in range(num_augmentations):\n",
    "            for c in range(len(VENDOR_CLASSES)):\n",
    "                cols.append(f'prob_{tag}_aug{i}_cls{c}')\n",
    "    return cols\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_features_batched(image_paths, members, num_augmentations, batch_size=None):\n",
    "    \"\"\"\n",
    "    Optimized batched feature extraction for maximum GPU efficiency.\n",
    "    - Pre-loads all images and generates augmentations once\n",
    "    - Processes all batches for each model before switching to the next model\n",
    "    - Minimizes GPU model loading/unloading overhead\n",
    "    \"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = FEATURE_BATCH_SIZE\n",
    "\n",
    "    num_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Step 1: Pre-load all images and generate augmentations once (model-independent)\n",
    "    print('Pre-loading images and generating augmentations...')\n",
    "    all_stats = []\n",
    "    all_augmentations = []  # List of lists: [img_idx][aug_idx] = PIL Image\n",
    "    \n",
    "    for img_idx, img_path in enumerate(tqdm(image_paths, desc='Loading images', disable=(not SHOW_PROGRESS))):\n",
    "        stats = extract_image_properties(img_path)\n",
    "        img = load_image_rgb(img_path)\n",
    "        augs = tta_aug.get_augmentations(img, num_augmentations=num_augmentations, seed_source=str(img_path))\n",
    "        all_stats.append(stats)\n",
    "        all_augmentations.append(augs)\n",
    "    \n",
    "    # Step 2: Process all batches for each model (minimize GPU model loading/unloading)\n",
    "    model_probs = {}  # tag -> list of (B, A, C) arrays, one per batch\n",
    "    \n",
    "    for m, proc, tag in tqdm(members, desc='Processing models', disable=(not SHOW_PROGRESS)):\n",
    "        # Load model to GPU once for all batches\n",
    "        m.to(device)\n",
    "        m.eval()\n",
    "        \n",
    "        model_batch_probs = []  # Store probabilities for all batches of this model\n",
    "        \n",
    "        for start in tqdm(range(0, len(image_paths), batch_size),\n",
    "                          desc=f'  {tag} batches',\n",
    "                          mininterval=TQDM_MININTERVAL,\n",
    "                          disable=(not SHOW_PROGRESS),\n",
    "                          total=num_batches,\n",
    "                          leave=False):\n",
    "            batch_end = min(start + batch_size, len(image_paths))\n",
    "            B = batch_end - start\n",
    "            \n",
    "            # Collect augmentations for this batch\n",
    "            batch_augs = []\n",
    "            for img_idx in range(start, batch_end):\n",
    "                batch_augs.extend(all_augmentations[img_idx])\n",
    "            \n",
    "            # Process batch\n",
    "            inputs = proc(batch_augs, return_tensors='pt')\n",
    "            x = inputs['pixel_values'].to(device)\n",
    "            out = m(pixel_values=x)\n",
    "            probs = torch.softmax(out.logits, dim=-1)\n",
    "            probs = probs.view(B, num_augmentations, -1).detach().cpu().numpy()\n",
    "            model_batch_probs.append(probs)\n",
    "        \n",
    "        model_probs[tag] = model_batch_probs\n",
    "        \n",
    "        # Move model back to CPU after processing all batches\n",
    "        if device.type == 'cuda':\n",
    "            m.to('cpu')\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Step 3: Combine stats and probabilities into final feature matrix\n",
    "    print('Combining features...')\n",
    "    all_rows = []\n",
    "    for img_idx, stats in enumerate(all_stats):\n",
    "        row = dict(stats)\n",
    "        \n",
    "        # Find which batch this image belongs to\n",
    "        batch_idx = img_idx // batch_size\n",
    "        batch_local_idx = img_idx % batch_size\n",
    "        \n",
    "        # Extract probabilities for this image from all models\n",
    "        for tag, batch_probs_list in model_probs.items():\n",
    "            batch_probs = batch_probs_list[batch_idx]  # (B, A, C)\n",
    "            for a in range(num_augmentations):\n",
    "                for c in range(len(VENDOR_CLASSES)):\n",
    "                    row[f'prob_{tag}_aug{a}_cls{c}'] = float(batch_probs[batch_local_idx, a, c])\n",
    "        \n",
    "        all_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    prob_cols = _prob_cols_for_members(members, num_augmentations)\n",
    "    all_cols = STAT_COLS + prob_cols\n",
    "    for col in all_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    return df[all_cols]\n",
    "\n",
    "# Estimate total features based on model configuration\n",
    "example_tags = [CNN_TAG, f'dino_seed{DINO_SEED}'] + [tag for _, tag in EFFICIENTNET_MODELS]\n",
    "print('Total features:', len(STAT_COLS) + len(_prob_cols_for_members([('','',t) for t in example_tags], NUM_TTA_AUGS)))\n",
    "print('Using optimized batched feature extraction with batch_size:', FEATURE_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained_members list from the training pipeline\n",
    "members = trained_members\n",
    "print('Members for features:', [t for _,_,t in members])\n",
    "\n",
    "X_train = build_features_batched(train_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "X_val = build_features_batched(val_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "y_train = np.array(train_y)\n",
    "y_val = np.array(val_y)\n",
    "print('X_train:', X_train.shape, 'X_val:', X_val.shape)\n",
    "\n",
    "if USE_LIGHTGBM and HAS_LGB:\n",
    "    meta = lgb.LGBMClassifier(**LGB_PARAMS)\n",
    "    meta.fit(X_train, y_train)\n",
    "    val_pred = meta.predict(X_val)\n",
    "    print('Meta(LGB) val acc:', accuracy_score(y_val, val_pred)*100.0)\n",
    "    print(classification_report(y_val, val_pred, target_names=VENDOR_CLASSES))\n",
    "    meta.booster_.save_model('meta_lgb_v14.txt')\n",
    "    print('Saved meta_lgb_v14.txt')\n",
    "else:\n",
    "    raise RuntimeError('LightGBM not available in this environment. Please install lightgbm or switch USE_LIGHTGBM=False')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final fit on Train+Val and predict Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = trained_members\n",
    "combined_paths = train_paths + val_paths\n",
    "combined_y = train_y + val_y\n",
    "\n",
    "X_all = build_features_batched(combined_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "y_all = np.array(combined_y)\n",
    "\n",
    "if not (USE_LIGHTGBM and HAS_LGB):\n",
    "    raise RuntimeError('LightGBM not available for final fit')\n",
    "meta_final = lgb.LGBMClassifier(**LGB_PARAMS)\n",
    "meta_final.fit(X_all, y_all)\n",
    "meta_final.booster_.save_model('meta_lgb_v14_final.txt')\n",
    "print('Saved meta_lgb_v14_final.txt')\n",
    "\n",
    "test_dir = SECOND_DATASET_TEST_DIR\n",
    "if not test_dir.exists():\n",
    "    raise FileNotFoundError(f'Missing test dir: {test_dir}')\n",
    "test_paths = []\n",
    "for ext in ('.png','.jpg','.jpeg','.PNG','.JPG','.JPEG'):\n",
    "    test_paths += [str(p) for p in test_dir.rglob(f'*{ext}')]\n",
    "test_paths = sorted(set(test_paths))\n",
    "print('Found test images:', len(test_paths))\n",
    "\n",
    "# Process all test images at once (optimized)\n",
    "print('Extracting features for test set...')\n",
    "X_test = build_features_batched(test_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "preds = meta_final.predict(X_test)\n",
    "\n",
    "pred_ids = []\n",
    "pred_labels = []\n",
    "for img_path, pred in zip(test_paths, preds):\n",
    "    img_id = Path(img_path).stem\n",
    "    pred = int(pred)\n",
    "    pred = max(0, min(pred, len(VENDOR_CLASSES)-1))\n",
    "    pred_ids.append(img_id)\n",
    "    pred_labels.append(IDX_TO_VENDOR[pred])\n",
    "\n",
    "out_path = Path(PREDICTIONS_OUTPUT_FILE)\n",
    "with out_path.open('w') as f:\n",
    "    f.write('Id,Label\\n')\n",
    "    for i,l in zip(pred_ids, pred_labels):\n",
    "        f.write(f'{str(i).strip()},{l}\\n')\n",
    "print('Wrote:', out_path, 'rows:', len(pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
