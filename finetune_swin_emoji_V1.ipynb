{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SkPvm15jO68"
      },
      "source": [
        "# Fine-tuning Swin Transformer for Emoji Vendor Classification\n",
        "\n",
        "This notebook fine-tunes a Swin Transformer model to classify emoji images by vendor (Apple, DoCoMo, Facebook, Gmail, Google, JoyPixels, KDDI, Samsung, SoftBank, Twitter, Windows).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install -q pydrive2 google-api-python-client google-auth-httplib2 google-auth-oauthlib\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Detect if running on Kaggle or Colab\n",
        "IS_KAGGLE = 'KAGGLE_KERNEL_RUN_TYPE' in os.environ\n",
        "IS_COLAB = 'COLAB_GPU' in os.environ or 'google.colab' in sys.modules\n",
        "\n",
        "print(f\"Running on: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SmZl8ei6jO6-"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q kagglehub transformers torch torchvision pillow datasets accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR3MBurMjO6_",
        "outputId": "9f1cd362-37ec-4890-cb82-5f1be91cef4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "GPU: Tesla T4\n",
            "CUDA Version: 12.6\n",
            "GPU Memory: 14.74 GB\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import json\n",
        "from transformers import AutoModel, AutoImageProcessor, Trainer, TrainingArguments\n",
        "from transformers.modeling_outputs import ImageClassifierOutput\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset as HFDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# GPU Setup - Ensure everything runs on GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "    # Clear GPU cache\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\"WARNING: CUDA not available. Training will be slow on CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8cb2CEQjO6_"
      },
      "source": [
        "## Download Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnihRx41jO6_",
        "outputId": "3fc1f45d-3341-4eb1-adbc-7056b3272cc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/subinium/emojiimage-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 47.6M/47.6M [00:00<00:00, 131MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/subinium/emojiimage-dataset/versions/2\n"
          ]
        }
      ],
      "source": [
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"subinium/emojiimage-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2hkHv1OjO7A"
      },
      "source": [
        "## Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "2b82a4a1f28140fbb820de56a8c5fbcb",
            "8156d9d52e284c14a2ca25b422dcb8b5",
            "d8700343196e4c328590744664e0091f",
            "1e8967965b644e71a9865bc957d6a374",
            "ae9e989d060a45d880ca042f4db072f9",
            "39577bde28f14abb87cbfb5f13cb1138",
            "18647aad44bc4b6dbdf37f771f50c27c",
            "9b615077840f4feb93cb709b08133444",
            "b1c1dd505bd045788d3762dd463bf5c8",
            "8cc3d3c881e446028bf956b963a60d85",
            "b882015593314371938ee85abb42fceb",
            "35624a110d4d43b790d97b55bb133459",
            "a65d01649a9048368ce899760b7170cd",
            "131c51dae59c409a8c08e10b4a4d8999",
            "e0aba4e21af243ebbd1df38779524a9a",
            "df8d5851b93b4816bb148499f15e47bd",
            "35a8c0cb5b8b44b7b5ed9e8e68ed419c",
            "19b89d13cc724d889831ebd3a2a2abcf",
            "18e9ad54bbe447f1a1835c14eac595b3",
            "d8c1a278f7874e308046c57e9b302d89",
            "e512b252867d454d86f7856ef4434b91",
            "1196e7e30592407fbdce33c12eb8e0b9"
          ]
        },
        "id": "xJsll1CjjO7A",
        "outputId": "276b1c26-75f2-4b07-8144-0e1aad609d8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model with dtype: torch.bfloat16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2b82a4a1f28140fbb820de56a8c5fbcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35624a110d4d43b790d97b55bb133459",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/451M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Model device: cuda:0\n",
            "Model dtype: torch.bfloat16\n",
            "GPU Memory allocated: 0.17 GB\n",
            "GPU Memory reserved: 0.20 GB\n"
          ]
        }
      ],
      "source": [
        "# Load model directly to GPU\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Load model with appropriate dtype for GPU\n",
        "if torch.cuda.is_available():\n",
        "    # Use float16 for faster training on GPU (or bfloat16 if supported)\n",
        "    if torch.cuda.is_bf16_supported():\n",
        "        model_dtype = torch.bfloat16\n",
        "    else:\n",
        "        model_dtype = torch.float16\n",
        "    print(f\"Loading model with dtype: {model_dtype}\")\n",
        "else:\n",
        "    model_dtype = torch.float32\n",
        "\n",
        "model = AutoModel.from_pretrained(\n",
        "    \"timm/swin_base_patch4_window12_384.ms_in22k\",\n",
        "    torch_dtype=model_dtype\n",
        ")\n",
        "\n",
        "# Move model to GPU immediately\n",
        "model = model.to(device)\n",
        "model.eval()  # Set to eval mode initially\n",
        "\n",
        "# Load image processor\n",
        "processor = AutoImageProcessor.from_pretrained(\"timm/swin_base_patch4_window12_384.ms_in22k\")\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n",
        "\n",
        "# Verify GPU memory usage\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMCwHTEijO7A"
      },
      "source": [
        "## Explore Dataset Structure\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3U_es2aFjO7B",
        "outputId": "4ca4465b-f419-4cbe-af83-54cada316a5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset contents:\n",
            "  full_emoji.csv\n",
            "  image\n",
            "\n",
            "Subdirectories found:\n",
            "  image\n"
          ]
        }
      ],
      "source": [
        "# Explore the dataset directory structure\n",
        "dataset_path = Path(path)\n",
        "print(\"Dataset contents:\")\n",
        "for item in sorted(dataset_path.iterdir()):\n",
        "    print(f\"  {item.name}\")\n",
        "\n",
        "# Check for subdirectories\n",
        "if dataset_path.is_dir():\n",
        "    subdirs = [d for d in dataset_path.iterdir() if d.is_dir()]\n",
        "    if subdirs:\n",
        "        print(\"\\nSubdirectories found:\")\n",
        "        for subdir in subdirs[:5]:  # Show first 5\n",
        "            print(f\"  {subdir.name}\")\n",
        "            # Check if it contains images\n",
        "            images = list(subdir.glob(\"*.png\")) + list(subdir.glob(\"*.jpg\")) + list(subdir.glob(\"*.jpeg\"))\n",
        "            if images:\n",
        "                print(f\"    - Contains {len(images)} images\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUnMxWfDjO7B"
      },
      "source": [
        "## Define Vendor Classes and Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWVOFyusjO7B",
        "outputId": "f4f8d0ce-8766-48e2-c334-c7deded8603c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of vendor classes: 11\n",
            "Vendor classes: ['Apple', 'DoCoMo', 'Facebook', 'Gmail', 'Google', 'JoyPixels', 'KDDI', 'Samsung', 'SoftBank', 'Twitter', 'Windows']\n"
          ]
        }
      ],
      "source": [
        "# Define vendor classes\n",
        "VENDOR_CLASSES = [\n",
        "    \"Apple\",\n",
        "    \"DoCoMo\",\n",
        "    \"Facebook\",\n",
        "    \"Gmail\",\n",
        "    \"Google\",\n",
        "    \"JoyPixels\",\n",
        "    \"KDDI\",\n",
        "    \"Samsung\",\n",
        "    \"SoftBank\",\n",
        "    \"Twitter\",\n",
        "    \"Windows\"\n",
        "]\n",
        "\n",
        "VENDOR_TO_IDX = {vendor: idx for idx, vendor in enumerate(VENDOR_CLASSES)}\n",
        "IDX_TO_VENDOR = {idx: vendor for vendor, idx in VENDOR_TO_IDX.items()}\n",
        "\n",
        "print(f\"Number of vendor classes: {len(VENDOR_CLASSES)}\")\n",
        "print(\"Vendor classes:\", VENDOR_CLASSES)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lIvj79jajO7B"
      },
      "outputs": [],
      "source": [
        "class EmojiDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, processor, transform=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.labels = labels\n",
        "        self.processor = processor\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load image\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            # Return a blank image if loading fails\n",
        "            image = Image.new('RGB', (384, 384), color='white')\n",
        "\n",
        "        # Process image with the processor\n",
        "        inputs = self.processor(image, return_tensors=\"pt\")\n",
        "        pixel_values = inputs['pixel_values'].squeeze(0)  # Remove batch dimension\n",
        "\n",
        "        return {\n",
        "            'pixel_values': pixel_values,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBLKzBp3jO7C"
      },
      "source": [
        "## Prepare Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cq4S38HgjO7C",
        "outputId": "f7ddf453-8055-41c4-b0f0-2b65da2fbae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 14253 images\n",
            "Labels distribution: [1813  251 1727  720 1816 1816  637 1724  476 1816 1457]\n",
            "\n",
            "Sample image paths:\n",
            "  /root/.cache/kagglehub/datasets/subinium/emojiimage-dataset/versions/2/image/DoCoMo/1513.png -> DoCoMo\n",
            "  /root/.cache/kagglehub/datasets/subinium/emojiimage-dataset/versions/2/image/DoCoMo/1063.png -> DoCoMo\n",
            "  /root/.cache/kagglehub/datasets/subinium/emojiimage-dataset/versions/2/image/DoCoMo/94.png -> DoCoMo\n",
            "  /root/.cache/kagglehub/datasets/subinium/emojiimage-dataset/versions/2/image/DoCoMo/1473.png -> DoCoMo\n",
            "  /root/.cache/kagglehub/datasets/subinium/emojiimage-dataset/versions/2/image/DoCoMo/126.png -> DoCoMo\n"
          ]
        }
      ],
      "source": [
        "def prepare_dataset(dataset_path):\n",
        "    \"\"\"Prepare dataset by finding all images and their corresponding vendor labels.\"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "\n",
        "    dataset_path = Path(dataset_path)\n",
        "\n",
        "    # Common image extensions\n",
        "    image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
        "\n",
        "    # Strategy 1: Check if vendor names are in directory names\n",
        "    for vendor in VENDOR_CLASSES:\n",
        "        vendor_dir = dataset_path / vendor\n",
        "        if vendor_dir.exists() and vendor_dir.is_dir():\n",
        "            for ext in image_extensions:\n",
        "                images = list(vendor_dir.glob(f\"*{ext}\"))\n",
        "                for img_path in images:\n",
        "                    image_paths.append(str(img_path))\n",
        "                    labels.append(VENDOR_TO_IDX[vendor])\n",
        "\n",
        "    # Strategy 2: Check if vendor names are in filenames\n",
        "    if len(image_paths) == 0:\n",
        "        for ext in image_extensions:\n",
        "            all_images = list(dataset_path.rglob(f\"*{ext}\"))\n",
        "            for img_path in all_images:\n",
        "                filename = img_path.name.lower()\n",
        "                for vendor in VENDOR_CLASSES:\n",
        "                    if vendor.lower() in filename or vendor.lower() in str(img_path.parent).lower():\n",
        "                        image_paths.append(str(img_path))\n",
        "                        labels.append(VENDOR_TO_IDX[vendor])\n",
        "                        break\n",
        "\n",
        "    # Strategy 3: If still empty, check all subdirectories\n",
        "    if len(image_paths) == 0:\n",
        "        for subdir in dataset_path.iterdir():\n",
        "            if subdir.is_dir():\n",
        "                vendor_name = subdir.name\n",
        "                # Try to match vendor name\n",
        "                matched = False\n",
        "                for vendor in VENDOR_CLASSES:\n",
        "                    if vendor.lower() in vendor_name.lower() or vendor_name.lower() in vendor.lower():\n",
        "                        for ext in image_extensions:\n",
        "                            images = list(subdir.glob(f\"*{ext}\"))\n",
        "                            for img_path in images:\n",
        "                                image_paths.append(str(img_path))\n",
        "                                labels.append(VENDOR_TO_IDX[vendor])\n",
        "                        matched = True\n",
        "                        break\n",
        "\n",
        "                # If no match, check if it's a nested structure\n",
        "                if not matched:\n",
        "                    for vendor in VENDOR_CLASSES:\n",
        "                        vendor_subdir = subdir / vendor\n",
        "                        if vendor_subdir.exists():\n",
        "                            for ext in image_extensions:\n",
        "                                images = list(vendor_subdir.glob(f\"*{ext}\"))\n",
        "                                for img_path in images:\n",
        "                                    image_paths.append(str(img_path))\n",
        "                                    labels.append(VENDOR_TO_IDX[vendor])\n",
        "\n",
        "    return image_paths, labels\n",
        "\n",
        "# Prepare dataset\n",
        "image_paths, labels = prepare_dataset(path)\n",
        "\n",
        "print(f\"Found {len(image_paths)} images\")\n",
        "print(f\"Labels distribution: {np.bincount(labels)}\")\n",
        "\n",
        "# Show some examples\n",
        "if len(image_paths) > 0:\n",
        "    print(\"\\nSample image paths:\")\n",
        "    for i in range(min(5, len(image_paths))):\n",
        "        print(f\"  {image_paths[i]} -> {IDX_TO_VENDOR[labels[i]]}\")\n",
        "else:\n",
        "    print(\"WARNING: No images found! Please check the dataset structure.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnDg9rmbjO7C"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5R-JGOujO7C",
        "outputId": "9e00e34c-355e-4713-ca44-02f64a3f47e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created and moved to cuda\n",
            "All parameters on GPU: True\n",
            "Total parameters: 86,891,907\n",
            "GPU Memory allocated: 0.17 GB\n",
            "GPU Memory reserved: 0.20 GB\n"
          ]
        }
      ],
      "source": [
        "class SwinForEmojiClassification(nn.Module):\n",
        "    def __init__(self, num_labels=len(VENDOR_CLASSES)):\n",
        "        super().__init__()\n",
        "        self.swin = model\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # Get the hidden size from the model config\n",
        "        # For TimmBackbone (used by AutoModel for timm models), access num_features from the wrapped timm_model\n",
        "        hidden_size = self.swin.timm_model.num_features\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_size),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        # Get embeddings from Swin\n",
        "        outputs = self.swin(pixel_values=pixel_values)\n",
        "\n",
        "        # Use pooler_output if available, otherwise use last_hidden_state mean\n",
        "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "            pooled_output = outputs.pooler_output\n",
        "        else:\n",
        "            # Mean pooling over sequence dimension\n",
        "            pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        return ImageClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits\n",
        "        )\n",
        "\n",
        "# Create the model (base model is already on GPU)\n",
        "classification_model = SwinForEmojiClassification(num_labels=len(VENDOR_CLASSES))\n",
        "\n",
        "# Ensure entire model is on GPU\n",
        "classification_model = classification_model.to(device)\n",
        "\n",
        "# Verify all parameters are on GPU\n",
        "all_on_gpu = all(p.device.type == 'cuda' for p in classification_model.parameters())\n",
        "print(f\"Model created and moved to {device}\")\n",
        "print(f\"All parameters on GPU: {all_on_gpu}\")\n",
        "print(f\"Total parameters: {sum(p.numel() for p in classification_model.parameters()):,}\")\n",
        "\n",
        "# GPU memory info\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
        "    print(f\"GPU Memory reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9rr2ZMtjO7C"
      },
      "source": [
        "## Split Dataset and Create DataLoaders\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8DmWYx7jO7C",
        "outputId": "7a3f42cf-8c70-4988-c1de-78c260d36599"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 11402\n",
            "Validation samples: 2851\n",
            "Batch size: 8\n",
            "Pin memory: True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Split dataset into train and validation\n",
        "if len(image_paths) > 0:\n",
        "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "        image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
        "    )\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = EmojiDataset(train_paths, train_labels, processor)\n",
        "    val_dataset = EmojiDataset(val_paths, val_labels, processor)\n",
        "\n",
        "    # Create data loaders with GPU optimizations\n",
        "    # Use larger batch size on GPU, pin_memory for faster GPU transfer\n",
        "    batch_size = 8 if torch.cuda.is_available() else 8 # Reduced batch size to 8\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=4 if torch.cuda.is_available() else 2,\n",
        "        pin_memory=torch.cuda.is_available(),  # Pin memory for faster GPU transfer\n",
        "        persistent_workers=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=4 if torch.cuda.is_available() else 2,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        persistent_workers=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    print(f\"Train samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation samples: {len(val_dataset)}\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Pin memory: {torch.cuda.is_available()}\")\n",
        "else:\n",
        "    print(\"ERROR: No images found. Cannot create data loaders.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no9SZHkNjO7D"
      },
      "source": [
        "## Training Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "029jARVvjO7D",
        "outputId": "527ee6b8-811f-4d22-910e-c81c1a2ffd52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mixed precision training: Enabled (bfloat16 without GradScaler)\n",
            "Training setup complete!\n"
          ]
        }
      ],
      "source": [
        "# Training parameters\n",
        "num_epochs = 5\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Optimizer and scheduler\n",
        "optimizer = torch.optim.AdamW(\n",
        "    classification_model.parameters(),\n",
        "    lr=learning_rate,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Learning rate scheduler\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "# Mixed precision scaler for GPU training\n",
        "# GradScaler is typically used for float16 to prevent underflow/overflow.\n",
        "# For bfloat16, which has a wider dynamic range, it's often not needed.\n",
        "scaler = None\n",
        "if torch.cuda.is_available():\n",
        "    # model_dtype is set in cell xJsll1CjjO7A\n",
        "    if model_dtype == torch.float16:\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "        print(\"Mixed precision training: Enabled (float16 with GradScaler)\")\n",
        "    elif model_dtype == torch.bfloat16:\n",
        "        # For bfloat16, we use autocast but often don't need a GradScaler.\n",
        "        # The `train_epoch` function will correctly use autocast but skip scaler steps.\n",
        "        print(\"Mixed precision training: Enabled (bfloat16 without GradScaler)\")\n",
        "    else:\n",
        "        print(\"Mixed precision training: Disabled (GPU, non-fp16/bf16 dtype)\")\n",
        "else:\n",
        "    print(\"Mixed precision training: Disabled (CPU)\")\n",
        "\n",
        "print(\"Training setup complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3-oTMGQjO7D"
      },
      "source": [
        "## Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6QwuBYUjO7D",
        "outputId": "0d55903f-c4da-4b74-c057-ffac4650be83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training on GPU...\n",
            "\n",
            "Epoch 1/5\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining:   0%|          | 0/1426 [00:00<?, ?it/s]/tmp/ipython-input-3768013970.py:19: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=is_cuda_available):\n",
            "Training: 100%|██████████| 1426/1426 [09:01<00:00,  2.63it/s, loss=0.0597, acc=60.28%]\n",
            "Validation:   0%|          | 0/357 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/tmp/ipython-input-3768013970.py:64: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=is_cuda_available):\n",
            "Validation: 100%|██████████| 357/357 [00:52<00:00,  6.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Loss: 1.1874, Train Acc: 60.28%\n",
            "Val Loss: 0.6162, Val Acc: 77.17%\n",
            "GPU Memory: 6.56 GB / 7.71 GB\n",
            "Saved best model with validation accuracy: 77.17%\n",
            "\n",
            "Epoch 2/5\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:  19%|█▉        | 273/1426 [01:45<07:22,  2.60it/s, loss=0.571, acc=76.79%]"
          ]
        }
      ],
      "source": [
        "def train_epoch(model, train_loader, optimizer, device, scaler=None):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Check if CUDA is available to enable autocast\n",
        "    is_cuda_available = (device.type == 'cuda')\n",
        "\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        # Move data to GPU (with pin_memory, this should be fast)\n",
        "        pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n",
        "        labels = batch['labels'].to(device, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Always use autocast if on CUDA, it handles casting inputs to model's precision\n",
        "        with torch.cuda.amp.autocast(enabled=is_cuda_available):\n",
        "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        if scaler is not None:\n",
        "            # For float16, use scaler\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            # For bfloat16 or CPU, standard backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.logits, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'loss': loss.item(),\n",
        "            'acc': f'{100 * correct / total:.2f}%'\n",
        "        })\n",
        "\n",
        "    # Clear GPU cache periodically\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return total_loss / len(train_loader), 100 * correct / total\n",
        "\n",
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Check if CUDA is available to enable autocast\n",
        "    is_cuda_available = (device.type == 'cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
        "            pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n",
        "            labels = batch['labels'].to(device, non_blocking=True)\n",
        "\n",
        "            # Always use autocast if on CUDA\n",
        "            with torch.cuda.amp.autocast(enabled=is_cuda_available):\n",
        "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return total_loss / len(val_loader), 100 * correct / total\n",
        "\n",
        "# Training\n",
        "if len(image_paths) > 0:\n",
        "    print(\"Starting training on GPU...\" if torch.cuda.is_available() else \"Starting training on CPU...\")\n",
        "    best_val_acc = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        train_loss, train_acc = train_epoch(classification_model, train_loader, optimizer, device, scaler)\n",
        "        val_loss, val_acc = validate(classification_model, val_loader, device)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "        # GPU memory info\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB / {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(classification_model.state_dict(), 'best_swin_emoji_model.pt')\n",
        "            print(f\"Saved best model with validation accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "    print(\"\\nTraining completed!\")\n",
        "else:\n",
        "    print(\"ERROR: Cannot train without data.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCedi_FOjO7E"
      },
      "source": [
        "## Evaluation and Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNzLnpTjjO7E"
      },
      "outputs": [],
      "source": [
        "# Load best model (ensure it's loaded to GPU)\n",
        "if os.path.exists('best_swin_emoji_model.pt'):\n",
        "    # Load state dict and ensure model is on GPU\n",
        "    state_dict = torch.load('best_swin_emoji_model.pt', map_location=device)\n",
        "    classification_model.load_state_dict(state_dict)\n",
        "    classification_model = classification_model.to(device)\n",
        "    print(\"Loaded best model for evaluation\")\n",
        "    print(f\"Model device: {next(classification_model.parameters()).device}\")\n",
        "\n",
        "# Final evaluation\n",
        "if len(image_paths) > 0:\n",
        "    val_loss, val_acc = validate(classification_model, val_loader, device)\n",
        "    print(f\"\\nFinal Validation Accuracy: {val_acc:.2f}%\")\n",
        "\n",
        "    # Per-class accuracy\n",
        "    classification_model.eval()\n",
        "    class_correct = [0] * len(VENDOR_CLASSES)\n",
        "    class_total = [0] * len(VENDOR_CLASSES)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = classification_model(pixel_values=pixel_values)\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "\n",
        "            for i in range(labels.size(0)):\n",
        "                label = labels[i].item()\n",
        "                class_total[label] += 1\n",
        "                if predicted[i] == labels[i]:\n",
        "                    class_correct[label] += 1\n",
        "\n",
        "    print(\"\\nPer-class accuracy:\")\n",
        "    for i, vendor in enumerate(VENDOR_CLASSES):\n",
        "        if class_total[i] > 0:\n",
        "            acc = 100 * class_correct[i] / class_total[i]\n",
        "            print(f\"  {vendor}: {acc:.2f}% ({class_correct[i]}/{class_total[i]})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2nd Dataset"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1196e7e30592407fbdce33c12eb8e0b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "131c51dae59c409a8c08e10b4a4d8999": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18e9ad54bbe447f1a1835c14eac595b3",
            "max": 450703408,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d8c1a278f7874e308046c57e9b302d89",
            "value": 450703408
          }
        },
        "18647aad44bc4b6dbdf37f771f50c27c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18e9ad54bbe447f1a1835c14eac595b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19b89d13cc724d889831ebd3a2a2abcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e8967965b644e71a9865bc957d6a374": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cc3d3c881e446028bf956b963a60d85",
            "placeholder": "​",
            "style": "IPY_MODEL_b882015593314371938ee85abb42fceb",
            "value": " 644/644 [00:00&lt;00:00, 65.9kB/s]"
          }
        },
        "2b82a4a1f28140fbb820de56a8c5fbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8156d9d52e284c14a2ca25b422dcb8b5",
              "IPY_MODEL_d8700343196e4c328590744664e0091f",
              "IPY_MODEL_1e8967965b644e71a9865bc957d6a374"
            ],
            "layout": "IPY_MODEL_ae9e989d060a45d880ca042f4db072f9"
          }
        },
        "35624a110d4d43b790d97b55bb133459": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a65d01649a9048368ce899760b7170cd",
              "IPY_MODEL_131c51dae59c409a8c08e10b4a4d8999",
              "IPY_MODEL_e0aba4e21af243ebbd1df38779524a9a"
            ],
            "layout": "IPY_MODEL_df8d5851b93b4816bb148499f15e47bd"
          }
        },
        "35a8c0cb5b8b44b7b5ed9e8e68ed419c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39577bde28f14abb87cbfb5f13cb1138": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8156d9d52e284c14a2ca25b422dcb8b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39577bde28f14abb87cbfb5f13cb1138",
            "placeholder": "​",
            "style": "IPY_MODEL_18647aad44bc4b6dbdf37f771f50c27c",
            "value": "config.json: 100%"
          }
        },
        "8cc3d3c881e446028bf956b963a60d85": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b615077840f4feb93cb709b08133444": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a65d01649a9048368ce899760b7170cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35a8c0cb5b8b44b7b5ed9e8e68ed419c",
            "placeholder": "​",
            "style": "IPY_MODEL_19b89d13cc724d889831ebd3a2a2abcf",
            "value": "model.safetensors: 100%"
          }
        },
        "ae9e989d060a45d880ca042f4db072f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1c1dd505bd045788d3762dd463bf5c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b882015593314371938ee85abb42fceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d8700343196e4c328590744664e0091f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b615077840f4feb93cb709b08133444",
            "max": 644,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b1c1dd505bd045788d3762dd463bf5c8",
            "value": 644
          }
        },
        "d8c1a278f7874e308046c57e9b302d89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df8d5851b93b4816bb148499f15e47bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0aba4e21af243ebbd1df38779524a9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e512b252867d454d86f7856ef4434b91",
            "placeholder": "​",
            "style": "IPY_MODEL_1196e7e30592407fbdce33c12eb8e0b9",
            "value": " 451M/451M [00:03&lt;00:00, 327MB/s]"
          }
        },
        "e512b252867d454d86f7856ef4434b91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
