{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V10 — Ensemble DINOv2 (base/large × seeds) + TTA Prob-Vector Features + Meta-Model\n",
    "\n",
    "**Goal:** maximize Kaggle leaderboard score with stacking on top of rich probability features.\n",
    "\n",
    "**Pipeline:**\n",
    "- Train ensemble DINOv2 members on 2nd dataset only\n",
    "- For each image, compute per-augmentation probability vectors (NUM_TTA_AUGS × 7 probs)\n",
    "- Concatenate with statistical features (incl. `original_mode`)\n",
    "- Train a meta-model: **LightGBM (preferred)**, fallback to **XGBoost**\n",
    "- Final fit on Train+Val, predict Test\n",
    "\n",
    "**Classes:** apple, google, whatsapp, facebook, samsung, mozilla, messenger\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except Exception as e:\n",
    "    HAS_LGB = False\n",
    "    print('LightGBM import failed:', e)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "SECOND_DATASET_BASE_PATH = '.'\n",
    "SECOND_DATASET_TRAIN_DIR = Path(SECOND_DATASET_BASE_PATH) / 'train'\n",
    "SECOND_DATASET_CSV_PATH = Path(SECOND_DATASET_BASE_PATH) / 'train_labels.csv'\n",
    "SECOND_DATASET_TEST_DIR = Path(SECOND_DATASET_BASE_PATH) / 'test'\n",
    "\n",
    "# Ensemble\n",
    "MODEL_IDS = ['facebook/dinov2-base']  # e.g. ['facebook/dinov2-base','facebook/dinov2-large']\n",
    "SEEDS = [42, 123, 456, 789]\n",
    "\n",
    "# Train\n",
    "VAL_SIZE = 0.10\n",
    "RANDOM_STATE = 42\n",
    "NUM_EPOCHS = 20  # Increased from 7, with early stopping after 3 epochs without improvement\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE_CUDA = 16\n",
    "BATCH_SIZE_CPU = 4\n",
    "NUM_WORKERS = 2\n",
    "FINAL_EPOCHS = 0\n",
    "FINAL_LR_MULT = 0.5\n",
    "\n",
    "# TTA\n",
    "NUM_TTA_AUGS = 10\n",
    "TQDM_MININTERVAL = 10\n",
    "SHOW_PROGRESS = True  # set False to reduce output\n",
    "FEATURE_BATCH_SIZE = 16  # Batch size for feature extraction (images per batch)\n",
    "\n",
    "# Meta-model\n",
    "USE_LIGHTGBM = True\n",
    "LGB_PARAMS = {\n",
    "    'n_estimators': 800,\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 63,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "XGB_PARAMS = {\n",
    "    'n_estimators': 800,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.03,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "# Output\n",
    "PREDICTIONS_OUTPUT_FILE = 'predictions_V10.csv'\n",
    "\n",
    "print('Ensemble size:', len(MODEL_IDS) * len(SEEDS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENDOR_CLASSES = ['apple','google','whatsapp','facebook','samsung','mozilla','messenger']\n",
    "VENDOR_TO_IDX = {v:i for i,v in enumerate(VENDOR_CLASSES)}\n",
    "IDX_TO_VENDOR = {i:v for v,i in VENDOR_TO_IDX.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Augmentation (Predictable TTA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical features (incl. original_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_properties(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        mode_mapping = {'L':0,'LA':1,'P':2,'RGB':3,'RGBA':4}\n",
    "        original_mode = float(mode_mapping.get(img.mode, 3))\n",
    "        # Normalize image to RGB for pixel stats\n",
    "        if img.mode == 'P':\n",
    "            img = img.convert('RGBA')\n",
    "        if img.mode == 'RGBA':\n",
    "            bg = Image.new('RGB', img.size, (255,255,255))\n",
    "            bg.paste(img, mask=img.split()[3])\n",
    "            img = bg\n",
    "        elif img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        w,h = img.size\n",
    "        ar = w / h if h else 0.0\n",
    "        pix = float(w*h)\n",
    "        arr = np.array(img)\n",
    "        mean_r = float(arr[:,:,0].mean()); mean_g = float(arr[:,:,1].mean()); mean_b = float(arr[:,:,2].mean())\n",
    "        std_r = float(arr[:,:,0].std());  std_g  = float(arr[:,:,1].std());  std_b  = float(arr[:,:,2].std())\n",
    "        brightness = float((mean_r+mean_g+mean_b)/3.0)\n",
    "        is_mostly_white = float(brightness > 200)\n",
    "        return {\n",
    "            'width': float(w), 'height': float(h), 'aspect_ratio': float(ar), 'pixel_count': pix,\n",
    "            'mean_r': mean_r, 'mean_g': mean_g, 'mean_b': mean_b,\n",
    "            'std_r': std_r, 'std_g': std_g, 'std_b': std_b,\n",
    "            'brightness': brightness, 'is_mostly_white': is_mostly_white,\n",
    "            'original_mode': original_mode\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'width':224.0,'height':224.0,'aspect_ratio':1.0,'pixel_count':50176.0,\n",
    "            'mean_r':128.0,'mean_g':128.0,'mean_b':128.0,\n",
    "            'std_r':50.0,'std_g':50.0,'std_b':50.0,\n",
    "            'brightness':128.0,'is_mostly_white':0.0,'original_mode':3.0\n",
    "        }\n",
    "\n",
    "STAT_COLS = ['width','height','aspect_ratio','pixel_count','mean_r','mean_g','mean_b','std_r','std_g','std_b','brightness','is_mostly_white','original_mode']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicAugmentation:\n",
    "    def __init__(self, image_size=224, seed=42):\n",
    "        self.image_size = image_size\n",
    "        self.seed = seed\n",
    "        self.rotation_angles = [-10, -5, 5, 10]\n",
    "        self.crop_ratios = [0.75, 0.85, 0.9, 0.95]\n",
    "        self.color_jitter_params = {'brightness':0.3,'contrast':0.3,'saturation':0.3,'hue':0.1}\n",
    "        self.translate_range = (0.1, 0.1)\n",
    "        self.blur_sigma = (0.1, 0.5)\n",
    "\n",
    "    def _get_deterministic_seed(self, image_or_hash):\n",
    "        if isinstance(image_or_hash, Image.Image):\n",
    "            img_bytes = image_or_hash.tobytes()\n",
    "            return int(hashlib.md5(img_bytes).hexdigest()[:8], 16)\n",
    "        return hash(str(image_or_hash)) & 0xFFFFFFFF\n",
    "\n",
    "    def horizontal_flip(self, image):\n",
    "        return F.hflip(image)\n",
    "\n",
    "    def rotation(self, image, angle):\n",
    "        return F.rotate(image, angle)\n",
    "\n",
    "    def center_crop(self, image, crop_ratio=0.9):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        return F.center_crop(image, [crop,crop])\n",
    "\n",
    "    def corner_crop(self, image, crop_ratio=0.9, position='tl'):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        if position=='tl': return F.crop(image, 0, 0, crop, crop)\n",
    "        if position=='tr': return F.crop(image, 0, w-crop, crop, crop)\n",
    "        if position=='bl': return F.crop(image, h-crop, 0, crop, crop)\n",
    "        if position=='br': return F.crop(image, h-crop, w-crop, crop, crop)\n",
    "        return image\n",
    "\n",
    "    def resized_crop(self, image, crop_ratio=0.85):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        cropped = F.center_crop(image, [crop,crop])\n",
    "        return cropped.resize((self.image_size,self.image_size), Image.BILINEAR)\n",
    "\n",
    "    def color_jitter(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        b = 1.0 + np.random.uniform(-self.color_jitter_params['brightness'], self.color_jitter_params['brightness'])\n",
    "        c = 1.0 + np.random.uniform(-self.color_jitter_params['contrast'], self.color_jitter_params['contrast'])\n",
    "        s = 1.0 + np.random.uniform(-self.color_jitter_params['saturation'], self.color_jitter_params['saturation'])\n",
    "        h = np.random.uniform(-self.color_jitter_params['hue'], self.color_jitter_params['hue'])\n",
    "        img = F.adjust_brightness(image, b)\n",
    "        img = F.adjust_contrast(img, c)\n",
    "        img = F.adjust_saturation(img, s)\n",
    "        img = F.adjust_hue(img, h)\n",
    "        return img\n",
    "\n",
    "    def affine_transform(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        tx = np.random.uniform(-self.translate_range[0], self.translate_range[0])\n",
    "        ty = np.random.uniform(-self.translate_range[1], self.translate_range[1])\n",
    "        return F.affine(image, angle=0, translate=(tx*image.width, ty*image.height), scale=1.0, shear=0.0)\n",
    "\n",
    "    def gaussian_blur(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        sigma = np.random.uniform(self.blur_sigma[0], self.blur_sigma[1])\n",
    "        return F.gaussian_blur(image, kernel_size=3, sigma=[sigma,sigma])\n",
    "\n",
    "    def get_augmentations(self, image, num_augmentations=10, seed_source=None):\n",
    "        if seed_source is None:\n",
    "            seed_val = self._get_deterministic_seed(image)\n",
    "        elif isinstance(seed_source, str):\n",
    "            # Convert string (e.g., image path) to integer seed\n",
    "            seed_val = self._get_deterministic_seed(seed_source)\n",
    "        else:\n",
    "            # Already an integer\n",
    "            seed_val = int(seed_source)\n",
    "        augs = []\n",
    "        augs.append(image.resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        augs.append(self.horizontal_flip(image).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        for angle in self.rotation_angles[:max(0, min(4, num_augmentations-len(augs)))]:\n",
    "            augs.append(self.rotation(image, angle).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        corners = ['tl','tr','bl','br']\n",
    "        for cpos in corners[:max(0, min(4, num_augmentations-len(augs)))]:\n",
    "            augs.append(self.corner_crop(image, 0.9, cpos).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.center_crop(image, 0.9).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.resized_crop(image, 0.85))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.color_jitter(image, seed_val).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.affine_transform(image, seed_val+1).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.gaussian_blur(image, seed_val+2).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        return augs[:num_augmentations]\n",
    "    \n",
    "    def apply_training_augmentation(self, image, seed_source=None):\n",
    "        \"\"\"\n",
    "        Apply deterministic training augmentation (same methods as TTA).\n",
    "        Deterministic based on image path/index for reproducibility.\n",
    "        \"\"\"\n",
    "        if seed_source is None:\n",
    "            seed_val = self._get_deterministic_seed(image)\n",
    "        elif isinstance(seed_source, str):\n",
    "            # Convert string (e.g., image path) to integer seed\n",
    "            seed_val = self._get_deterministic_seed(seed_source)\n",
    "        else:\n",
    "            # Already an integer\n",
    "            seed_val = int(seed_source)\n",
    "        \n",
    "        # Use seed to deterministically select augmentations\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        \n",
    "        # Horizontal flip (50% probability, deterministic)\n",
    "        if (seed_val % 2 == 0):\n",
    "            image = self.horizontal_flip(image)\n",
    "        \n",
    "        # Rotation (deterministic angle selection)\n",
    "        angle_idx = (seed_val // 2) % len(self.rotation_angles)\n",
    "        angle = self.rotation_angles[angle_idx]\n",
    "        image = self.rotation(image, angle)\n",
    "        \n",
    "        # Crop (deterministic crop ratio)\n",
    "        crop_idx = (seed_val // 10) % len(self.crop_ratios)\n",
    "        crop_ratio = self.crop_ratios[crop_idx]\n",
    "        w, h = image.size\n",
    "        crop_size = int(min(w, h) * crop_ratio)\n",
    "        image = F.center_crop(image, [crop_size, crop_size])\n",
    "        \n",
    "        # Color jitter (deterministic)\n",
    "        image = self.color_jitter(image, seed_val)\n",
    "        \n",
    "        # Affine transform (50% probability, deterministic)\n",
    "        if (seed_val // 3) % 2 == 0:\n",
    "            image = self.affine_transform(image, seed_val + 1)\n",
    "        \n",
    "        # Gaussian blur (20% probability, deterministic)\n",
    "        if (seed_val // 5) % 5 == 0:\n",
    "            image = self.gaussian_blur(image, seed_val + 2)\n",
    "        \n",
    "        # Resize to final size\n",
    "        image = image.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        \n",
    "        return image\n",
    "\n",
    "tta_aug = DeterministicAugmentation(image_size=224, seed=42)\n",
    "print('Deterministic augmentation ready.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading (CSV labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_from_csv(train_dir, csv_path):\n",
    "    train_dir = Path(train_dir); csv_path = Path(csv_path)\n",
    "    if not train_dir.exists() or not csv_path.exists():\n",
    "        raise FileNotFoundError(f'Missing train_dir or csv: {train_dir} / {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    label_map = {v: VENDOR_TO_IDX[v] for v in VENDOR_CLASSES}\n",
    "    img_paths=[]; labels=[]\n",
    "    missing=0; unmapped=0\n",
    "    for _, r in df.iterrows():\n",
    "        img_id = str(r['Id']).zfill(5)\n",
    "        lab = str(r['Label']).lower()\n",
    "        if lab not in label_map:\n",
    "            unmapped += 1\n",
    "            continue\n",
    "        found = None\n",
    "        for ext in ('.png','.jpg','.jpeg','.PNG','.JPG','.JPEG'):\n",
    "            p = train_dir / f'{img_id}{ext}'\n",
    "            if p.exists():\n",
    "                found = str(p)\n",
    "                break\n",
    "        if found is None:\n",
    "            missing += 1\n",
    "            continue\n",
    "        img_paths.append(found)\n",
    "        labels.append(int(label_map[lab]))\n",
    "    print('Loaded:', len(img_paths), 'images')\n",
    "    print('Unmapped labels skipped:', unmapped, 'Missing files skipped:', missing)\n",
    "    if labels:\n",
    "        print('Label distribution:', np.bincount(np.array(labels), minlength=len(VENDOR_CLASSES)))\n",
    "    return img_paths, labels\n",
    "\n",
    "all_paths, all_labels = prepare_dataset_from_csv(SECOND_DATASET_TRAIN_DIR, SECOND_DATASET_CSV_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split (stratified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = np.bincount(np.array(all_labels), minlength=len(VENDOR_CLASSES)).min()\n",
    "can_stratify = (min_count >= 2)\n",
    "print('Min class count:', int(min_count), 'Stratify:', can_stratify)\n",
    "train_paths, val_paths, train_y, val_y = train_test_split(\n",
    "    all_paths, all_labels, test_size=VAL_SIZE, random_state=RANDOM_STATE,\n",
    "    stratify=all_labels if can_stratify else None\n",
    ")\n",
    "print('Train:', len(train_paths), 'Val:', len(val_paths))\n",
    "print('Train dist:', np.bincount(np.array(train_y), minlength=len(VENDOR_CLASSES)))\n",
    "print('Val   dist:', np.bincount(np.array(val_y), minlength=len(VENDOR_CLASSES)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_rgb(path):\n",
    "    img = Image.open(path)\n",
    "    if img.mode == 'P':\n",
    "        img = img.convert('RGBA')\n",
    "    if img.mode == 'RGBA':\n",
    "        bg = Image.new('RGB', img.size, (255,255,255))\n",
    "        bg.paste(img, mask=img.split()[3])\n",
    "        img = bg\n",
    "    elif img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, processor, use_augmentation=False):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.labels = list(labels)\n",
    "        self.processor = processor\n",
    "        self.use_augmentation = use_augmentation\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_paths[idx]\n",
    "        y = int(self.labels[idx])\n",
    "        img = load_image_rgb(p)\n",
    "        \n",
    "        # Apply training augmentation if enabled (deterministic based on image path)\n",
    "        if self.use_augmentation:\n",
    "            img = tta_aug.apply_training_augmentation(img, seed_source=str(p))\n",
    "        \n",
    "        inputs = self.processor(img, return_tensors='pt')\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        y = int(max(0, min(y, len(VENDOR_CLASSES)-1)))\n",
    "        return {'pixel_values': pixel_values, 'labels': torch.tensor(y, dtype=torch.long)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model wrapper (DINOv2 backbone → 7 classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv2ForEmojiClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_labels = num_labels\n",
    "        hidden = getattr(base_model.config, 'hidden_size', 1024)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden//2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden//2, num_labels)\n",
    "        )\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        out = self.base_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "        if hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
    "            pooled = out.pooler_output\n",
    "        else:\n",
    "            pooled = out.hidden_states[-1][:,0,:]\n",
    "        logits = self.classifier(pooled)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "            loss = nn.CrossEntropyLoss(label_smoothing=0.05)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DINOv2ForEmojiClassification(nn.Module):\n",
    "#     def __init__(self, base_model, num_labels):\n",
    "#         super().__init__()\n",
    "#         self.base_model = base_model\n",
    "#         self.num_labels = num_labels\n",
    "#         hidden = getattr(base_model.config, 'hidden_size', 1024)\n",
    "        \n",
    "#         # Simpler head: less capacity, less overfitting risk\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.LayerNorm(hidden),\n",
    "#             nn.Dropout(0.1),  # Lower dropout\n",
    "#             nn.Linear(hidden, num_labels)  # Direct projection\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, pixel_values, labels=None):\n",
    "#         out = self.base_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "#         if hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
    "#             pooled = out.pooler_output\n",
    "#         else:\n",
    "#             pooled = out.hidden_states[-1][:,0,:]\n",
    "#         logits = self.classifier(pooled)\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "#             loss = nn.CrossEntropyLoss(label_smoothing=0.1)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "#         return ImageClassifierOutput(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Validate loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def make_loaders(processor):\n",
    "    train_ds = EmojiDataset(train_paths, train_y, processor, use_augmentation=True)  # Enable augmentation for training\n",
    "    val_ds = EmojiDataset(val_paths, val_y, processor, use_augmentation=False)  # No augmentation for validation\n",
    "    batch_size = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss=0.0; correct=0; total=0\n",
    "    use_amp = (device.type=='cuda')\n",
    "    for batch in tqdm(loader, desc='Training', mininterval=TQDM_MININTERVAL):\n",
    "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        y = batch['labels'].to(device, non_blocking=True)\n",
    "        y = torch.clamp(y, 0, model.num_labels-1)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(pixel_values=x, labels=y)\n",
    "            loss = out.loss\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += float(loss.item())\n",
    "        pred = torch.argmax(out.logits, dim=1)\n",
    "        correct += int((pred==y).sum().item())\n",
    "        total += int(y.size(0))\n",
    "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss=0.0; correct=0; total=0\n",
    "    preds=[]; labels=[]\n",
    "    use_amp = (device.type=='cuda')\n",
    "    for batch in tqdm(loader, desc='Validation', mininterval=TQDM_MININTERVAL):\n",
    "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        y = batch['labels'].to(device, non_blocking=True)\n",
    "        y = torch.clamp(y, 0, model.num_labels-1)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(pixel_values=x, labels=y)\n",
    "            loss = out.loss\n",
    "        total_loss += float(loss.item())\n",
    "        pred = torch.argmax(out.logits, dim=1)\n",
    "        pred = torch.clamp(pred, 0, model.num_labels-1)\n",
    "        correct += int((pred==y).sum().item())\n",
    "        total += int(y.size(0))\n",
    "        preds.extend(pred.cpu().numpy().tolist())\n",
    "        labels.extend(y.cpu().numpy().tolist())\n",
    "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total), preds, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ensemble members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_member(model_id, seed):\n",
    "    seed_everything(seed)\n",
    "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "    backbone = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
    "    model = DINOv2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
    "    train_loader, val_loader = make_loaders(processor)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, min_lr=1e-7, cooldown=1)\n",
    "    scaler = None\n",
    "    if torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()):\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    best_acc = -1.0\n",
    "    best_path = f'best_{model_id.split(\"/\")[-1]}_seed{seed}.pt'\n",
    "    early_stopping_patience = 3\n",
    "    early_stopping_counter = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f'\\n[{model_id}] seed={seed} epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, device, scaler)\n",
    "        va_loss, va_acc, va_pred, va_true = validate(model, val_loader, device)\n",
    "        scheduler.step(va_acc)\n",
    "        print(f'Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% | Val: loss={va_loss:.4f} acc={va_acc:.2f}%')\n",
    "        if va_acc > best_acc:\n",
    "            best_acc = va_acc\n",
    "            early_stopping_counter = 0  # Reset counter on improvement\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print('✓ saved', best_path)\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(f'Early stopping triggered after {epoch+1} epochs (no improvement for {early_stopping_patience} epochs)')\n",
    "                break\n",
    "    # reload best\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    print('Best val acc:', best_acc)\n",
    "    print('Val report:')\n",
    "    print(classification_report(va_true, va_pred, target_names=VENDOR_CLASSES))\n",
    "    # optional final fit on combined\n",
    "    if FINAL_EPOCHS and FINAL_EPOCHS > 0:\n",
    "        combined_ds = EmojiDataset(train_paths + val_paths, train_y + val_y, processor)\n",
    "        bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
    "        combined_loader = DataLoader(combined_ds, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "        opt2 = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE*FINAL_LR_MULT, weight_decay=0.01)\n",
    "        for ep in range(FINAL_EPOCHS):\n",
    "            print(f'Final fit epoch {ep+1}/{FINAL_EPOCHS}')\n",
    "            tr_loss2, tr_acc2 = train_epoch(model, combined_loader, opt2, device, scaler)\n",
    "            print(f'Final fit: loss={tr_loss2:.4f} acc={tr_acc2:.2f}%')\n",
    "        final_path = best_path.replace('.pt','_final.pt')\n",
    "        torch.save(model.state_dict(), final_path)\n",
    "        best_path = final_path\n",
    "        print('✓ saved', best_path)\n",
    "    return best_path\n",
    "\n",
    "member_ckpts = []\n",
    "for mid in MODEL_IDS:\n",
    "    for seed in SEEDS:\n",
    "        member_ckpts.append(train_one_member(mid, seed))\n",
    "\n",
    "print('Ensemble checkpoints:')\n",
    "for p in member_ckpts:\n",
    "    print(' -', p)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load member function (for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_member(model_id, ckpt_path):\n",
    "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "    backbone = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
    "    model = DINOv2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA probabilities per augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba_tta_per_aug(model, processor, image, num_augmentations, device):\n",
    "    \"\"\"Batched TTA prediction - processes all augmentations at once for speed\"\"\"\n",
    "    model.eval()\n",
    "    augs = tta_aug.get_augmentations(image, num_augmentations=num_augmentations)\n",
    "    # Batch process all augmentations at once (much faster!)\n",
    "    inputs = processor(augs, return_tensors='pt')\n",
    "    x = inputs['pixel_values'].to(device)\n",
    "    out = model(pixel_values=x)\n",
    "    probs = torch.softmax(out.logits, dim=-1)  # Shape: (num_augmentations, num_classes)\n",
    "    return probs.detach().cpu().numpy()  # (A, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature matrix: stats + (A×C) prob vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_row(image_path, members, num_augmentations):\n",
    "    stats = extract_image_properties(image_path)\n",
    "    img = load_image_rgb(image_path)\n",
    "    # ensemble-average per-aug probs\n",
    "    probs_ens = None\n",
    "    for m, proc in members:\n",
    "        pa = predict_proba_tta_per_aug(m, proc, img, num_augmentations, device)\n",
    "        probs_ens = pa if probs_ens is None else (probs_ens + pa)\n",
    "    probs_ens = probs_ens / float(len(members))\n",
    "    row = dict(stats)\n",
    "    # flatten probs as prob_aug{i}_cls{c}\n",
    "    for i in range(num_augmentations):\n",
    "        for c in range(len(VENDOR_CLASSES)):\n",
    "            row[f'prob_aug{i}_cls{c}'] = float(probs_ens[i, c])\n",
    "    return row\n",
    "\n",
    "def build_features(image_paths, members, num_augmentations):\n",
    "    it = tqdm(image_paths, desc='Building features', mininterval=TQDM_MININTERVAL, disable=(not SHOW_PROGRESS))\n",
    "    rows = [build_feature_row(p, members, num_augmentations) for p in it]\n",
    "    df = pd.DataFrame(rows)\n",
    "    prob_cols = [f'prob_aug{i}_cls{c}' for i in range(num_augmentations) for c in range(len(VENDOR_CLASSES))]\n",
    "    all_cols = STAT_COLS + prob_cols\n",
    "    for col in all_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    return df[all_cols]\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_features_batched(image_paths, members, num_augmentations, batch_size=None):\n",
    "    \"\"\"\n",
    "    Batched version - processes multiple images at once for much faster feature extraction.\n",
    "    Processes: batch_size images × num_augmentations augs × num_members models\n",
    "    \"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = FEATURE_BATCH_SIZE\n",
    "    \n",
    "    all_rows = []\n",
    "    num_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Process images in batches\n",
    "    for batch_idx in tqdm(range(0, len(image_paths), batch_size), \n",
    "                          desc='Building features (batched)', \n",
    "                          mininterval=TQDM_MININTERVAL, \n",
    "                          disable=(not SHOW_PROGRESS),\n",
    "                          total=num_batches):\n",
    "        batch_paths = image_paths[batch_idx:batch_idx + batch_size]\n",
    "        batch_size_actual = len(batch_paths)\n",
    "        \n",
    "        # Extract stats for batch\n",
    "        batch_stats = [extract_image_properties(p) for p in batch_paths]\n",
    "        batch_imgs = [load_image_rgb(p) for p in batch_paths]\n",
    "        \n",
    "        # Ensemble-average per-aug probs\n",
    "        probs_ens = None\n",
    "        for m, proc in members:\n",
    "            # Collect all augmentations for all images in batch\n",
    "            batch_augs = []\n",
    "            for img in batch_imgs:\n",
    "                augs = tta_aug.get_augmentations(img, num_augmentations=num_augmentations)\n",
    "                batch_augs.extend(augs)\n",
    "            \n",
    "            # Batch process: (batch_size_actual * num_augmentations, ...)\n",
    "            inputs = proc(batch_augs, return_tensors='pt')\n",
    "            x = inputs['pixel_values'].to(device)\n",
    "            m.eval()\n",
    "            out = m(pixel_values=x)\n",
    "            probs = torch.softmax(out.logits, dim=-1)  # (batch_size_actual * num_augmentations, num_classes)\n",
    "            \n",
    "            # Reshape: (batch_size_actual, num_augmentations, num_classes)\n",
    "            probs = probs.view(batch_size_actual, num_augmentations, -1)\n",
    "            \n",
    "            if probs_ens is None:\n",
    "                probs_ens = probs\n",
    "            else:\n",
    "                probs_ens = probs_ens + probs\n",
    "        \n",
    "        # Average across ensemble members\n",
    "        probs_ens = probs_ens / float(len(members))\n",
    "        probs_ens = probs_ens.cpu().numpy()\n",
    "        \n",
    "        # Build rows for this batch\n",
    "        for i, (stats, probs_img) in enumerate(zip(batch_stats, probs_ens)):\n",
    "            row = dict(stats)\n",
    "            for aug_idx in range(num_augmentations):\n",
    "                for cls_idx in range(len(VENDOR_CLASSES)):\n",
    "                    row[f'prob_aug{aug_idx}_cls{cls_idx}'] = float(probs_img[aug_idx, cls_idx])\n",
    "            all_rows.append(row)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    prob_cols = [f'prob_aug{i}_cls{c}' for i in range(num_augmentations) for c in range(len(VENDOR_CLASSES))]\n",
    "    all_cols = STAT_COLS + prob_cols\n",
    "    for col in all_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    return df[all_cols]\n",
    "\n",
    "\n",
    "print('Total features:', len(STAT_COLS) + NUM_TTA_AUGS*len(VENDOR_CLASSES))\n",
    "print('Using batched feature extraction with batch_size:', FEATURE_BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train meta-model (LightGBM preferred, XGBoost fallback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all members\n",
    "members = []\n",
    "for ckpt in member_ckpts:\n",
    "    model_name = ckpt.split('_')[1]\n",
    "    model_id = None\n",
    "    for mid in MODEL_IDS:\n",
    "        if mid.split('/')[-1] == model_name:\n",
    "            model_id = mid\n",
    "            break\n",
    "    if model_id is None:\n",
    "        model_id = MODEL_IDS[0]\n",
    "    m, proc = load_member(model_id, ckpt)\n",
    "    members.append((m, proc))\n",
    "print('Loaded members:', len(members))\n",
    "\n",
    "X_train = build_features_batched(train_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "X_val = build_features_batched(val_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "y_train = np.array(train_y)\n",
    "y_val = np.array(val_y)\n",
    "print('X_train:', X_train.shape, 'X_val:', X_val.shape)\n",
    "\n",
    "if USE_LIGHTGBM and HAS_LGB:\n",
    "    meta = lgb.LGBMClassifier(**LGB_PARAMS)\n",
    "    meta.fit(X_train, y_train)\n",
    "    val_pred = meta.predict(X_val)\n",
    "    print('Meta(LGB) val acc:', accuracy_score(y_val, val_pred)*100.0)\n",
    "    print(classification_report(y_val, val_pred, target_names=VENDOR_CLASSES))\n",
    "    meta.booster_.save_model('meta_lgb_v10.txt')\n",
    "    print('Saved meta_lgb_v10.txt')\n",
    "else:\n",
    "    meta = xgb.XGBClassifier(**XGB_PARAMS)\n",
    "    meta.fit(X_train, y_train, verbose=False)\n",
    "    val_pred = meta.predict(X_val)\n",
    "    print('Meta(XGB) val acc:', accuracy_score(y_val, val_pred)*100.0)\n",
    "    print(classification_report(y_val, val_pred, target_names=VENDOR_CLASSES))\n",
    "    # avoid sklearn save_model bug\n",
    "    meta.get_booster().save_model('meta_xgb_v10.json')\n",
    "    print('Saved meta_xgb_v10.json')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final fit on Train+Val and predict Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_paths = train_paths + val_paths\n",
    "combined_y = train_y + val_y\n",
    "\n",
    "X_all = build_features_batched(combined_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "y_all = np.array(combined_y)\n",
    "\n",
    "if USE_LIGHTGBM and HAS_LGB:\n",
    "    meta_final = lgb.LGBMClassifier(**LGB_PARAMS)\n",
    "    meta_final.fit(X_all, y_all)\n",
    "    meta_final.booster_.save_model('meta_lgb_v10_final.txt')\n",
    "    print('Saved meta_lgb_v10_final.txt')\n",
    "else:\n",
    "    meta_final = xgb.XGBClassifier(**XGB_PARAMS)\n",
    "    meta_final.fit(X_all, y_all, verbose=False)\n",
    "    meta_final.get_booster().save_model('meta_xgb_v10_final.json')\n",
    "    print('Saved meta_xgb_v10_final.json')\n",
    "\n",
    "test_dir = SECOND_DATASET_TEST_DIR\n",
    "if not test_dir.exists():\n",
    "    raise FileNotFoundError(f'Missing test dir: {test_dir}')\n",
    "test_paths = []\n",
    "for ext in ('.png','.jpg','.jpeg','.PNG','.JPG','.JPEG'):\n",
    "    test_paths += [str(p) for p in test_dir.rglob(f'*{ext}')]\n",
    "test_paths = sorted(set(test_paths))\n",
    "print('Found test images:', len(test_paths))\n",
    "\n",
    "pred_ids = []\n",
    "pred_labels = []\n",
    "\n",
    "# Process test images in batches for faster inference\n",
    "test_batch_size = FEATURE_BATCH_SIZE  # Reuse feature batch size\n",
    "num_test_batches = (len(test_paths) + test_batch_size - 1) // test_batch_size\n",
    "\n",
    "for batch_idx in tqdm(range(0, len(test_paths), test_batch_size),\n",
    "                          desc='Predicting test (batched)',\n",
    "                          mininterval=TQDM_MININTERVAL,\n",
    "                          disable=(not SHOW_PROGRESS),\n",
    "                          total=num_test_batches):\n",
    "    batch_paths = test_paths[batch_idx:batch_idx + test_batch_size]\n",
    "    batch_ids = [Path(p).stem for p in batch_paths]\n",
    "    \n",
    "    # Generate features for batch\n",
    "    Xp_batch = build_features_batched(batch_paths, members, NUM_TTA_AUGS, batch_size=test_batch_size)\n",
    "    \n",
    "    # Predict on batch\n",
    "    preds_batch = meta_final.predict(Xp_batch)\n",
    "    \n",
    "    # Store predictions\n",
    "    for img_id, pred in zip(batch_ids, preds_batch):\n",
    "        pred = int(pred)\n",
    "        pred = max(0, min(pred, len(VENDOR_CLASSES)-1))\n",
    "        pred_ids.append(img_id)\n",
    "        pred_labels.append(IDX_TO_VENDOR[pred])\n",
    "\n",
    "out_path = Path(PREDICTIONS_OUTPUT_FILE)\n",
    "with out_path.open('w') as f:\n",
    "    f.write('Id,Label\\n')\n",
    "    for i,l in zip(pred_ids, pred_labels):\n",
    "        f.write(f'{str(i).strip()},{l}\\n')\n",
    "\n",
    "print('Wrote:', out_path, 'rows:', len(pred_labels))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
