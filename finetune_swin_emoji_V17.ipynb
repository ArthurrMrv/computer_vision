{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V17 — No K-Fold, Gentle Fine-tune + Regularization\n",
        "\n",
        "**Goal:** Improve generalization with gentler fine-tuning, stronger regularization, better augmentations, and no K-fold (single split).\n",
        "\n",
        "**Key Features (V17):**\n",
        "- **5 Architectures**: EfficientNet B0/B1/B2 + ConvNeXtV2 + DINOv2\n",
        "- **No K-Fold**: Single stratified train/val split; each model trained once\n",
        "- **Gentle Fine-tune**: Lower LR, shorter fine-tune, smaller scheduler patience; partial unfreeze\n",
        "- **Regularization**: Higher dropout, label smoothing, optional head-only weight decay\n",
        "- **Augmentations**: Light MixUp, small RandAugment/ColorJitter, enhanced TTA (extra scale + blur/sharpen)\n",
        "- **Pseudo-Labeling**: Confidence raised (0.98–0.99) with V17-tagged checkpoints\n",
        "- **LightGBM**: Tighter regularization\n",
        "\n",
        "**Classes:** apple, google, whatsapp, facebook, samsung, mozilla, messenger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Setup & Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt --upgrade -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import hashlib\n",
        "import random\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple, Dict, Optional\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification, AutoModel\n",
        "from transformers.modeling_outputs import ImageClassifierOutput\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    HAS_LGB = True\n",
        "except Exception as e:\n",
        "    HAS_LGB = False\n",
        "    print('LightGBM import failed:', e)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Configuration & Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "# Data Paths\n",
        "SECOND_DATASET_BASE_PATH = '.'\n",
        "SECOND_DATASET_TRAIN_DIR = Path(SECOND_DATASET_BASE_PATH) / 'train'\n",
        "SECOND_DATASET_CSV_PATH = Path(SECOND_DATASET_BASE_PATH) / 'train_labels.csv'\n",
        "SECOND_DATASET_TEST_DIR = Path(SECOND_DATASET_BASE_PATH) / 'test'\n",
        "\n",
        "# HuggingFace Dataset\n",
        "HF_DATASET_ID = 'subinium/emojiimage-dataset'\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL CONFIGURATIONS - 5 Architectures with native resolutions\n",
        "# ============================================================================\n",
        "# Format: (model_id, tag, image_size)\n",
        "MODELS = [\n",
        "    ('google/efficientnet-b0', 'effnet_b0', 224),\n",
        "    ('google/efficientnet-b1', 'effnet_b1', 240),\n",
        "    ('google/efficientnet-b2', 'effnet_b2', 260),\n",
        "    ('facebook/convnextv2-base-22k-224', 'cnn_base', 224),\n",
        "    ('facebook/dinov2-base', 'dino', 224),\n",
        "]\n",
        "\n",
        "# Model type mapping for architecture-specific handling\n",
        "MODEL_TYPES = {\n",
        "    'effnet_b0': 'efficientnet',\n",
        "    'effnet_b1': 'efficientnet',\n",
        "    'effnet_b2': 'efficientnet',\n",
        "    'cnn_base': 'convnext',\n",
        "    'dino': 'dino',\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING HYPERPARAMETERS\n",
        "# ============================================================================\n",
        "RANDOM_STATE = 42\n",
        "USE_KFOLD = False  # No K-Fold in V17\n",
        "NUM_FOLDS = 1      # Placeholder to keep legacy functions inert\n",
        "\n",
        "# Two-Phase Training\n",
        "WARMUP_EPOCHS = 5            # Phase 1: Frozen backbone\n",
        "WARMUP_LR = 1e-3\n",
        "FINETUNE_EPOCHS = 12         # Gentler fine-tune\n",
        "FINETUNE_LR = 4e-6\n",
        "EARLY_STOPPING_PATIENCE = 2  # Lower patience so LR drops sooner\n",
        "\n",
        "# Batch sizes\n",
        "BATCH_SIZE_CUDA = 16\n",
        "BATCH_SIZE_CPU = 4\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# Regularization\n",
        "LABEL_SMOOTHING = 0.1  # base; heads may override\n",
        "WEIGHT_DECAY = 1e-5\n",
        "HEAD_WEIGHT_DECAY = 1e-4  # optional, heads only\n",
        "HEAD_DROPOUT_DELTA = 0.05  # add to existing head dropouts\n",
        "LABEL_SMOOTHING_CNN = 0.14\n",
        "LABEL_SMOOTHING_DINO = 0.1\n",
        "\n",
        "# MixUp / Augment\n",
        "USE_MIXUP = True\n",
        "MIXUP_ALPHA = 0.3\n",
        "USE_EXTRA_COLOR = True  # small RandAugment/ColorJitter-like boost\n",
        "\n",
        "# Pseudo-labeling\n",
        "PSEUDO_LABEL_CONFIDENCE = 0.985\n",
        "PSEUDO_LABEL_EPOCHS = 3\n",
        "PSEUDO_LABEL_LR = 5e-6\n",
        "\n",
        "# TTA tweaks\n",
        "EXTRA_TTA_SCALE = 0.95\n",
        "USE_TTA_BLUR_SHARPEN = True\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TTA & FEATURE EXTRACTION\n",
        "# ============================================================================\n",
        "NUM_TTA_AUGS = 12  # Multi-scale TTA\n",
        "FEATURE_BATCH_SIZE = 16\n",
        "TQDM_MININTERVAL = 10\n",
        "SHOW_PROGRESS = True\n",
        "\n",
        "# ============================================================================\n",
        "# META-MODEL (LightGBM)\n",
        "# ============================================================================\n",
        "USE_LIGHTGBM = True\n",
        "LGB_PARAMS = {\n",
        "    'n_estimators': 2000,\n",
        "    'learning_rate': 0.015,\n",
        "    'num_leaves': 95,\n",
        "    'max_depth': -1,\n",
        "    'subsample': 0.8,\n",
        "    'bagging_freq': 1,\n",
        "    'feature_fraction': 0.85,\n",
        "    'min_child_samples': 40,\n",
        "    'lambda_l1': 0.2,\n",
        "    'lambda_l2': 0.2,\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'verbosity': -1,\n",
        "    'n_jobs': -1,\n",
        "}\n",
        "\n",
        "# Output\n",
        "PREDICTIONS_OUTPUT_FILE = 'predictions_V17.csv'\n",
        "\n",
        "# ============================================================================\n",
        "# PRINT CONFIGURATION\n",
        "# ============================================================================\n",
        "print('='*60)\n",
        "print('V17 Configuration (No K-Fold)')\n",
        "print('='*60)\n",
        "print(f'Models: {len(MODELS)} architectures')\n",
        "for model_id, tag, img_size in MODELS:\n",
        "    print(f'  - {tag}: {model_id} ({img_size}x{img_size})')\n",
        "print(f'Two-Phase: {WARMUP_EPOCHS} warmup + {FINETUNE_EPOCHS} finetune epochs')\n",
        "print(f'Fine-tune LR: {FINETUNE_LR}, scheduler patience: {EARLY_STOPPING_PATIENCE}')\n",
        "print(f'TTA: {NUM_TTA_AUGS} augmentations (multi-scale)')\n",
        "print(f'Pseudo-labeling: confidence > {PSEUDO_LABEL_CONFIDENCE}')\n",
        "print('='*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CLASS DEFINITIONS & LABEL MAPPING\n",
        "# ============================================================================\n",
        "\n",
        "VENDOR_CLASSES = ['apple', 'google', 'whatsapp', 'facebook', 'samsung', 'mozilla', 'messenger']\n",
        "VENDOR_TO_IDX = {v: i for i, v in enumerate(VENDOR_CLASSES)}\n",
        "IDX_TO_VENDOR = {i: v for v, i in VENDOR_TO_IDX.items()}\n",
        "NUM_CLASSES = len(VENDOR_CLASSES)\n",
        "\n",
        "# Label mapping from HuggingFace dataset (11 classes) to target dataset (7 classes)\n",
        "HF_TO_TARGET_MAPPING = {\n",
        "    'Apple': 'apple',\n",
        "    'Google': 'google', 'Gmail': 'google', 'Mozilla': 'google',\n",
        "    'Facebook': 'facebook',\n",
        "    'Samsung': 'samsung',\n",
        "    'WhatsApp': 'whatsapp',\n",
        "    'Messenger': 'messenger',\n",
        "    'DoCoMo': 'apple', 'JoyPixels': 'apple', 'KDDI': 'apple', 'SoftBank': 'apple',\n",
        "    'Twitter': 'google', 'Windows': 'google'\n",
        "}\n",
        "\n",
        "print('VENDOR_CLASSES:', VENDOR_CLASSES)\n",
        "print('NUM_CLASSES:', NUM_CLASSES)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Model Wrappers with Metadata Fusion\n",
        "\n",
        "Each model wrapper includes:\n",
        "- Backbone feature extraction\n",
        "- Metadata branch (image size + transparency)\n",
        "- Fusion layer combining visual features + metadata\n",
        "- Classification head with label smoothing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EfficientNetWithMetadata(nn.Module):\n",
        "    \"\"\"EfficientNet wrapper with metadata fusion.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_model, num_labels, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.num_labels = num_labels\n",
        "        self.label_smoothing = label_smoothing\n",
        "        \n",
        "        # Dynamic hidden size detection\n",
        "        self._hidden_size = None\n",
        "        self.classifier = None\n",
        "        self._needs_init = True\n",
        "        \n",
        "        # Metadata branch: [height_norm, width_norm, has_alpha]\n",
        "        self.metadata_mlp = nn.Sequential(\n",
        "            nn.Linear(3, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2 + HEAD_DROPOUT_DELTA),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "    \n",
        "    def _extract_pooled_features(self, pixel_values):\n",
        "        \"\"\"Extract pooled features from EfficientNet backbone.\"\"\"\n",
        "        backbone = getattr(self.base_model, 'efficientnet', self.base_model)\n",
        "        backbone_out = backbone(pixel_values)\n",
        "        \n",
        "        # Handle different output types\n",
        "        if hasattr(backbone_out, 'pooler_output') and backbone_out.pooler_output is not None:\n",
        "            return backbone_out.pooler_output\n",
        "        elif hasattr(backbone_out, 'last_hidden_state'):\n",
        "            features = backbone_out.last_hidden_state\n",
        "            if len(features.shape) == 4:\n",
        "                return features.mean(dim=[2, 3])\n",
        "            elif len(features.shape) == 3:\n",
        "                return features[:, 0]\n",
        "            return features\n",
        "        elif isinstance(backbone_out, tuple):\n",
        "            features = backbone_out[0]\n",
        "            if len(features.shape) == 4:\n",
        "                return features.mean(dim=[2, 3])\n",
        "            return features\n",
        "        else:\n",
        "            if len(backbone_out.shape) == 4:\n",
        "                return backbone_out.mean(dim=[2, 3])\n",
        "            return backbone_out\n",
        "    \n",
        "    def _ensure_classifier_initialized(self, pixel_values):\n",
        "        \"\"\"Initialize classifier with correct hidden size.\"\"\"\n",
        "        if self.classifier is not None:\n",
        "            return\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            pooled = self._extract_pooled_features(pixel_values[:1])\n",
        "            hidden_size = pooled.shape[-1]\n",
        "        \n",
        "        self._hidden_size = hidden_size\n",
        "        # Fusion: image features + metadata\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden_size + 32),\n",
        "            nn.Dropout(0.3 + HEAD_DROPOUT_DELTA),\n",
        "            nn.Linear(hidden_size + 32, 256),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(256),\n",
        "            nn.Dropout(0.2 + HEAD_DROPOUT_DELTA),\n",
        "            nn.Linear(256, self.num_labels)\n",
        "        ).to(pixel_values.device)\n",
        "        self._needs_init = False\n",
        "        print(f'Initialized classifier with hidden_size={hidden_size}')\n",
        "    \n",
        "    def forward(self, pixel_values, metadata=None, labels=None):\n",
        "        # Ensure classifier is initialized\n",
        "        if self.classifier is None:\n",
        "            self._ensure_classifier_initialized(pixel_values)\n",
        "        \n",
        "        # Extract visual features\n",
        "        pooled = self._extract_pooled_features(pixel_values)\n",
        "        \n",
        "        # Process metadata\n",
        "        if metadata is not None:\n",
        "            meta_features = self.metadata_mlp(metadata)\n",
        "            fused = torch.cat([pooled, meta_features], dim=-1)\n",
        "        else:\n",
        "            # If no metadata, use zeros\n",
        "            batch_size = pooled.shape[0]\n",
        "            zero_meta = torch.zeros(batch_size, 32, device=pooled.device)\n",
        "            fused = torch.cat([pooled, zero_meta], dim=-1)\n",
        "        \n",
        "        logits = self.classifier(fused)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = torch.clamp(labels, 0, self.num_labels - 1)\n",
        "            loss = nn.CrossEntropyLoss(label_smoothing=self.label_smoothing)(\n",
        "                logits.view(-1, self.num_labels), labels.view(-1)\n",
        "            )\n",
        "        \n",
        "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
        "    \n",
        "    def freeze_backbone(self):\n",
        "        \"\"\"Freeze backbone for warmup training.\"\"\"\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        # Keep classifier and metadata trainable\n",
        "        for param in self.metadata_mlp.parameters():\n",
        "            param.requires_grad = True\n",
        "        if self.classifier is not None:\n",
        "            for param in self.classifier.parameters():\n",
        "                param.requires_grad = True\n",
        "    \n",
        "    def unfreeze_backbone(self):\n",
        "        \"\"\"Unfreeze backbone for fine-tuning.\"\"\"\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "print('EfficientNetWithMetadata defined')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvNeXtWithMetadata(nn.Module):\n",
        "    \"\"\"ConvNeXtV2 wrapper with metadata fusion.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_model, num_labels, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.num_labels = num_labels\n",
        "        self.label_smoothing = label_smoothing\n",
        "        \n",
        "        # Get hidden size from config\n",
        "        hidden = getattr(getattr(base_model, 'config', None), 'hidden_sizes', [1024])[-1]\n",
        "        self._hidden_size = hidden\n",
        "        \n",
        "        # Metadata branch\n",
        "        self.metadata_mlp = nn.Sequential(\n",
        "            nn.Linear(3, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2 + HEAD_DROPOUT_DELTA),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Fusion classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden + 32),\n",
        "            nn.Dropout(0.3 + HEAD_DROPOUT_DELTA),\n",
        "            nn.Linear(hidden + 32, hidden // 2),\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(hidden // 2),\n",
        "            nn.Dropout(0.2 + HEAD_DROPOUT_DELTA),\n",
        "            nn.Linear(hidden // 2, num_labels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, pixel_values, metadata=None, labels=None):\n",
        "        # Get backbone - try convnextv2 first, then convnext\n",
        "        backbone = getattr(self.base_model, 'convnextv2', None)\n",
        "        if backbone is None:\n",
        "            backbone = getattr(self.base_model, 'convnext', self.base_model)\n",
        "        \n",
        "        out = backbone(pixel_values)\n",
        "        feats = out.last_hidden_state\n",
        "        \n",
        "        # Pool spatial dimensions\n",
        "        if len(feats.shape) == 4:\n",
        "            pooled = feats.mean(dim=[2, 3])\n",
        "        else:\n",
        "            pooled = feats\n",
        "        \n",
        "        # Process metadata\n",
        "        if metadata is not None:\n",
        "            meta_features = self.metadata_mlp(metadata)\n",
        "            fused = torch.cat([pooled, meta_features], dim=-1)\n",
        "        else:\n",
        "            batch_size = pooled.shape[0]\n",
        "            zero_meta = torch.zeros(batch_size, 32, device=pooled.device)\n",
        "            fused = torch.cat([pooled, zero_meta], dim=-1)\n",
        "        \n",
        "        logits = self.classifier(fused)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = torch.clamp(labels, 0, self.num_labels - 1)\n",
        "            loss = nn.CrossEntropyLoss(label_smoothing=self.label_smoothing)(\n",
        "                logits.view(-1, self.num_labels), labels.view(-1)\n",
        "            )\n",
        "        \n",
        "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
        "    \n",
        "    def freeze_backbone(self):\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.metadata_mlp.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "    \n",
        "    def unfreeze_backbone(self):\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "print('ConvNeXtWithMetadata defined')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DINOWithMetadata(nn.Module):\n",
        "    \"\"\"DINOv2 wrapper with metadata fusion.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_model, num_labels, label_smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.num_labels = num_labels\n",
        "        self.label_smoothing = label_smoothing\n",
        "        \n",
        "        hidden = getattr(base_model.config, 'hidden_size', 768)\n",
        "        self._hidden_size = hidden\n",
        "        \n",
        "        # Metadata branch\n",
        "        self.metadata_mlp = nn.Sequential(\n",
        "            nn.Linear(3, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(32, 32),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Simpler classifier for DINO (less prone to overfitting)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden + 32),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden + 32, num_labels)\n",
        "        )\n",
        "    \n",
        "    def forward(self, pixel_values, metadata=None, labels=None):\n",
        "        out = self.base_model(pixel_values=pixel_values, output_hidden_states=True)\n",
        "        \n",
        "        # Get pooled output\n",
        "        if hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
        "            pooled = out.pooler_output\n",
        "        else:\n",
        "            pooled = out.hidden_states[-1][:, 0, :]\n",
        "        \n",
        "        # Process metadata\n",
        "        if metadata is not None:\n",
        "            meta_features = self.metadata_mlp(metadata)\n",
        "            fused = torch.cat([pooled, meta_features], dim=-1)\n",
        "        else:\n",
        "            batch_size = pooled.shape[0]\n",
        "            zero_meta = torch.zeros(batch_size, 32, device=pooled.device)\n",
        "            fused = torch.cat([pooled, zero_meta], dim=-1)\n",
        "        \n",
        "        logits = self.classifier(fused)\n",
        "        \n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = torch.clamp(labels, 0, self.num_labels - 1)\n",
        "            loss = nn.CrossEntropyLoss(label_smoothing=self.label_smoothing)(\n",
        "                logits.view(-1, self.num_labels), labels.view(-1)\n",
        "            )\n",
        "        \n",
        "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
        "    \n",
        "    def freeze_backbone(self):\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.metadata_mlp.parameters():\n",
        "            param.requires_grad = True\n",
        "        for param in self.classifier.parameters():\n",
        "            param.requires_grad = True\n",
        "    \n",
        "    def unfreeze_backbone(self):\n",
        "        for param in self.base_model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "print('DINOWithMetadata defined')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_model(model_id: str, tag: str, num_labels: int = NUM_CLASSES) -> Tuple[nn.Module, any]:\n",
        "    \"\"\"\n",
        "    Factory function to create the appropriate model wrapper.\n",
        "    Returns (model, processor)\n",
        "    \"\"\"\n",
        "    model_type = MODEL_TYPES[tag]\n",
        "    \n",
        "    if model_type == 'efficientnet':\n",
        "        processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "        backbone = AutoModelForImageClassification.from_pretrained(model_id)\n",
        "        model = EfficientNetWithMetadata(backbone, num_labels, label_smoothing=LABEL_SMOOTHING_CNN)\n",
        "    elif model_type == 'convnext':\n",
        "        processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "        backbone = AutoModelForImageClassification.from_pretrained(model_id)\n",
        "        model = ConvNeXtWithMetadata(backbone, num_labels, label_smoothing=LABEL_SMOOTHING_CNN)\n",
        "    elif model_type == 'dino':\n",
        "        processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "        backbone = AutoModel.from_pretrained(model_id)\n",
        "        model = DINOWithMetadata(backbone, num_labels, label_smoothing=LABEL_SMOOTHING_DINO)\n",
        "    else:\n",
        "        raise ValueError(f'Unknown model type: {model_type}')\n",
        "    \n",
        "    return model, processor\n",
        "\n",
        "print('Model factory function defined')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Utility Functions & Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    \"\"\"Set all random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def load_image_rgb(path: str) -> Image.Image:\n",
        "    \"\"\"Load image and convert to RGB with white background for transparency.\"\"\"\n",
        "    img = Image.open(path)\n",
        "    if img.mode == 'P':\n",
        "        img = img.convert('RGBA')\n",
        "    if img.mode == 'RGBA':\n",
        "        bg = Image.new('RGB', img.size, (255, 255, 255))\n",
        "        bg.paste(img, mask=img.split()[3])\n",
        "        img = bg\n",
        "    elif img.mode != 'RGB':\n",
        "        img = img.convert('RGB')\n",
        "    return img\n",
        "\n",
        "def extract_metadata(image_path: str) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Extract metadata features from image for fusion.\n",
        "    Returns: dict with height_norm, width_norm, has_alpha\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        w, h = img.size\n",
        "        has_alpha = 1.0 if img.mode in ('RGBA', 'LA', 'P') else 0.0\n",
        "        return {\n",
        "            'height_norm': h / 256.0,\n",
        "            'width_norm': w / 256.0,\n",
        "            'has_alpha': has_alpha\n",
        "        }\n",
        "    except Exception:\n",
        "        return {'height_norm': 1.0, 'width_norm': 1.0, 'has_alpha': 0.0}\n",
        "\n",
        "def extract_image_properties(image_path: str) -> Dict[str, float]:\n",
        "    \"\"\"Extract full statistical properties from image for feature matrix.\"\"\"\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        mode_mapping = {'L': 0, 'LA': 1, 'P': 2, 'RGB': 3, 'RGBA': 4}\n",
        "        original_mode = float(mode_mapping.get(img.mode, 3))\n",
        "        \n",
        "        # Convert for pixel stats\n",
        "        if img.mode == 'P':\n",
        "            img = img.convert('RGBA')\n",
        "        if img.mode == 'RGBA':\n",
        "            bg = Image.new('RGB', img.size, (255, 255, 255))\n",
        "            bg.paste(img, mask=img.split()[3])\n",
        "            img = bg\n",
        "        elif img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        \n",
        "        w, h = img.size\n",
        "        arr = np.array(img)\n",
        "        \n",
        "        return {\n",
        "            'width': float(w),\n",
        "            'height': float(h),\n",
        "            'aspect_ratio': float(w / h) if h else 1.0,\n",
        "            'pixel_count': float(w * h),\n",
        "            'mean_r': float(arr[:, :, 0].mean()),\n",
        "            'mean_g': float(arr[:, :, 1].mean()),\n",
        "            'mean_b': float(arr[:, :, 2].mean()),\n",
        "            'std_r': float(arr[:, :, 0].std()),\n",
        "            'std_g': float(arr[:, :, 1].std()),\n",
        "            'std_b': float(arr[:, :, 2].std()),\n",
        "            'brightness': float((arr[:, :, 0].mean() + arr[:, :, 1].mean() + arr[:, :, 2].mean()) / 3.0),\n",
        "            'is_mostly_white': float(arr.mean() > 200),\n",
        "            'original_mode': original_mode\n",
        "        }\n",
        "    except Exception:\n",
        "        return {\n",
        "            'width': 224.0, 'height': 224.0, 'aspect_ratio': 1.0, 'pixel_count': 50176.0,\n",
        "            'mean_r': 128.0, 'mean_g': 128.0, 'mean_b': 128.0,\n",
        "            'std_r': 50.0, 'std_g': 50.0, 'std_b': 50.0,\n",
        "            'brightness': 128.0, 'is_mostly_white': 0.0, 'original_mode': 3.0\n",
        "        }\n",
        "\n",
        "STAT_COLS = ['width', 'height', 'aspect_ratio', 'pixel_count', 'mean_r', 'mean_g', 'mean_b',\n",
        "             'std_r', 'std_g', 'std_b', 'brightness', 'is_mostly_white', 'original_mode']\n",
        "\n",
        "print('Utility functions defined')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_class_weights(labels: List[int]) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Compute class weights for handling class imbalance.\n",
        "    Weight = total_samples / (num_classes * class_count)\n",
        "    \"\"\"\n",
        "    labels_arr = np.array(labels)\n",
        "    class_counts = np.bincount(labels_arr, minlength=NUM_CLASSES)\n",
        "    total_samples = len(labels)\n",
        "    \n",
        "    # Avoid division by zero\n",
        "    class_counts = np.maximum(class_counts, 1)\n",
        "    \n",
        "    weights = total_samples / (NUM_CLASSES * class_counts)\n",
        "    weights = weights / weights.sum() * NUM_CLASSES  # Normalize\n",
        "    \n",
        "    print('Class weights:')\n",
        "    for i, (cls, w) in enumerate(zip(VENDOR_CLASSES, weights)):\n",
        "        count = class_counts[i]\n",
        "        print(f'  {cls:12s}: {w:.3f} (n={count})')\n",
        "    \n",
        "    return torch.tensor(weights, dtype=torch.float32)\n",
        "\n",
        "print('Class weights function defined')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Multi-Scale TTA Augmentations (12 augmentations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiScaleTTA:\n",
        "    \"\"\"\n",
        "    Multi-scale test-time augmentation with 12 augmentations:\n",
        "    - Original + HFlip + VFlip (3)\n",
        "    - Rotations: +5, +10, -5, -10 degrees (4)\n",
        "    - Corner crops: TL, TR, BL, BR at 90% (4)\n",
        "    - Center crop at 85% (1)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, base_image_size: int = 224):\n",
        "        self.base_size = base_image_size\n",
        "        self.rotation_angles = [-10, -5, 5, 10]\n",
        "        self.crop_ratio = 0.9\n",
        "        self.center_crop_ratio = 0.85\n",
        "    \n",
        "    def _get_deterministic_seed(self, image_or_hash):\n",
        "        \"\"\"Get deterministic seed from image content.\"\"\"\n",
        "        if isinstance(image_or_hash, Image.Image):\n",
        "            img_bytes = image_or_hash.tobytes()\n",
        "            return int(hashlib.md5(img_bytes).hexdigest()[:8], 16)\n",
        "        return hash(str(image_or_hash)) & 0xFFFFFFFF\n",
        "    \n",
        "    def get_augmentations(self, image: Image.Image, num_augmentations: int = 12, \n",
        "                          seed_source=None) -> List[Image.Image]:\n",
        "        \"\"\"Generate deterministic augmentations.\"\"\"\n",
        "        augs = []\n",
        "        target_size = self.base_size\n",
        "        \n",
        "        # 1. Original\n",
        "        augs.append(image.resize((target_size, target_size), Image.BILINEAR))\n",
        "        \n",
        "        # 2. Horizontal flip\n",
        "        if len(augs) < num_augmentations:\n",
        "            augs.append(F.hflip(image).resize((target_size, target_size), Image.BILINEAR))\n",
        "        \n",
        "        # 3. Vertical flip\n",
        "        if len(augs) < num_augmentations:\n",
        "            augs.append(F.vflip(image).resize((target_size, target_size), Image.BILINEAR))\n",
        "        \n",
        "        # 4-7. Rotations\n",
        "        for angle in self.rotation_angles:\n",
        "            if len(augs) >= num_augmentations:\n",
        "                break\n",
        "            augs.append(F.rotate(image, angle).resize((target_size, target_size), Image.BILINEAR))\n",
        "        \n",
        "        # 8-11. Corner crops\n",
        "        w, h = image.size\n",
        "        crop_size = int(min(w, h) * self.crop_ratio)\n",
        "        corners = [\n",
        "            (0, 0),  # TL\n",
        "            (w - crop_size, 0),  # TR\n",
        "            (0, h - crop_size),  # BL\n",
        "            (w - crop_size, h - crop_size),  # BR\n",
        "        ]\n",
        "        for (left, top) in corners:\n",
        "            if len(augs) >= num_augmentations:\n",
        "                break\n",
        "            cropped = F.crop(image, top, left, crop_size, crop_size)\n",
        "            augs.append(cropped.resize((target_size, target_size), Image.BILINEAR))\n",
        "        \n",
        "        # 12. Center crop\n",
        "        if len(augs) < num_augmentations:\n",
        "            center_size = int(min(w, h) * self.center_crop_ratio)\n",
        "            center_cropped = F.center_crop(image, [center_size, center_size])\n",
        "            augs.append(center_cropped.resize((target_size, target_size), Image.BILINEAR))\n",
        "        \n",
        "        # Extra mild scale\n",
        "        if EXTRA_TTA_SCALE and len(augs) < num_augmentations:\n",
        "            scale_size = int(min(w, h) * EXTRA_TTA_SCALE)\n",
        "            scaled = F.center_crop(image, [scale_size, scale_size])\n",
        "            augs.append(scaled.resize((target_size, target_size), Image.BILINEAR))\n",
        "        \n",
        "        # Blur / sharpen variants\n",
        "        if USE_TTA_BLUR_SHARPEN and len(augs) < num_augmentations:\n",
        "            blur_img = F.gaussian_blur(image, kernel_size=3, sigma=(0.5, 0.8))\n",
        "            augs.append(blur_img.resize((target_size, target_size), Image.BILINEAR))\n",
        "        if USE_TTA_BLUR_SHARPEN and len(augs) < num_augmentations:\n",
        "            sharp_img = F.adjust_sharpness(image, sharpness_factor=1.5)\n",
        "            augs.append(sharp_img.resize((target_size, target_size), Image.BILINEAR))\n",
        "        \n",
        "        return augs[:num_augmentations]\n",
        "    \n",
        "    def apply_training_augmentation(self, image: Image.Image, seed_source=None) -> Image.Image:\n",
        "        \"\"\"Apply random augmentation for training.\"\"\"\n",
        "        if seed_source is None:\n",
        "            seed_val = self._get_deterministic_seed(image)\n",
        "        elif isinstance(seed_source, str):\n",
        "            seed_val = self._get_deterministic_seed(seed_source)\n",
        "        else:\n",
        "            seed_val = int(seed_source)\n",
        "        \n",
        "        np.random.seed(seed_val % (2**32))\n",
        "        \n",
        "        # Random horizontal flip\n",
        "        if seed_val % 2 == 0:\n",
        "            image = F.hflip(image)\n",
        "        \n",
        "        # Random rotation\n",
        "        angle_idx = (seed_val // 2) % len(self.rotation_angles)\n",
        "        angle = self.rotation_angles[angle_idx]\n",
        "        image = F.rotate(image, angle)\n",
        "        \n",
        "        # Random crop\n",
        "        w, h = image.size\n",
        "        crop_ratio = 0.85 + np.random.uniform(0, 0.15)\n",
        "        crop_size = int(min(w, h) * crop_ratio)\n",
        "        image = F.center_crop(image, [crop_size, crop_size])\n",
        "        \n",
        "        # Random color jitter\n",
        "        brightness = 1.0 + np.random.uniform(-0.3, 0.3)\n",
        "        contrast = 1.0 + np.random.uniform(-0.3, 0.3)\n",
        "        saturation = 1.0 + np.random.uniform(-0.3, 0.3)\n",
        "        if USE_EXTRA_COLOR:\n",
        "            hue = np.random.uniform(-0.08, 0.08)\n",
        "            image = F.adjust_hue(image, hue)\n",
        "            saturation *= 1.05\n",
        "            contrast *= 1.05\n",
        "        \n",
        "        image = F.adjust_brightness(image, brightness)\n",
        "        image = F.adjust_contrast(image, contrast)\n",
        "        image = F.adjust_saturation(image, saturation)\n",
        "        \n",
        "        # Optional mild blur/sharpen during training\n",
        "        if USE_TTA_BLUR_SHARPEN and (seed_val % 5 == 0):\n",
        "            image = F.gaussian_blur(image, kernel_size=3, sigma=(0.5, 0.8))\n",
        "        if USE_TTA_BLUR_SHARPEN and (seed_val % 7 == 0):\n",
        "            image = F.adjust_sharpness(image, sharpness_factor=1.5)\n",
        "        \n",
        "        # Resize to target\n",
        "        image = image.resize((self.base_size, self.base_size), Image.BILINEAR)\n",
        "        \n",
        "        return image\n",
        "\n",
        "# Create TTA instances for different image sizes\n",
        "tta_instances = {}\n",
        "for model_id, tag, img_size in MODELS:\n",
        "    tta_instances[tag] = MultiScaleTTA(base_image_size=img_size)\n",
        "\n",
        "print(f'Created TTA instances for {len(tta_instances)} model sizes')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 6: Dataset Class with Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmojiDatasetWithMetadata(Dataset):\n",
        "    \"\"\"Dataset with metadata extraction for model fusion.\"\"\"\n",
        "    \n",
        "    def __init__(self, image_paths: List[str], labels: List[int], \n",
        "                 processor, model_tag: str, use_augmentation: bool = False):\n",
        "        self.image_paths = list(image_paths)\n",
        "        self.labels = list(labels)\n",
        "        self.processor = processor\n",
        "        self.model_tag = model_tag\n",
        "        self.use_augmentation = use_augmentation\n",
        "        self.tta = tta_instances.get(model_tag, MultiScaleTTA(224))\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        path = self.image_paths[idx]\n",
        "        label = int(self.labels[idx])\n",
        "        \n",
        "        # Load image\n",
        "        img = load_image_rgb(path)\n",
        "        \n",
        "        # Extract metadata\n",
        "        meta = extract_metadata(path)\n",
        "        metadata = torch.tensor([\n",
        "            meta['height_norm'],\n",
        "            meta['width_norm'],\n",
        "            meta['has_alpha']\n",
        "        ], dtype=torch.float32)\n",
        "        \n",
        "        # Apply augmentation if training\n",
        "        if self.use_augmentation:\n",
        "            img = self.tta.apply_training_augmentation(img, seed_source=str(path))\n",
        "        \n",
        "        # Process image\n",
        "        inputs = self.processor(img, return_tensors='pt')\n",
        "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
        "        \n",
        "        label = int(max(0, min(label, NUM_CLASSES - 1)))\n",
        "        \n",
        "        return {\n",
        "            'pixel_values': pixel_values,\n",
        "            'metadata': metadata,\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "print('EmojiDatasetWithMetadata defined')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 7: Two-Phase Training Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, device, scaler=None, class_weights=None,\n",
        "                use_mixup=False, mixup_alpha=0.3):\n",
        "    \"\"\"Train for one epoch with optional class weights and MixUp.\"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    use_amp = (device.type == 'cuda')\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=getattr(model, 'label_smoothing', LABEL_SMOOTHING))\n",
        "    \n",
        "    for batch in tqdm(loader, desc='Training', mininterval=TQDM_MININTERVAL, disable=(not SHOW_PROGRESS)):\n",
        "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
        "        meta = batch['metadata'].to(device, non_blocking=True)\n",
        "        y = batch['labels'].to(device, non_blocking=True)\n",
        "        y = torch.clamp(y, 0, model.num_labels - 1)\n",
        "        \n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        \n",
        "        if use_mixup:\n",
        "            lam = np.random.beta(mixup_alpha, mixup_alpha)\n",
        "            perm = torch.randperm(x.size(0), device=device)\n",
        "            x_mixed = lam * x + (1 - lam) * x[perm]\n",
        "            meta_mixed = lam * meta + (1 - lam) * meta[perm]\n",
        "            targets_a, targets_b = y, y[perm]\n",
        "            \n",
        "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "                out = model(pixel_values=x_mixed, metadata=meta_mixed, labels=None)\n",
        "                logits = out.logits\n",
        "                loss = lam * criterion(logits, targets_a) + (1 - lam) * criterion(logits, targets_b)\n",
        "        else:\n",
        "            with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "                out = model(pixel_values=x, metadata=meta, labels=y)\n",
        "                loss = out.loss\n",
        "                logits = out.logits\n",
        "        \n",
        "        # Apply class weights if provided\n",
        "        if class_weights is not None:\n",
        "            weight_tensor = class_weights.to(device)\n",
        "            if use_mixup:\n",
        "                # weight per sample: interpolate weights\n",
        "                batch_weights = lam * weight_tensor[targets_a] + (1 - lam) * weight_tensor[targets_b]\n",
        "                loss = loss * batch_weights.mean()\n",
        "            else:\n",
        "                batch_weights = weight_tensor[y]\n",
        "                loss = (loss * batch_weights).mean() if loss.dim() > 0 else loss\n",
        "        \n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        total_loss += float(loss.item())\n",
        "        pred = torch.argmax(logits, dim=1)\n",
        "        correct += int((pred == y).sum().item())\n",
        "        total += int(y.size(0))\n",
        "    \n",
        "    return total_loss / max(1, len(loader)), 100.0 * correct / max(1, total)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, device):\n",
        "    \"\"\"Validate model.\"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    preds = []\n",
        "    labels = []\n",
        "    use_amp = (device.type == 'cuda')\n",
        "    \n",
        "    for batch in tqdm(loader, desc='Validation', mininterval=TQDM_MININTERVAL, disable=(not SHOW_PROGRESS)):\n",
        "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
        "        meta = batch['metadata'].to(device, non_blocking=True)\n",
        "        y = batch['labels'].to(device, non_blocking=True)\n",
        "        y = torch.clamp(y, 0, model.num_labels - 1)\n",
        "        \n",
        "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "            out = model(pixel_values=x, metadata=meta, labels=y)\n",
        "            loss = out.loss\n",
        "        \n",
        "        total_loss += float(loss.item())\n",
        "        pred = torch.argmax(out.logits, dim=1)\n",
        "        pred = torch.clamp(pred, 0, model.num_labels - 1)\n",
        "        correct += int((pred == y).sum().item())\n",
        "        total += int(y.size(0))\n",
        "        preds.extend(pred.cpu().numpy().tolist())\n",
        "        labels.extend(y.cpu().numpy().tolist())\n",
        "    \n",
        "    return total_loss / max(1, len(loader)), 100.0 * correct / max(1, total), preds, labels\n",
        "\n",
        "print('Train/validate functions defined')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def two_phase_train(model, processor, model_tag, train_paths, train_labels, \n",
        "                    val_paths, val_labels, checkpoint_prefix, class_weights=None):\n",
        "    \"\"\"\n",
        "    Two-phase training:\n",
        "    Phase 1: Frozen backbone, train classifier head (warmup)\n",
        "    Phase 2: Partial unfreeze + gentle fine-tune with lower LR\n",
        "    \"\"\"\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Two-Phase Training: {model_tag}')\n",
        "    print(f'{\"=\"*60}')\n",
        "    \n",
        "    model = model.to(device)\n",
        "    img_size = next((s for _, t, s in MODELS if t == model_tag), 224)\n",
        "    \n",
        "    # Initialize classifier if needed (for EfficientNet)\n",
        "    if hasattr(model, '_ensure_classifier_initialized') and model.classifier is None:\n",
        "        dummy_input = torch.randn(1, 3, img_size, img_size).to(device)\n",
        "        model._ensure_classifier_initialized(dummy_input)\n",
        "    \n",
        "    # Create datasets\n",
        "    train_ds = EmojiDatasetWithMetadata(train_paths, train_labels, processor, model_tag, use_augmentation=True)\n",
        "    val_ds = EmojiDatasetWithMetadata(val_paths, val_labels, processor, model_tag, use_augmentation=False)\n",
        "    \n",
        "    bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
        "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, \n",
        "                              num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
        "    val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False,\n",
        "                            num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
        "    \n",
        "    scaler = None\n",
        "    if torch.cuda.is_available() and not torch.cuda.is_bf16_supported():\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    best_acc = -1.0\n",
        "    best_path = f'{checkpoint_prefix}_V17.pt'\n",
        "    \n",
        "    # ========================================\n",
        "    # Phase 1: Warmup (Frozen Backbone)\n",
        "    # ========================================\n",
        "    print(f'\\n[Phase 1] Warmup - Training head only (frozen backbone)')\n",
        "    model.freeze_backbone()\n",
        "    \n",
        "    # Only train unfrozen parameters\n",
        "    head_params = [p for p in model.parameters() if p.requires_grad]\n",
        "    optimizer = torch.optim.Adam(head_params, lr=WARMUP_LR)\n",
        "    \n",
        "    for epoch in range(WARMUP_EPOCHS):\n",
        "        print(f'\\n[Warmup] Epoch {epoch+1}/{WARMUP_EPOCHS}')\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, device, scaler, class_weights,\n",
        "                                      use_mixup=False)\n",
        "        va_loss, va_acc, _, _ = validate(model, val_loader, device)\n",
        "        print(f'Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% | Val: loss={va_loss:.4f} acc={va_acc:.2f}%')\n",
        "        \n",
        "        if va_acc > best_acc + 1e-6:\n",
        "            best_acc = va_acc\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print(f'✓ Saved {best_path}')\n",
        "    \n",
        "    # ========================================\n",
        "    # Phase 2: Fine-tuning (Partial Unfreeze)\n",
        "    # ========================================\n",
        "    print(f'\\n[Phase 2] Fine-tuning - Partial unfreeze')\n",
        "    model.unfreeze_backbone()\n",
        "    \n",
        "    # Partial unfreeze: keep early 70% frozen for stability\n",
        "    backbone_params = list(model.base_model.parameters())\n",
        "    freeze_upto = int(len(backbone_params) * 0.7)\n",
        "    for i, p in enumerate(backbone_params):\n",
        "        p.requires_grad = (i >= freeze_upto)\n",
        "    # Heads always trainable\n",
        "    for p in model.metadata_mlp.parameters():\n",
        "        p.requires_grad = True\n",
        "    if getattr(model, 'classifier', None) is not None:\n",
        "        for p in model.classifier.parameters():\n",
        "            p.requires_grad = True\n",
        "    \n",
        "    # Param groups: backbone vs head\n",
        "    head_params = list(model.metadata_mlp.parameters())\n",
        "    if getattr(model, 'classifier', None) is not None:\n",
        "        head_params += list(model.classifier.parameters())\n",
        "    backbone_trainable = [p for p in model.base_model.parameters() if p.requires_grad]\n",
        "    \n",
        "    optimizer = torch.optim.AdamW([\n",
        "        {'params': head_params, 'weight_decay': HEAD_WEIGHT_DECAY},\n",
        "        {'params': backbone_trainable, 'weight_decay': WEIGHT_DECAY},\n",
        "    ], lr=FINETUNE_LR)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='max', factor=0.5, patience=max(1, EARLY_STOPPING_PATIENCE), min_lr=1e-7\n",
        "    )\n",
        "    \n",
        "    bad_epochs = 0\n",
        "    for epoch in range(FINETUNE_EPOCHS):\n",
        "        print(f'\\n[Fine-tune] Epoch {epoch+1}/{FINETUNE_EPOCHS}')\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, device, scaler, class_weights,\n",
        "                                      use_mixup=USE_MIXUP, mixup_alpha=MIXUP_ALPHA)\n",
        "        va_loss, va_acc, _, _ = validate(model, val_loader, device)\n",
        "        \n",
        "        scheduler.step(va_acc)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f'Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% | Val: loss={va_loss:.4f} acc={va_acc:.2f}% | LR: {current_lr:.2e}')\n",
        "        \n",
        "        if va_acc > best_acc + 1e-6:\n",
        "            best_acc = va_acc\n",
        "            bad_epochs = 0\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print(f'✓ Saved {best_path}')\n",
        "        else:\n",
        "            bad_epochs += 1\n",
        "            if bad_epochs >= EARLY_STOPPING_PATIENCE:\n",
        "                print(f'Early stopping after {EARLY_STOPPING_PATIENCE} epochs without improvement')\n",
        "                break\n",
        "    \n",
        "    # Load best weights\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    print(f'\\n✓ Two-phase training complete! Best val accuracy: {best_acc:.2f}%')\n",
        "    \n",
        "    return model, best_path, best_acc\n",
        "\n",
        "print('Two-phase training function defined')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 8: Data Loading Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_hf_dataset_with_mapping(dataset_path: str) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"\n",
        "    Prepare HuggingFace dataset by finding all images and mapping vendor labels.\n",
        "    Maps 11 HF classes to 7 target classes using HF_TO_TARGET_MAPPING.\n",
        "    \"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    dataset_path = Path(dataset_path)\n",
        "    image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
        "    \n",
        "    # HF dataset has vendor folders\n",
        "    for hf_vendor, target_vendor in HF_TO_TARGET_MAPPING.items():\n",
        "        if target_vendor not in VENDOR_TO_IDX:\n",
        "            continue\n",
        "        \n",
        "        vendor_dir = dataset_path / hf_vendor\n",
        "        if vendor_dir.exists() and vendor_dir.is_dir():\n",
        "            for ext in image_extensions:\n",
        "                images = list(vendor_dir.glob(f\"*{ext}\"))\n",
        "                for img_path in images:\n",
        "                    image_paths.append(str(img_path))\n",
        "                    labels.append(VENDOR_TO_IDX[target_vendor])\n",
        "    \n",
        "    # Fallback: scan all images if no vendor folders found\n",
        "    if len(image_paths) == 0:\n",
        "        for ext in image_extensions:\n",
        "            all_images = list(dataset_path.rglob(f\"*{ext}\"))\n",
        "            for img_path in all_images:\n",
        "                filename = img_path.name.lower()\n",
        "                parent_dir = img_path.parent.name\n",
        "                for hf_vendor, target_vendor in HF_TO_TARGET_MAPPING.items():\n",
        "                    if target_vendor not in VENDOR_TO_IDX:\n",
        "                        continue\n",
        "                    if hf_vendor.lower() in filename or hf_vendor.lower() in parent_dir.lower():\n",
        "                        image_paths.append(str(img_path))\n",
        "                        labels.append(VENDOR_TO_IDX[target_vendor])\n",
        "                        break\n",
        "    \n",
        "    print(f'Loaded {len(image_paths)} images from HuggingFace dataset')\n",
        "    if len(labels) > 0:\n",
        "        label_counts = np.bincount(np.array(labels), minlength=NUM_CLASSES)\n",
        "        print(f'Label distribution: {label_counts}')\n",
        "    \n",
        "    return image_paths, labels\n",
        "\n",
        "\n",
        "def prepare_dataset_from_csv(train_dir: Path, csv_path: Path) -> Tuple[List[str], List[int]]:\n",
        "    \"\"\"Load dataset from CSV labels file.\"\"\"\n",
        "    train_dir = Path(train_dir)\n",
        "    csv_path = Path(csv_path)\n",
        "    \n",
        "    if not train_dir.exists() or not csv_path.exists():\n",
        "        raise FileNotFoundError(f'Missing train_dir or csv: {train_dir} / {csv_path}')\n",
        "    \n",
        "    df = pd.read_csv(csv_path)\n",
        "    label_map = {v: VENDOR_TO_IDX[v] for v in VENDOR_CLASSES}\n",
        "    \n",
        "    img_paths = []\n",
        "    labels = []\n",
        "    missing = 0\n",
        "    unmapped = 0\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        img_id = str(row['Id']).zfill(5)\n",
        "        lab = str(row['Label']).lower()\n",
        "        \n",
        "        if lab not in label_map:\n",
        "            unmapped += 1\n",
        "            continue\n",
        "        \n",
        "        found = None\n",
        "        for ext in ('.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'):\n",
        "            p = train_dir / f'{img_id}{ext}'\n",
        "            if p.exists():\n",
        "                found = str(p)\n",
        "                break\n",
        "        \n",
        "        if found is None:\n",
        "            missing += 1\n",
        "            continue\n",
        "        \n",
        "        img_paths.append(found)\n",
        "        labels.append(int(label_map[lab]))\n",
        "    \n",
        "    print(f'Loaded: {len(img_paths)} images')\n",
        "    print(f'Unmapped labels: {unmapped}, Missing files: {missing}')\n",
        "    if labels:\n",
        "        print(f'Label distribution: {np.bincount(np.array(labels), minlength=NUM_CLASSES)}')\n",
        "    \n",
        "    return img_paths, labels\n",
        "\n",
        "print('Data loading functions defined')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 9: HuggingFace Pre-training & K-Fold Training Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretrain_on_hf_dataset(model_id: str, tag: str, class_weights=None):\n",
        "    \"\"\"\n",
        "    Pre-train model on HuggingFace emoji dataset with two-phase training.\n",
        "    Returns: (model, processor, checkpoint_path, best_acc)\n",
        "    \"\"\"\n",
        "    seed_everything(RANDOM_STATE)\n",
        "    \n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Pre-training {tag} on HuggingFace dataset')\n",
        "    print(f'{\"=\"*60}')\n",
        "    \n",
        "    # Download HF dataset\n",
        "    hf_path = kagglehub.dataset_download(HF_DATASET_ID)\n",
        "    print(f'HuggingFace dataset path: {hf_path}')\n",
        "    \n",
        "    hf_paths, hf_labels = prepare_hf_dataset_with_mapping(hf_path)\n",
        "    \n",
        "    if len(hf_paths) == 0:\n",
        "        raise ValueError('No images found in HuggingFace dataset')\n",
        "    \n",
        "    # Split HF dataset\n",
        "    labels_arr = np.array(hf_labels)\n",
        "    min_count = np.bincount(labels_arr, minlength=NUM_CLASSES).min()\n",
        "    can_stratify = (min_count >= 2)\n",
        "    \n",
        "    hf_train_paths, hf_val_paths, hf_train_y, hf_val_y = train_test_split(\n",
        "        hf_paths, hf_labels,\n",
        "        test_size=0.1, random_state=RANDOM_STATE,\n",
        "        stratify=hf_labels if can_stratify else None\n",
        "    )\n",
        "    \n",
        "    print(f'HF Train: {len(hf_train_paths)}, HF Val: {len(hf_val_paths)}')\n",
        "    \n",
        "    # Create model\n",
        "    model, processor = create_model(model_id, tag)\n",
        "    \n",
        "    # Two-phase training on HF dataset\n",
        "    checkpoint_prefix = f'hf_pretrained_{tag}'\n",
        "    model, best_path, best_acc = two_phase_train(\n",
        "        model, processor, tag,\n",
        "        hf_train_paths, hf_train_y,\n",
        "        hf_val_paths, hf_val_y,\n",
        "        checkpoint_prefix, class_weights\n",
        "    )\n",
        "    \n",
        "    return model, processor, best_path, best_acc\n",
        "\n",
        "print('HF pre-training function defined')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def kfold_training_pipeline(all_paths: List[str], all_labels: List[int], \n",
        "                            pretrained_checkpoints: Dict[str, str] = None):\n",
        "    \"\"\"\n",
        "    K-Fold cross-validation training pipeline.\n",
        "    Trains each model architecture on each fold.\n",
        "    \n",
        "    Returns: List of (model, processor, tag, fold, checkpoint_path, val_acc)\n",
        "    \"\"\"\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'K-Fold Training Pipeline ({NUM_FOLDS} folds x {len(MODELS)} models)')\n",
        "    print(f'{\"=\"*60}')\n",
        "    \n",
        "    # Compute class weights\n",
        "    class_weights = compute_class_weights(all_labels)\n",
        "    \n",
        "    # Setup K-Fold\n",
        "    skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    \n",
        "    trained_models = []\n",
        "    \n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(skf.split(all_paths, all_labels)):\n",
        "        print(f'\\n{\"=\"*60}')\n",
        "        print(f'FOLD {fold_idx + 1}/{NUM_FOLDS}')\n",
        "        print(f'{\"=\"*60}')\n",
        "        \n",
        "        # Split data for this fold\n",
        "        fold_train_paths = [all_paths[i] for i in train_idx]\n",
        "        fold_train_labels = [all_labels[i] for i in train_idx]\n",
        "        fold_val_paths = [all_paths[i] for i in val_idx]\n",
        "        fold_val_labels = [all_labels[i] for i in val_idx]\n",
        "        \n",
        "        print(f'Fold {fold_idx + 1}: Train={len(fold_train_paths)}, Val={len(fold_val_paths)}')\n",
        "        \n",
        "        # Train each model architecture on this fold\n",
        "        for model_id, tag, img_size in MODELS:\n",
        "            print(f'\\n--- Training {tag} (fold {fold_idx + 1}) ---')\n",
        "            \n",
        "            # Create fresh model\n",
        "            model, processor = create_model(model_id, tag)\n",
        "            \n",
        "            # Load pre-trained weights if available\n",
        "            if pretrained_checkpoints and tag in pretrained_checkpoints:\n",
        "                pretrained_path = pretrained_checkpoints[tag]\n",
        "                if os.path.exists(pretrained_path):\n",
        "                    model.load_state_dict(torch.load(pretrained_path, map_location=device))\n",
        "                    print(f'✓ Loaded pre-trained weights from {pretrained_path}')\n",
        "            \n",
        "            # Two-phase training on this fold\n",
        "            checkpoint_prefix = f'fold{fold_idx + 1}_{tag}'\n",
        "            model, best_path, best_acc = two_phase_train(\n",
        "                model, processor, tag,\n",
        "                fold_train_paths, fold_train_labels,\n",
        "                fold_val_paths, fold_val_labels,\n",
        "                checkpoint_prefix, class_weights\n",
        "            )\n",
        "            \n",
        "            trained_models.append({\n",
        "                'model': model,\n",
        "                'processor': processor,\n",
        "                'tag': tag,\n",
        "                'model_id': model_id,\n",
        "                'fold': fold_idx + 1,\n",
        "                'checkpoint': best_path,\n",
        "                'val_acc': best_acc\n",
        "            })\n",
        "            \n",
        "            # Move to CPU to save GPU memory\n",
        "            if device.type == 'cuda':\n",
        "                model = model.to('cpu')\n",
        "                torch.cuda.empty_cache()\n",
        "    \n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'K-Fold Training Complete!')\n",
        "    print(f'Total trained models: {len(trained_models)}')\n",
        "    print(f'{\"=\"*60}')\n",
        "    \n",
        "    # Print summary\n",
        "    for m in trained_models:\n",
        "        print(f\"  {m['tag']} fold{m['fold']}: {m['val_acc']:.2f}%\")\n",
        "    \n",
        "    return trained_models\n",
        "\n",
        "print('K-Fold training pipeline defined')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 10: Optimized Feature Extraction for Meta-Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _prob_cols_for_members(trained_models: List[Dict], num_augmentations: int) -> List[str]:\n",
        "    \"\"\"Generate column names for probability features (no folds in V17).\"\"\"\n",
        "    cols = []\n",
        "    for m in trained_models:\n",
        "        tag = m['tag']\n",
        "        for i in range(num_augmentations):\n",
        "            for c in range(NUM_CLASSES):\n",
        "                cols.append(f'prob_{tag}_aug{i}_cls{c}')\n",
        "    return cols\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_features_batched(image_paths: List[str], trained_models: List[Dict], \n",
        "                           num_augmentations: int, batch_size: int = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Optimized batched feature extraction for maximum GPU efficiency.\n",
        "    - Pre-loads all images and generates augmentations once per model (size-specific)\n",
        "    - Processes all batches for each model before switching\n",
        "    - Minimizes GPU model loading/unloading overhead\n",
        "    \"\"\"\n",
        "    if batch_size is None:\n",
        "        batch_size = FEATURE_BATCH_SIZE\n",
        "    \n",
        "    num_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
        "    \n",
        "    # Step 1: Pre-load all images and extract stats\n",
        "    print('Pre-loading images and extracting stats...')\n",
        "    all_stats = []\n",
        "    all_images = []\n",
        "    \n",
        "    for img_path in tqdm(image_paths, desc='Loading images', disable=(not SHOW_PROGRESS)):\n",
        "        stats = extract_image_properties(img_path)\n",
        "        img = load_image_rgb(img_path)\n",
        "        all_stats.append(stats)\n",
        "        all_images.append((img, img_path))\n",
        "    \n",
        "    # Step 2: Process each model\n",
        "    model_probs = {}  # tag -> list of batch probability arrays\n",
        "    \n",
        "    # Group models by image size for efficient augmentation caching\n",
        "    models_by_size = {}\n",
        "    for m in trained_models:\n",
        "        model_config = next((cfg for cfg in MODELS if cfg[1] == m['tag']), None)\n",
        "        img_size = model_config[2] if model_config else 224\n",
        "        models_by_size.setdefault(img_size, []).append(m)\n",
        "    \n",
        "    for img_size, size_models in models_by_size.items():\n",
        "        print(f'\\nProcessing models with image size {img_size}x{img_size}...')\n",
        "        \n",
        "        # Generate augmentations for this size\n",
        "        tta = MultiScaleTTA(base_image_size=img_size)\n",
        "        all_augmentations = []\n",
        "        for img, img_path in tqdm(all_images, desc=f'Generating {img_size}px augmentations', disable=(not SHOW_PROGRESS)):\n",
        "            augs = tta.get_augmentations(img, num_augmentations=num_augmentations, seed_source=img_path)\n",
        "            all_augmentations.append(augs)\n",
        "        \n",
        "        # Process each model of this size\n",
        "        for m in tqdm(size_models, desc=f'Processing {img_size}px models', disable=(not SHOW_PROGRESS)):\n",
        "            model = m['model']\n",
        "            processor = m['processor']\n",
        "            tag = m['tag']\n",
        "            \n",
        "            # Load model to GPU\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "            \n",
        "            batch_probs = []\n",
        "            \n",
        "            for start in range(0, len(image_paths), batch_size):\n",
        "                batch_end = min(start + batch_size, len(image_paths))\n",
        "                B = batch_end - start\n",
        "                \n",
        "                # Collect augmentations for this batch\n",
        "                batch_augs = []\n",
        "                batch_meta = []\n",
        "                for img_idx in range(start, batch_end):\n",
        "                    batch_augs.extend(all_augmentations[img_idx])\n",
        "                    meta = extract_metadata(image_paths[img_idx])\n",
        "                    for _ in range(num_augmentations):\n",
        "                        batch_meta.append([meta['height_norm'], meta['width_norm'], meta['has_alpha']])\n",
        "                \n",
        "                # Process batch\n",
        "                inputs = processor(batch_augs, return_tensors='pt')\n",
        "                x = inputs['pixel_values'].to(device)\n",
        "                meta_tensor = torch.tensor(batch_meta, dtype=torch.float32).to(device)\n",
        "                \n",
        "                out = model(pixel_values=x, metadata=meta_tensor)\n",
        "                probs = torch.softmax(out.logits, dim=-1)\n",
        "                probs = probs.view(B, num_augmentations, -1).detach().cpu().numpy()\n",
        "                batch_probs.append(probs)\n",
        "            \n",
        "            model_probs[tag] = batch_probs\n",
        "            \n",
        "            # Move model back to CPU\n",
        "            if device.type == 'cuda':\n",
        "                model.to('cpu')\n",
        "                torch.cuda.empty_cache()\n",
        "    \n",
        "    # Step 3: Combine stats and probabilities into final feature matrix\n",
        "    print('Combining features...')\n",
        "    all_rows = []\n",
        "    \n",
        "    for img_idx, stats in enumerate(all_stats):\n",
        "        row = dict(stats)\n",
        "        \n",
        "        batch_idx = img_idx // batch_size\n",
        "        batch_local_idx = img_idx % batch_size\n",
        "        \n",
        "        for m in trained_models:\n",
        "            tag = m['tag']\n",
        "            batch_probs_list = model_probs[tag]\n",
        "            batch_probs = batch_probs_list[batch_idx]\n",
        "            \n",
        "            for a in range(num_augmentations):\n",
        "                for c in range(NUM_CLASSES):\n",
        "                    row[f'prob_{tag}_aug{a}_cls{c}'] = float(batch_probs[batch_local_idx, a, c])\n",
        "        \n",
        "        all_rows.append(row)\n",
        "    \n",
        "    df = pd.DataFrame(all_rows)\n",
        "    prob_cols = _prob_cols_for_members(trained_models, num_augmentations)\n",
        "    all_cols = STAT_COLS + prob_cols\n",
        "    \n",
        "    for col in all_cols:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0.0\n",
        "    \n",
        "    return df[all_cols]\n",
        "\n",
        "print('Feature extraction function defined')\n",
        "print(f'Expected features: {len(STAT_COLS)} stats + {len(MODELS) * NUM_TTA_AUGS * NUM_CLASSES} probabilities')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 11: Pseudo-Labeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pseudo_label_training(trained_models: List[Dict], meta_model, \n",
        "                          train_paths: List[str], train_labels: List[int],\n",
        "                          test_paths: List[str]):\n",
        "    \"\"\"\n",
        "    Pseudo-labeling: use confident test predictions as additional training data.\n",
        "    \n",
        "    1. Generate predictions on test set\n",
        "    2. Filter to high-confidence predictions\n",
        "    3. Add pseudo-labeled data to training set\n",
        "    4. Retrain models with augmented dataset\n",
        "    \"\"\"\n",
        "    print(f'\\n{\"=\"*60}')\n",
        "    print(f'Pseudo-Labeling (confidence > {PSEUDO_LABEL_CONFIDENCE})')\n",
        "    print(f'{\"=\"*60}')\n",
        "    \n",
        "    # Step 1: Extract features for test set\n",
        "    print('\\nExtracting features for test set...')\n",
        "    X_test = build_features_batched(test_paths, trained_models, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
        "    \n",
        "    # Step 2: Get predictions and confidence\n",
        "    test_probs = meta_model.predict_proba(X_test)\n",
        "    test_preds = test_probs.argmax(axis=1)\n",
        "    test_confidence = test_probs.max(axis=1)\n",
        "    \n",
        "    # Step 3: Filter high-confidence predictions\n",
        "    confident_mask = test_confidence >= PSEUDO_LABEL_CONFIDENCE\n",
        "    num_confident = confident_mask.sum()\n",
        "    \n",
        "    print(f'Test samples: {len(test_paths)}')\n",
        "    print(f'Confident predictions (>={PSEUDO_LABEL_CONFIDENCE}): {num_confident} ({100*num_confident/len(test_paths):.1f}%)')\n",
        "    \n",
        "    if num_confident < 10:\n",
        "        print('Too few confident predictions for pseudo-labeling. Skipping.')\n",
        "        return trained_models\n",
        "    \n",
        "    # Get pseudo-labeled data\n",
        "    pseudo_paths = [test_paths[i] for i in range(len(test_paths)) if confident_mask[i]]\n",
        "    pseudo_labels = [int(test_preds[i]) for i in range(len(test_paths)) if confident_mask[i]]\n",
        "    \n",
        "    # Show pseudo-label distribution\n",
        "    pseudo_dist = np.bincount(pseudo_labels, minlength=NUM_CLASSES)\n",
        "    print(f'Pseudo-label distribution: {pseudo_dist}')\n",
        "    \n",
        "    # Step 4: Combine with original training data\n",
        "    combined_paths = train_paths + pseudo_paths\n",
        "    combined_labels = train_labels + pseudo_labels\n",
        "    \n",
        "    print(f'Combined training set: {len(combined_paths)} samples')\n",
        "    \n",
        "    # Step 5: Retrain models with pseudo-labeled data (fewer epochs)\n",
        "    print('\\nRetraining with pseudo-labeled data...')\n",
        "    \n",
        "    class_weights = compute_class_weights(combined_labels)\n",
        "    \n",
        "    updated_models = []\n",
        "    for m in trained_models:\n",
        "        model = m['model']\n",
        "        processor = m['processor']\n",
        "        tag = m['tag']\n",
        "        \n",
        "        print(f'\\n--- Retraining {tag} with pseudo-labels ---')\n",
        "        \n",
        "        model = model.to(device)\n",
        "        \n",
        "        # Create dataset with combined data\n",
        "        train_ds = EmojiDatasetWithMetadata(\n",
        "            combined_paths, combined_labels, processor, tag, use_augmentation=True\n",
        "        )\n",
        "        bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
        "        train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True,\n",
        "                                  num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
        "        \n",
        "        # Train with low LR\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=PSEUDO_LABEL_LR, weight_decay=WEIGHT_DECAY)\n",
        "        scaler = None\n",
        "        if torch.cuda.is_available() and not torch.cuda.is_bf16_supported():\n",
        "            scaler = torch.cuda.amp.GradScaler()\n",
        "        \n",
        "        for epoch in range(PSEUDO_LABEL_EPOCHS):\n",
        "            tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, device, scaler, class_weights,\n",
        "                                          use_mixup=USE_MIXUP, mixup_alpha=MIXUP_ALPHA)\n",
        "            print(f'  Epoch {epoch+1}/{PSEUDO_LABEL_EPOCHS}: loss={tr_loss:.4f} acc={tr_acc:.2f}%')\n",
        "        \n",
        "        # Save updated checkpoint\n",
        "        checkpoint_path = f'pseudo_{tag}_V17.pt'\n",
        "        torch.save(model.state_dict(), checkpoint_path)\n",
        "        \n",
        "        updated_models.append({\n",
        "            'model': model,\n",
        "            'processor': processor,\n",
        "            'tag': tag,\n",
        "            'model_id': m['model_id'],\n",
        "            'checkpoint': checkpoint_path,\n",
        "            'val_acc': m.get('val_acc')\n",
        "        })\n",
        "        \n",
        "        if device.type == 'cuda':\n",
        "            model = model.to('cpu')\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    print(f'\\n✓ Pseudo-labeling complete! {len(updated_models)} models updated')\n",
        "    \n",
        "    return updated_models\n",
        "\n",
        "print('Pseudo-labeling function defined')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 12: Main Execution Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 1: Load Target Dataset\n",
        "# ============================================================================\n",
        "print('='*60)\n",
        "print('STEP 1: Loading Target Dataset')\n",
        "print('='*60)\n",
        "\n",
        "all_paths, all_labels = prepare_dataset_from_csv(SECOND_DATASET_TRAIN_DIR, SECOND_DATASET_CSV_PATH)\n",
        "\n",
        "# Show class distribution\n",
        "print(f'\\nTotal samples: {len(all_paths)}')\n",
        "print(f'Class distribution: {np.bincount(np.array(all_labels), minlength=NUM_CLASSES)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 2: Pre-train Models on HuggingFace Dataset\n",
        "# ============================================================================\n",
        "print('='*60)\n",
        "print('STEP 2: Pre-training on HuggingFace Dataset')\n",
        "print('='*60)\n",
        "\n",
        "pretrained_checkpoints = {}\n",
        "\n",
        "for model_id, tag, img_size in MODELS:\n",
        "    print(f'\\n--- Pre-training {tag} ---')\n",
        "    model, processor, checkpoint, best_acc = pretrain_on_hf_dataset(model_id, tag)\n",
        "    pretrained_checkpoints[tag] = checkpoint\n",
        "    \n",
        "    # Move to CPU to save memory\n",
        "    if device.type == 'cuda':\n",
        "        model = model.to('cpu')\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\n✓ Pre-trained checkpoints: {list(pretrained_checkpoints.keys())}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 3: Train on Target Dataset (No K-Fold)\n",
        "# ============================================================================\n",
        "print('='*60)\n",
        "print('STEP 3: Training on Target Dataset (No K-Fold)')\n",
        "print('='*60)\n",
        "\n",
        "# Stratified split\n",
        "val_size = 0.10\n",
        "min_count = np.bincount(np.array(all_labels), minlength=NUM_CLASSES).min()\n",
        "can_stratify = (min_count >= 2)\n",
        "train_paths, val_paths, train_y, val_y = train_test_split(\n",
        "    all_paths, all_labels, test_size=val_size, random_state=RANDOM_STATE,\n",
        "    stratify=all_labels if can_stratify else None\n",
        ")\n",
        "print(f'Train: {len(train_paths)}, Val: {len(val_paths)}')\n",
        "print('Train dist:', np.bincount(np.array(train_y), minlength=NUM_CLASSES))\n",
        "print('Val   dist:', np.bincount(np.array(val_y), minlength=NUM_CLASSES))\n",
        "\n",
        "class_weights = compute_class_weights(train_y)\n",
        "\n",
        "trained_models = []\n",
        "for model_id, tag, img_size in MODELS:\n",
        "    print(f'\\n--- Training {tag} ---')\n",
        "    model, processor = create_model(model_id, tag)\n",
        "    \n",
        "    # Load HF pretrained weights if available\n",
        "    if tag in pretrained_checkpoints and os.path.exists(pretrained_checkpoints[tag]):\n",
        "        model.load_state_dict(torch.load(pretrained_checkpoints[tag], map_location=device))\n",
        "        print(f'✓ Loaded HF pretrained weights from {pretrained_checkpoints[tag]}')\n",
        "    \n",
        "    checkpoint_prefix = f'finetuned_{tag}'\n",
        "    model, best_path, best_acc = two_phase_train(\n",
        "        model, processor, tag,\n",
        "        train_paths, train_y,\n",
        "        val_paths, val_y,\n",
        "        checkpoint_prefix, class_weights\n",
        "    )\n",
        "    \n",
        "    trained_models.append({\n",
        "        'model': model,\n",
        "        'processor': processor,\n",
        "        'tag': tag,\n",
        "        'model_id': model_id,\n",
        "        'checkpoint': best_path,\n",
        "        'val_acc': best_acc\n",
        "    })\n",
        "    \n",
        "    # Move to CPU to save GPU memory\n",
        "    if device.type == 'cuda':\n",
        "        model = model.to('cpu')\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "print(f'\\n✓ Total trained models: {len(trained_models)}')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 13: LightGBM Meta-Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 4: Build Features and Train LightGBM Meta-Model\n",
        "# ============================================================================\n",
        "print('='*60)\n",
        "print('STEP 4: Training LightGBM Meta-Model')\n",
        "print('='*60)\n",
        "\n",
        "# For validation, use one fold's validation set\n",
        "# (In K-Fold, we use out-of-fold predictions for unbiased evaluation)\n",
        "# Here we'll use a simple split for quick validation\n",
        "\n",
        "# Split data for meta-model validation\n",
        "meta_train_paths, meta_val_paths, meta_train_y, meta_val_y = train_test_split(\n",
        "    all_paths, all_labels,\n",
        "    test_size=0.1, random_state=RANDOM_STATE,\n",
        "    stratify=all_labels\n",
        ")\n",
        "\n",
        "print(f'Meta train: {len(meta_train_paths)}, Meta val: {len(meta_val_paths)}')\n",
        "\n",
        "# Extract features\n",
        "print('\\nExtracting training features...')\n",
        "X_train = build_features_batched(meta_train_paths, trained_models, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
        "print('\\nExtracting validation features...')\n",
        "X_val = build_features_batched(meta_val_paths, trained_models, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
        "\n",
        "y_train = np.array(meta_train_y)\n",
        "y_val = np.array(meta_val_y)\n",
        "\n",
        "print(f'\\nX_train: {X_train.shape}, X_val: {X_val.shape}')\n",
        "\n",
        "# Train LightGBM meta-model\n",
        "if USE_LIGHTGBM and HAS_LGB:\n",
        "    print('\\nTraining LightGBM meta-model...')\n",
        "    meta_model = lgb.LGBMClassifier(**LGB_PARAMS)\n",
        "    meta_model.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate\n",
        "    val_pred = meta_model.predict(X_val)\n",
        "    val_acc = accuracy_score(y_val, val_pred) * 100.0\n",
        "    \n",
        "    print(f'\\nMeta-model validation accuracy: {val_acc:.2f}%')\n",
        "    print('\\nClassification Report:')\n",
        "    print(classification_report(y_val, val_pred, target_names=VENDOR_CLASSES))\n",
        "    \n",
        "    # Save model\n",
        "    meta_model.booster_.save_model('meta_lgb_v17.txt')\n",
        "    print('✓ Saved meta_lgb_v17.txt')\n",
        "else:\n",
        "    raise RuntimeError('LightGBM not available!')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 14: Pseudo-Labeling & Final Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 5: Load Test Data\n",
        "# ============================================================================\n",
        "print('='*60)\n",
        "print('STEP 5: Loading Test Data')\n",
        "print('='*60)\n",
        "\n",
        "test_dir = SECOND_DATASET_TEST_DIR\n",
        "if not test_dir.exists():\n",
        "    raise FileNotFoundError(f'Missing test dir: {test_dir}')\n",
        "\n",
        "test_paths = []\n",
        "for ext in ('.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'):\n",
        "    test_paths += [str(p) for p in test_dir.rglob(f'*{ext}')]\n",
        "test_paths = sorted(set(test_paths))\n",
        "\n",
        "print(f'Found {len(test_paths)} test images')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 6: Pseudo-Labeling (Optional - Set to True to enable)\n",
        "# ============================================================================\n",
        "ENABLE_PSEUDO_LABELING = True\n",
        "\n",
        "if ENABLE_PSEUDO_LABELING:\n",
        "    print('='*60)\n",
        "    print('STEP 6: Pseudo-Labeling')\n",
        "    print('='*60)\n",
        "    \n",
        "    # Use pseudo-labeling to improve models\n",
        "    trained_models = pseudo_label_training(\n",
        "        trained_models, meta_model,\n",
        "        all_paths, all_labels,\n",
        "        test_paths\n",
        "    )\n",
        "else:\n",
        "    print('Pseudo-labeling disabled. Skipping...')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 7: Train Final Meta-Model on All Data\n",
        "# ============================================================================\n",
        "print('='*60)\n",
        "print('STEP 7: Training Final Meta-Model on All Data')\n",
        "print('='*60)\n",
        "\n",
        "# Extract features for all training data\n",
        "print('\\nExtracting features for all training data...')\n",
        "X_all = build_features_batched(all_paths, trained_models, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
        "y_all = np.array(all_labels)\n",
        "\n",
        "print(f'\\nX_all: {X_all.shape}')\n",
        "\n",
        "# Train final meta-model on all data\n",
        "print('\\nTraining final LightGBM meta-model...')\n",
        "meta_final = lgb.LGBMClassifier(**LGB_PARAMS)\n",
        "meta_final.fit(X_all, y_all)\n",
        "\n",
        "# Save final model\n",
        "meta_final.booster_.save_model('meta_lgb_v17_final.txt')\n",
        "print('✓ Saved meta_lgb_v17_final.txt')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Section 15: Generate Test Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# STEP 8: Generate Test Predictions\n",
        "# ============================================================================\n",
        "print('='*60)\n",
        "print('STEP 8: Generating Test Predictions')\n",
        "print('='*60)\n",
        "\n",
        "# Extract features for test set\n",
        "print('\\nExtracting features for test set...')\n",
        "X_test = build_features_batched(test_paths, trained_models, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
        "\n",
        "print(f'\\nX_test: {X_test.shape}')\n",
        "\n",
        "# Generate predictions\n",
        "print('\\nGenerating predictions...')\n",
        "test_preds = meta_final.predict(X_test)\n",
        "test_probs = meta_final.predict_proba(X_test)\n",
        "\n",
        "# Get prediction confidence\n",
        "test_confidence = test_probs.max(axis=1)\n",
        "avg_confidence = test_confidence.mean()\n",
        "print(f'Average prediction confidence: {avg_confidence:.4f}')\n",
        "\n",
        "# Create submission\n",
        "pred_ids = []\n",
        "pred_labels = []\n",
        "\n",
        "for img_path, pred in zip(test_paths, test_preds):\n",
        "    img_id = Path(img_path).stem\n",
        "    pred = int(pred)\n",
        "    pred = max(0, min(pred, NUM_CLASSES - 1))\n",
        "    pred_ids.append(img_id)\n",
        "    pred_labels.append(IDX_TO_VENDOR[pred])\n",
        "\n",
        "# Show prediction distribution\n",
        "pred_dist = np.bincount(test_preds.astype(int), minlength=NUM_CLASSES)\n",
        "print(f'\\nPrediction distribution:')\n",
        "for i, (cls, count) in enumerate(zip(VENDOR_CLASSES, pred_dist)):\n",
        "    print(f'  {cls:12s}: {count} ({100*count/len(test_preds):.1f}%)')\n",
        "\n",
        "# Save submission\n",
        "out_path = Path(PREDICTIONS_OUTPUT_FILE)\n",
        "with out_path.open('w') as f:\n",
        "    f.write('Id,Label\\n')\n",
        "    for img_id, label in zip(pred_ids, pred_labels):\n",
        "        f.write(f'{str(img_id).strip()},{label}\\n')\n",
        "\n",
        "print(f'\\n✓ Saved predictions to: {out_path}')\n",
        "print(f'  Total predictions: {len(pred_labels)}')\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary (V17)\n",
        "\n",
        "- 5 architectures: EffNet B0/B1/B2 + ConvNeXtV2 + DINOv2\n",
        "- No K-Fold: single stratified split, each model trained once\n",
        "- Gentle fine-tune: lower LR, shorter fine-tune, lower scheduler patience, partial unfreeze\n",
        "- Regularization: higher head dropout, stronger label smoothing, optional head-only weight decay\n",
        "- Augmentations: light MixUp, small RandAugment/ColorJitter, enhanced TTA (extra scale + blur/sharpen)\n",
        "- Pseudo-labeling: enabled with higher confidence (≈0.98–0.99), V17-tagged checkpoints\n",
        "- LightGBM: tighter regularization\n",
        "\n",
        "**Outputs:** `hf_pretrained_<tag>_V17.pt`, `finetuned_<tag>_V17.pt`, `pseudo_<tag>_V17.pt`, `meta_lgb_v17*.txt`, `predictions_V17.csv`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Dataset Class with Metadata\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
