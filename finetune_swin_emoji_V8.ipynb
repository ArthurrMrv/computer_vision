{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DINOv2-Large + XGBoost (TTA-columns) for Emoji Vendor Classification (V8)\n",
    "\n",
    "**Train only on the 2nd dataset** (CSV labels).\n",
    "\n",
    "**Pipeline:**\n",
    "- DINOv2-Large fine-tuning on 7 classes\n",
    "- Predictable TTA → create columns `dino_pred_0..dino_pred_{N-1}`\n",
    "- XGBoost learns to combine statistical features (incl. `original_mode`) + TTA columns\n",
    "\n",
    "**Classes:** apple, google, whatsapp, facebook, samsung, mozilla, messenger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (2.4.1+cu124)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.19.1+cu124)\n",
      "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (4.57.3)\n",
      "Requirement already satisfied: accelerate>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (1.26.3)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (2.3.3)\n",
      "Requirement already satisfied: pillow>=9.5.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (10.2.0)\n",
      "Requirement already satisfied: datasets>=2.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (4.4.2)\n",
      "Requirement already satisfied: scikit-learn>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (1.8.0)\n",
      "Requirement already satisfied: xgboost>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (3.1.2)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (3.10.8)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (4.67.1)\n",
      "Requirement already satisfied: kagglehub>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 28)) (0.3.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->-r requirements.txt (line 5)) (3.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 7)) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 7)) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 7)) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 7)) (2025.11.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 7)) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 7)) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.30.0->-r requirements.txt (line 7)) (0.7.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.0->-r requirements.txt (line 8)) (6.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 12)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 12)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->-r requirements.txt (line 12)) (2025.3)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 14)) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 14)) (0.4.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 14)) (0.27.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 14)) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.12.0->-r requirements.txt (line 14)) (0.70.18)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 17)) (1.16.3)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 17)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.3.0->-r requirements.txt (line 17)) (3.6.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 21)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 21)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 21)) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 21)) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->-r requirements.txt (line 21)) (3.3.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 14)) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 14)) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 14)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 14)) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 14)) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 14)) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.12.0->-r requirements.txt (line 14)) (0.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.30.0->-r requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->-r requirements.txt (line 12)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 7)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.30.0->-r requirements.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->-r requirements.txt (line 5)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->-r requirements.txt (line 5)) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 14)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 14)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 14)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 14)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 14)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 14)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.12.0->-r requirements.txt (line 14)) (1.22.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Using device: cuda\n",
      "GPU: Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=70\n",
      "HYPERPARAMETERS\n",
      "=70\n",
      "MODEL_ID: facebook/dinov2-small\n",
      "TRAIN_DIR: train\n",
      "CSV_PATH : train_labels.csv\n",
      "TEST_DIR : test\n",
      "NUM_TTA_AUGS: 10\n",
      "PREDICTIONS_OUTPUT_FILE: predictions.csv\n",
      "=70\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Model\n",
    "MODEL_ID = 'facebook/dinov2-small'\n",
    "\n",
    "# Second dataset paths (relative to this repo)\n",
    "SECOND_DATASET_BASE_PATH = '.'\n",
    "SECOND_DATASET_TRAIN_DIR = Path(SECOND_DATASET_BASE_PATH) / 'train'\n",
    "SECOND_DATASET_CSV_PATH = Path(SECOND_DATASET_BASE_PATH) / 'train_labels.csv'\n",
    "SECOND_DATASET_TEST_DIR = Path(SECOND_DATASET_BASE_PATH) / 'test'\n",
    "\n",
    "# TTA / Features\n",
    "NUM_TTA_AUGS = 10\n",
    "\n",
    "# Train\n",
    "VAL_SIZE = 0.10\n",
    "RANDOM_STATE = 42\n",
    "BATCH_SIZE_CUDA = 8\n",
    "BATCH_SIZE_CPU = 4\n",
    "NUM_EPOCHS = 7\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# XGBoost\n",
    "XGB_PARAMS = {\n",
    "    'n_estimators': 400,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'use_label_encoder': False\n",
    "}\n",
    "\n",
    "# Output\n",
    "PREDICTIONS_OUTPUT_FILE = 'predictions.csv'\n",
    "\n",
    "print('=', 70, sep='')\n",
    "print('HYPERPARAMETERS')\n",
    "print('=', 70, sep='')\n",
    "print('MODEL_ID:', MODEL_ID)\n",
    "print('TRAIN_DIR:', SECOND_DATASET_TRAIN_DIR)\n",
    "print('CSV_PATH :', SECOND_DATASET_CSV_PATH)\n",
    "print('TEST_DIR :', SECOND_DATASET_TEST_DIR)\n",
    "print('NUM_TTA_AUGS:', NUM_TTA_AUGS)\n",
    "print('PREDICTIONS_OUTPUT_FILE:', PREDICTIONS_OUTPUT_FILE)\n",
    "print('=', 70, sep='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Vendor Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 7\n",
      "Classes: ['apple', 'google', 'whatsapp', 'facebook', 'samsung', 'mozilla', 'messenger']\n"
     ]
    }
   ],
   "source": [
    "VENDOR_CLASSES = ['apple','google','whatsapp','facebook','samsung','mozilla','messenger']\n",
    "VENDOR_TO_IDX = {v:i for i,v in enumerate(VENDOR_CLASSES)}\n",
    "IDX_TO_VENDOR = {i:v for v,i in VENDOR_TO_IDX.items()}\n",
    "print('Num classes:', len(VENDOR_CLASSES))\n",
    "print('Classes:', VENDOR_CLASSES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Augmentation System (Predictable TTA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deterministic augmentation ready.\n"
     ]
    }
   ],
   "source": [
    "class DeterministicAugmentation:\n",
    "    def __init__(self, image_size=224, seed=42):\n",
    "        self.image_size = image_size\n",
    "        self.seed = seed\n",
    "        self.rotation_angles = [-10, -5, 5, 10]\n",
    "        self.crop_ratios = [0.75, 0.85, 0.9, 0.95]\n",
    "        self.color_jitter_params = {'brightness':0.3,'contrast':0.3,'saturation':0.3,'hue':0.1}\n",
    "        self.translate_range = (0.1, 0.1)\n",
    "        self.blur_sigma = (0.1, 0.5)\n",
    "\n",
    "    def _get_deterministic_seed(self, image_or_hash):\n",
    "        if isinstance(image_or_hash, Image.Image):\n",
    "            img_bytes = image_or_hash.tobytes()\n",
    "            return int(hashlib.md5(img_bytes).hexdigest()[:8], 16)\n",
    "        return hash(str(image_or_hash)) & 0xFFFFFFFF\n",
    "\n",
    "    def horizontal_flip(self, image):\n",
    "        return F.hflip(image)\n",
    "\n",
    "    def rotation(self, image, angle):\n",
    "        return F.rotate(image, angle)\n",
    "\n",
    "    def center_crop(self, image, crop_ratio=0.9):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        return F.center_crop(image, [crop,crop])\n",
    "\n",
    "    def corner_crop(self, image, crop_ratio=0.9, position='tl'):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        if position=='tl': return F.crop(image, 0, 0, crop, crop)\n",
    "        if position=='tr': return F.crop(image, 0, w-crop, crop, crop)\n",
    "        if position=='bl': return F.crop(image, h-crop, 0, crop, crop)\n",
    "        if position=='br': return F.crop(image, h-crop, w-crop, crop, crop)\n",
    "        return image\n",
    "\n",
    "    def resized_crop(self, image, crop_ratio=0.85):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        cropped = F.center_crop(image, [crop,crop])\n",
    "        return cropped.resize((self.image_size,self.image_size), Image.BILINEAR)\n",
    "\n",
    "    def color_jitter(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        b = 1.0 + np.random.uniform(-self.color_jitter_params['brightness'], self.color_jitter_params['brightness'])\n",
    "        c = 1.0 + np.random.uniform(-self.color_jitter_params['contrast'], self.color_jitter_params['contrast'])\n",
    "        s = 1.0 + np.random.uniform(-self.color_jitter_params['saturation'], self.color_jitter_params['saturation'])\n",
    "        h = np.random.uniform(-self.color_jitter_params['hue'], self.color_jitter_params['hue'])\n",
    "        img = F.adjust_brightness(image, b)\n",
    "        img = F.adjust_contrast(img, c)\n",
    "        img = F.adjust_saturation(img, s)\n",
    "        img = F.adjust_hue(img, h)\n",
    "        return img\n",
    "\n",
    "    def affine_transform(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        tx = np.random.uniform(-self.translate_range[0], self.translate_range[0])\n",
    "        ty = np.random.uniform(-self.translate_range[1], self.translate_range[1])\n",
    "        return F.affine(image, angle=0, translate=(tx*image.width, ty*image.height), scale=1.0, shear=0.0)\n",
    "\n",
    "    def gaussian_blur(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        sigma = np.random.uniform(self.blur_sigma[0], self.blur_sigma[1])\n",
    "        return F.gaussian_blur(image, kernel_size=3, sigma=[sigma,sigma])\n",
    "\n",
    "    def get_augmentations(self, image, num_augmentations=10, seed_source=None):\n",
    "        if seed_source is None:\n",
    "            seed_source = self._get_deterministic_seed(image)\n",
    "        seed_val = seed_source\n",
    "        augs = []\n",
    "        augs.append(image.resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        augs.append(self.horizontal_flip(image).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        for angle in self.rotation_angles[:max(0, min(4, num_augmentations-len(augs)))]:\n",
    "            augs.append(self.rotation(image, angle).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        corners = ['tl','tr','bl','br']\n",
    "        for cpos in corners[:max(0, min(4, num_augmentations-len(augs)))]:\n",
    "            augs.append(self.corner_crop(image, 0.9, cpos).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.center_crop(image, 0.9).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.resized_crop(image, 0.85))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.color_jitter(image, seed_val).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.affine_transform(image, seed_val+1).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.gaussian_blur(image, seed_val+2).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        return augs[:num_augmentations]\n",
    "\n",
    "augmentation_system = DeterministicAugmentation(image_size=224, seed=42)\n",
    "tta_aug = augmentation_system\n",
    "print('Deterministic augmentation ready.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Feature Extraction (incl. original_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num statistical features: 13\n"
     ]
    }
   ],
   "source": [
    "def extract_image_properties(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        mode_mapping = {'L':0,'LA':1,'P':2,'RGB':3,'RGBA':4}\n",
    "        original_mode = float(mode_mapping.get(img.mode, 3))\n",
    "        # Normalize image to RGB for pixel stats\n",
    "        if img.mode == 'P':\n",
    "            img = img.convert('RGBA')\n",
    "        if img.mode == 'RGBA':\n",
    "            bg = Image.new('RGB', img.size, (255,255,255))\n",
    "            bg.paste(img, mask=img.split()[3])\n",
    "            img = bg\n",
    "        elif img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        w,h = img.size\n",
    "        ar = w / h if h else 0.0\n",
    "        pix = float(w*h)\n",
    "        arr = np.array(img)\n",
    "        mean_r = float(arr[:,:,0].mean()); mean_g = float(arr[:,:,1].mean()); mean_b = float(arr[:,:,2].mean())\n",
    "        std_r = float(arr[:,:,0].std());  std_g  = float(arr[:,:,1].std());  std_b  = float(arr[:,:,2].std())\n",
    "        brightness = float((mean_r+mean_g+mean_b)/3.0)\n",
    "        is_mostly_white = float(brightness > 200)\n",
    "        return {\n",
    "            'width': float(w), 'height': float(h), 'aspect_ratio': float(ar), 'pixel_count': pix,\n",
    "            'mean_r': mean_r, 'mean_g': mean_g, 'mean_b': mean_b,\n",
    "            'std_r': std_r, 'std_g': std_g, 'std_b': std_b,\n",
    "            'brightness': brightness, 'is_mostly_white': is_mostly_white,\n",
    "            'original_mode': original_mode\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print('Feature extraction error:', image_path, e)\n",
    "        return {\n",
    "            'width':224.0,'height':224.0,'aspect_ratio':1.0,'pixel_count':50176.0,\n",
    "            'mean_r':128.0,'mean_g':128.0,'mean_b':128.0,\n",
    "            'std_r':50.0,'std_g':50.0,'std_b':50.0,\n",
    "            'brightness':128.0,'is_mostly_white':0.0,'original_mode':3.0\n",
    "        }\n",
    "\n",
    "STATISTICAL_FEATURE_COLS = [\n",
    "    'width','height','aspect_ratio','pixel_count',\n",
    "    'mean_r','mean_g','mean_b','std_r','std_g','std_b',\n",
    "    'brightness','is_mostly_white','original_mode'\n",
    "]\n",
    "print('Num statistical features:', len(STATISTICAL_FEATURE_COLS))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DINOv2-Large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of Dinov2ForImageClassification were not initialized from the model checkpoint at facebook/dinov2-small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: facebook/dinov2-small\n"
     ]
    }
   ],
   "source": [
    "processor = AutoImageProcessor.from_pretrained(MODEL_ID)\n",
    "base_model = AutoModelForImageClassification.from_pretrained(MODEL_ID).to(device)\n",
    "print('Loaded:', MODEL_ID)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_rgb(path):\n",
    "    img = Image.open(path)\n",
    "    if img.mode == 'P':\n",
    "        img = img.convert('RGBA')\n",
    "    if img.mode == 'RGBA':\n",
    "        bg = Image.new('RGB', img.size, (255,255,255))\n",
    "        bg.paste(img, mask=img.split()[3])\n",
    "        img = bg\n",
    "    elif img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, processor):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.labels = list(labels)\n",
    "        self.processor = processor\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_paths[idx]\n",
    "        y = int(self.labels[idx])\n",
    "        img = load_image_rgb(p)\n",
    "        inputs = self.processor(img, return_tensors='pt')\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        y = int(max(0, min(y, len(VENDOR_CLASSES)-1)))\n",
    "        return {'pixel_values': pixel_values, 'labels': torch.tensor(y, dtype=torch.long)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Head (DINOv2 features → 7 classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification model ready.\n"
     ]
    }
   ],
   "source": [
    "class DINOv2ForEmojiClassification(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_labels = num_labels\n",
    "        hidden = getattr(base_model.config, 'hidden_size', 1024)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden, hidden//2),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden//2),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden//2, num_labels)\n",
    "        )\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        out = self.base_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "        if hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
    "            pooled = out.pooler_output\n",
    "        else:\n",
    "            pooled = out.hidden_states[-1][:,0,:]\n",
    "        logits = self.classifier(pooled)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "            loss = nn.CrossEntropyLoss(label_smoothing=0.1)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "classification_model = DINOv2ForEmojiClassification(num_labels=len(VENDOR_CLASSES)).to(device)\n",
    "print('Classification model ready.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Validate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss=0.0; correct=0; total=0\n",
    "    use_amp = (device.type=='cuda')\n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        y = batch['labels'].to(device, non_blocking=True)\n",
    "        y = torch.clamp(y, 0, model.num_labels-1)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(pixel_values=x, labels=y)\n",
    "            loss = out.loss\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += float(loss.item())\n",
    "        pred = torch.argmax(out.logits, dim=1)\n",
    "        correct += int((pred==y).sum().item())\n",
    "        total += int(y.size(0))\n",
    "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss=0.0; correct=0; total=0\n",
    "    preds=[]; labels=[]\n",
    "    use_amp = (device.type=='cuda')\n",
    "    for batch in tqdm(loader, desc='Validation'):\n",
    "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        y = batch['labels'].to(device, non_blocking=True)\n",
    "        y = torch.clamp(y, 0, model.num_labels-1)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(pixel_values=x, labels=y)\n",
    "            loss = out.loss\n",
    "        total_loss += float(loss.item())\n",
    "        pred = torch.argmax(out.logits, dim=1)\n",
    "        pred = torch.clamp(pred, 0, model.num_labels-1)\n",
    "        correct += int((pred==y).sum().item())\n",
    "        total += int(y.size(0))\n",
    "        preds.extend(pred.cpu().numpy().tolist())\n",
    "        labels.extend(y.cpu().numpy().tolist())\n",
    "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total), preds, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA predictions → separate columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_with_tta_separate(model, image, processor, tta_aug, num_augmentations, device):\n",
    "    model.eval()\n",
    "    augs = tta_aug.get_augmentations(image, num_augmentations=num_augmentations)\n",
    "    preds = []\n",
    "    for aug in augs:\n",
    "        inp = processor(aug, return_tensors='pt')\n",
    "        x = inp['pixel_values'].to(device)\n",
    "        out = model(pixel_values=x)\n",
    "        pred = int(torch.argmax(out.logits, dim=-1).item())\n",
    "        preds.append(max(0, min(pred, len(VENDOR_CLASSES)-1)))\n",
    "    return preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature matrix for XGBoost (stats + dino_pred_0..N-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost feature dims: 23\n"
     ]
    }
   ],
   "source": [
    "def generate_features_for_xgboost(image_paths, model, processor, tta_aug, device, num_augmentations, loadin_bar = True):\n",
    "    if loadin_bar:\n",
    "        image_paths = tqdm(image_paths, desc='Extracting features')\n",
    "    rows = []\n",
    "    for p in image_paths:\n",
    "        stats = extract_image_properties(p)\n",
    "        img = load_image_rgb(p)\n",
    "        dino_preds = predict_with_tta_separate(model, img, processor, tta_aug, num_augmentations, device)\n",
    "        row = dict(stats)\n",
    "        for i,dp in enumerate(dino_preds):\n",
    "            row[f'dino_pred_{i}'] = float(dp)\n",
    "        rows.append(row)\n",
    "    df = pd.DataFrame(rows)\n",
    "    dino_cols = [f'dino_pred_{i}' for i in range(num_augmentations)]\n",
    "    all_cols = STATISTICAL_FEATURE_COLS + dino_cols\n",
    "    for c in all_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0.0\n",
    "    return df[all_cols]\n",
    "\n",
    "print('XGBoost feature dims:', len(STATISTICAL_FEATURE_COLS) + NUM_TTA_AUGS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 2nd dataset (CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 9879 images\n",
      "Unmapped labels skipped: 0 Missing files skipped: 0\n",
      "Label distribution: [1924 1877 1644 1667 1790  397  580]\n"
     ]
    }
   ],
   "source": [
    "def prepare_dataset_from_csv(train_dir, csv_path):\n",
    "    train_dir = Path(train_dir); csv_path = Path(csv_path)\n",
    "    if not train_dir.exists() or not csv_path.exists():\n",
    "        raise FileNotFoundError(f'Missing train_dir or csv: {train_dir} / {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    label_map = {v: VENDOR_TO_IDX[v] for v in VENDOR_CLASSES}\n",
    "    img_paths=[]; labels=[]\n",
    "    missing=0; unmapped=0\n",
    "    for _, r in df.iterrows():\n",
    "        img_id = str(r['Id']).zfill(5)\n",
    "        lab = str(r['Label']).lower()\n",
    "        if lab not in label_map:\n",
    "            unmapped += 1\n",
    "            continue\n",
    "        found = None\n",
    "        for ext in ('.png','.jpg','.jpeg','.PNG','.JPG','.JPEG'):\n",
    "            p = train_dir / f'{img_id}{ext}'\n",
    "            if p.exists():\n",
    "                found = str(p)\n",
    "                break\n",
    "        if found is None:\n",
    "            missing += 1\n",
    "            continue\n",
    "        img_paths.append(found)\n",
    "        labels.append(int(label_map[lab]))\n",
    "    print('Loaded:', len(img_paths), 'images')\n",
    "    print('Unmapped labels skipped:', unmapped, 'Missing files skipped:', missing)\n",
    "    if labels:\n",
    "        print('Label distribution:', np.bincount(np.array(labels), minlength=len(VENDOR_CLASSES)))\n",
    "    return img_paths, labels\n",
    "\n",
    "second_paths, second_labels = prepare_dataset_from_csv(SECOND_DATASET_TRAIN_DIR, SECOND_DATASET_CSV_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split (stratified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min class count: 397 Stratify: True\n",
      "Train: 8891 Val: 988\n",
      "Train dist: [1732 1689 1480 1500 1611  357  522]\n",
      "Val   dist: [192 188 164 167 179  40  58]\n"
     ]
    }
   ],
   "source": [
    "min_count = np.bincount(np.array(second_labels), minlength=len(VENDOR_CLASSES)).min()\n",
    "can_stratify = (min_count >= 2)\n",
    "print('Min class count:', int(min_count), 'Stratify:', can_stratify)\n",
    "train_paths, val_paths, train_y, val_y = train_test_split(\n",
    "    second_paths, second_labels, test_size=VAL_SIZE, random_state=RANDOM_STATE,\n",
    "    stratify=second_labels if can_stratify else None\n",
    ")\n",
    "print('Train:', len(train_paths), 'Val:', len(val_paths))\n",
    "print('Train dist:', np.bincount(np.array(train_y), minlength=len(VENDOR_CLASSES)))\n",
    "print('Val   dist:', np.bincount(np.array(val_y), minlength=len(VENDOR_CLASSES)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DINOv2 (2nd dataset only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1112/1112 [01:03<00:00, 17.43it/s]\n",
      "Validation: 100%|██████████| 124/124 [00:02<00:00, 52.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.8197 acc=83.59%\n",
      "Val  : loss=0.6135 acc=93.83%\n",
      "✓ saved best_dino_v8.pt\n",
      "\n",
      "Epoch 2/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1112/1112 [01:02<00:00, 17.81it/s]\n",
      "Validation: 100%|██████████| 124/124 [00:02<00:00, 52.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.6059 acc=93.92%\n",
      "Val  : loss=0.5728 acc=95.75%\n",
      "✓ saved best_dino_v8.pt\n",
      "\n",
      "Epoch 3/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1112/1112 [01:02<00:00, 17.85it/s]\n",
      "Validation: 100%|██████████| 124/124 [00:02<00:00, 51.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.5746 acc=95.02%\n",
      "Val  : loss=0.6139 acc=92.51%\n",
      "\n",
      "Epoch 4/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1112/1112 [01:03<00:00, 17.61it/s]\n",
      "Validation: 100%|██████████| 124/124 [00:02<00:00, 50.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.5535 acc=95.76%\n",
      "Val  : loss=0.5685 acc=95.34%\n",
      "\n",
      "Epoch 5/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1112/1112 [01:02<00:00, 17.77it/s]\n",
      "Validation: 100%|██████████| 124/124 [00:02<00:00, 52.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.5377 acc=96.37%\n",
      "Val  : loss=0.5687 acc=95.34%\n",
      "\n",
      "Epoch 6/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1112/1112 [01:02<00:00, 17.93it/s]\n",
      "Validation: 100%|██████████| 124/124 [00:02<00:00, 51.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4888 acc=98.61%\n",
      "Val  : loss=0.5332 acc=97.17%\n",
      "✓ saved best_dino_v8.pt\n",
      "\n",
      "Epoch 7/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1112/1112 [01:03<00:00, 17.60it/s]\n",
      "Validation: 100%|██████████| 124/124 [00:02<00:00, 51.95it/s]\n",
      "/tmp/ipykernel_6361/1244633474.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  classification_model.load_state_dict(torch.load('best_dino_v8.pt', map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: loss=0.4766 acc=99.17%\n",
      "Val  : loss=0.5596 acc=95.85%\n",
      "Best val acc: 97.16599190283401\n",
      "Loaded best_dino_v8.pt\n",
      "Validation report (DINO only, no XGB):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       0.91      0.98      0.95       192\n",
      "      google       0.95      0.95      0.95       188\n",
      "    whatsapp       0.97      0.95      0.96       164\n",
      "    facebook       0.99      0.94      0.97       167\n",
      "     samsung       0.97      0.96      0.97       179\n",
      "     mozilla       0.95      0.93      0.94        40\n",
      "   messenger       0.98      0.98      0.98        58\n",
      "\n",
      "    accuracy                           0.96       988\n",
      "   macro avg       0.96      0.96      0.96       988\n",
      "weighted avg       0.96      0.96      0.96       988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_ds = EmojiDataset(train_paths, train_y, processor)\n",
    "val_ds = EmojiDataset(val_paths, val_y, processor)\n",
    "batch_size = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "optimizer = torch.optim.AdamW(classification_model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, min_lr=1e-7, cooldown=1)\n",
    "\n",
    "scaler = None\n",
    "if torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()):\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "best_acc = 0.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    tr_loss, tr_acc = train_epoch(classification_model, train_loader, optimizer, device, scaler)\n",
    "    va_loss, va_acc, va_pred, va_true = validate(classification_model, val_loader, device)\n",
    "    scheduler.step(va_acc)\n",
    "    print(f'Train: loss={tr_loss:.4f} acc={tr_acc:.2f}%')\n",
    "    print(f'Val  : loss={va_loss:.4f} acc={va_acc:.2f}%')\n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        torch.save(classification_model.state_dict(), 'best_dino_v8.pt')\n",
    "        print('✓ saved best_dino_v8.pt')\n",
    "\n",
    "print('Best val acc:', best_acc)\n",
    "if os.path.exists('best_dino_v8.pt'):\n",
    "    classification_model.load_state_dict(torch.load('best_dino_v8.pt', map_location=device))\n",
    "    print('Loaded best_dino_v8.pt')\n",
    "\n",
    "print('Validation report (DINO only, no XGB):')\n",
    "print(classification_report(va_true, va_pred, target_names=VENDOR_CLASSES))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train XGBoost on top (TTA-columns + stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 8891/8891 [17:06<00:00,  8.66it/s]\n",
      "Extracting features: 100%|██████████| 988/988 [01:53<00:00,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (8891, 23) X_val: (988, 23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = generate_features_for_xgboost(train_paths, classification_model, processor, tta_aug, device, NUM_TTA_AUGS)\n",
    "X_val = generate_features_for_xgboost(val_paths, classification_model, processor, tta_aug, device, NUM_TTA_AUGS)\n",
    "y_train = np.array(train_y)\n",
    "y_val = np.array(val_y)\n",
    "print('X_train:', X_train.shape, 'X_val:', X_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:199: UserWarning: [20:47:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved xgb_v8.json\n",
      "XGB val acc: 97.06477732793523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       apple       0.93      0.97      0.95       192\n",
      "      google       0.96      0.97      0.97       188\n",
      "    whatsapp       0.98      0.97      0.98       164\n",
      "    facebook       0.98      0.97      0.98       167\n",
      "     samsung       0.99      0.97      0.98       179\n",
      "     mozilla       1.00      0.93      0.96        40\n",
      "   messenger       1.00      0.98      0.99        58\n",
      "\n",
      "    accuracy                           0.97       988\n",
      "   macro avg       0.98      0.97      0.97       988\n",
      "weighted avg       0.97      0.97      0.97       988\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_model = xgb.XGBClassifier(**XGB_PARAMS)\n",
    "\n",
    "# Train (no per-iteration printing)\n",
    "xgb_model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Save (workaround for your xgboost sklearn wrapper bug)\n",
    "xgb_model.get_booster().save_model('xgb_v8.json')\n",
    "print('Saved xgb_v8.json')\n",
    "\n",
    "# Evaluate\n",
    "val_pred = xgb_model.predict(X_val)\n",
    "print('XGB val acc:', accuracy_score(y_val, val_pred) * 100.0)\n",
    "print(classification_report(y_val, val_pred, target_names=VENDOR_CLASSES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final fit on Train+Val, then predict Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1235/1235 [01:08<00:00, 18.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train: loss=0.4877 acc=98.75%\n",
      "\n",
      "Final epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1235/1235 [01:09<00:00, 17.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final train: loss=0.4847 acc=98.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ... existing code above ...\n",
    "\n",
    "combined_paths = train_paths + val_paths\n",
    "combined_y = train_y + val_y\n",
    "\n",
    "combined_ds = EmojiDataset(combined_paths, combined_y, processor)\n",
    "combined_loader = DataLoader(\n",
    "    combined_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "final_epochs = 2\n",
    "final_lr = LEARNING_RATE / 2\n",
    "final_opt = torch.optim.AdamW(classification_model.parameters(), lr=final_lr, weight_decay=0.01)\n",
    "\n",
    "for ep in range(final_epochs):\n",
    "    print(f\"\\nFinal epoch {ep+1}/{final_epochs}\")\n",
    "    tr_loss, tr_acc = train_epoch(classification_model, combined_loader, final_opt, device, scaler)\n",
    "    print(f\"Final train: loss={tr_loss:.4f} acc={tr_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best_dino_v8_final.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting features: 100%|██████████| 9879/9879 [18:56<00:00,  8.69it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:199: UserWarning: [21:09:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved xgb_v8_final.json\n",
      "Found test images: 9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting test: 100%|██████████| 9879/9879 [19:43<00:00,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: predictions.csv rows: 9879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.save(classification_model.state_dict(), \"best_dino_v8_final.pt\")\n",
    "print(\"Saved best_dino_v8_final.pt\")\n",
    "\n",
    "X_all = generate_features_for_xgboost(\n",
    "    combined_paths, classification_model, processor, tta_aug, device, NUM_TTA_AUGS\n",
    ")\n",
    "y_all = np.array(combined_y)\n",
    "\n",
    "xgb_final = xgb.XGBClassifier(**XGB_PARAMS)\n",
    "xgb_final.fit(X_all, y_all, verbose=False)\n",
    "\n",
    "# ✅ FIX: save via Booster to avoid `_estimator_type` crash in your xgboost build\n",
    "xgb_final.get_booster().save_model(\"xgb_v8_final.json\")\n",
    "print(\"Saved xgb_v8_final.json\")\n",
    "\n",
    "test_dir = SECOND_DATASET_TEST_DIR\n",
    "if not test_dir.exists():\n",
    "    raise FileNotFoundError(f\"Missing test dir: {test_dir}\")\n",
    "\n",
    "test_paths = []\n",
    "for ext in (\".png\", \".jpg\", \".jpeg\", \".PNG\", \".JPG\", \".JPEG\"):\n",
    "    test_paths += [str(p) for p in test_dir.rglob(f\"*{ext}\")]\n",
    "test_paths = sorted(set(test_paths))\n",
    "print(\"Found test images:\", len(test_paths))\n",
    "\n",
    "pred_labels = []\n",
    "pred_ids = []\n",
    "for p in tqdm(test_paths, desc=\"Predicting test\"):\n",
    "    img_id = Path(p).stem\n",
    "    Xp = generate_features_for_xgboost([p], classification_model, processor, tta_aug, device, NUM_TTA_AUGS, False)\n",
    "    pred = int(xgb_final.predict(Xp)[0])\n",
    "    pred = max(0, min(pred, len(VENDOR_CLASSES) - 1))\n",
    "    pred_ids.append(img_id)\n",
    "    pred_labels.append(IDX_TO_VENDOR[pred])\n",
    "\n",
    "out_path = Path(PREDICTIONS_OUTPUT_FILE)\n",
    "with out_path.open(\"w\") as f:\n",
    "    f.write(\"Id,Label\\n\")\n",
    "    for i, l in zip(pred_ids, pred_labels):\n",
    "        f.write(f\"{str(i).strip()},{l}\\n\")\n",
    "\n",
    "print(\"Wrote:\", out_path, \"rows:\", len(pred_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
