{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V11 — Boosting-style 2-stage DINOv2 + 2-stage ConvNeXtV2 + Meta-Model\n",
    "\n",
    "**Goal:** improve accuracy by training successive models on the errors of the previous stage, then stacking everything in LightGBM.\n",
    "\n",
    "**Pipeline:**\n",
    "- Split train/val (stratified if possible)\n",
    "- Train **DINO stage 1** (seed=42)\n",
    "- Build a **hard-example train set** from stage-1 mistakes, then train **DINO stage 2** (seed=42)\n",
    "- Train **ConvNeXt stage 1** (seed=42)\n",
    "- Build a hard-example train set from stage-1 mistakes, then train **ConvNeXt stage 2** (seed=42)\n",
    "- For each image: compute deterministic TTA prob-vectors for **all 4 models** + statistical features\n",
    "- Train **LightGBM** meta-model on top\n",
    "\n",
    "**Classes:** apple, google, whatsapp, facebook, samsung, mozilla, messenger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGB = True\n",
    "except Exception as e:\n",
    "    HAS_LGB = False\n",
    "    print('LightGBM import failed:', e)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU:', torch.cuda.get_device_name(0))\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HYPERPARAMETERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "SECOND_DATASET_BASE_PATH = '.'\n",
    "SECOND_DATASET_TRAIN_DIR = Path(SECOND_DATASET_BASE_PATH) / 'train'\n",
    "SECOND_DATASET_CSV_PATH = Path(SECOND_DATASET_BASE_PATH) / 'train_labels.csv'\n",
    "SECOND_DATASET_TEST_DIR = Path(SECOND_DATASET_BASE_PATH) / 'test'\n",
    "\n",
    "# Models\n",
    "DINO_MODEL_ID = 'facebook/dinov2-base'\n",
    "CNN_MODEL_ID  = 'facebook/convnextv2-base-22k-224'\n",
    "SEED = 42\n",
    "\n",
    "# Train\n",
    "VAL_SIZE = 0.10\n",
    "RANDOM_STATE = 42\n",
    "NUM_EPOCHS = 20\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE_CUDA = 16\n",
    "BATCH_SIZE_CPU = 4\n",
    "NUM_WORKERS = 2\n",
    "LABEL_SMOOTHING = 0.05\n",
    "\n",
    "# Hard-example stage-2 training\n",
    "HARD_EASY_RATIO = 1.0  # include this many easy samples per hard sample\n",
    "HARD_MIN_SAMPLES = 200  # if fewer hard samples than this, just use full train\n",
    "REGENERATE_VAL_FOR_STAGE2 = False  # if True, rebuild (train,val) for stage2\n",
    "\n",
    "# TTA\n",
    "NUM_TTA_AUGS = 10\n",
    "TQDM_MININTERVAL = 10\n",
    "SHOW_PROGRESS = True  # set False to reduce output\n",
    "FEATURE_BATCH_SIZE = 16  # images per batch for feature extraction\n",
    "\n",
    "# Meta-model\n",
    "USE_LIGHTGBM = True\n",
    "LGB_PARAMS = {\n",
    "    'n_estimators': 1200,\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 63,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "\n",
    "# Output\n",
    "PREDICTIONS_OUTPUT_FILE = 'predictions_V11.csv'\n",
    "\n",
    "print('DINO_MODEL_ID:', DINO_MODEL_ID)\n",
    "print('CNN_MODEL_ID :', CNN_MODEL_ID)\n",
    "print('SEED:', SEED)\n",
    "print('NUM_EPOCHS:', NUM_EPOCHS, 'patience:', EARLY_STOPPING_PATIENCE)\n",
    "print('LABEL_SMOOTHING:', LABEL_SMOOTHING)\n",
    "print('NUM_TTA_AUGS:', NUM_TTA_AUGS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENDOR_CLASSES = ['apple','google','whatsapp','facebook','samsung','mozilla','messenger']\n",
    "VENDOR_TO_IDX = {v:i for i,v in enumerate(VENDOR_CLASSES)}\n",
    "IDX_TO_VENDOR = {i:v for v,i in VENDOR_TO_IDX.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Augmentation (Predictable TTA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical features (incl. original_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_properties(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        mode_mapping = {'L':0,'LA':1,'P':2,'RGB':3,'RGBA':4}\n",
    "        original_mode = float(mode_mapping.get(img.mode, 3))\n",
    "        # Normalize image to RGB for pixel stats\n",
    "        if img.mode == 'P':\n",
    "            img = img.convert('RGBA')\n",
    "        if img.mode == 'RGBA':\n",
    "            bg = Image.new('RGB', img.size, (255,255,255))\n",
    "            bg.paste(img, mask=img.split()[3])\n",
    "            img = bg\n",
    "        elif img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        w,h = img.size\n",
    "        ar = w / h if h else 0.0\n",
    "        pix = float(w*h)\n",
    "        arr = np.array(img)\n",
    "        mean_r = float(arr[:,:,0].mean()); mean_g = float(arr[:,:,1].mean()); mean_b = float(arr[:,:,2].mean())\n",
    "        std_r = float(arr[:,:,0].std());  std_g  = float(arr[:,:,1].std());  std_b  = float(arr[:,:,2].std())\n",
    "        brightness = float((mean_r+mean_g+mean_b)/3.0)\n",
    "        is_mostly_white = float(brightness > 200)\n",
    "        return {\n",
    "            'width': float(w), 'height': float(h), 'aspect_ratio': float(ar), 'pixel_count': pix,\n",
    "            'mean_r': mean_r, 'mean_g': mean_g, 'mean_b': mean_b,\n",
    "            'std_r': std_r, 'std_g': std_g, 'std_b': std_b,\n",
    "            'brightness': brightness, 'is_mostly_white': is_mostly_white,\n",
    "            'original_mode': original_mode\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'width':224.0,'height':224.0,'aspect_ratio':1.0,'pixel_count':50176.0,\n",
    "            'mean_r':128.0,'mean_g':128.0,'mean_b':128.0,\n",
    "            'std_r':50.0,'std_g':50.0,'std_b':50.0,\n",
    "            'brightness':128.0,'is_mostly_white':0.0,'original_mode':3.0\n",
    "        }\n",
    "\n",
    "STAT_COLS = ['width','height','aspect_ratio','pixel_count','mean_r','mean_g','mean_b','std_r','std_g','std_b','brightness','is_mostly_white','original_mode']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicAugmentation:\n",
    "    def __init__(self, image_size=224, seed=42):\n",
    "        self.image_size = image_size\n",
    "        self.seed = seed\n",
    "        self.rotation_angles = [-10, -5, 5, 10]\n",
    "        self.crop_ratios = [0.75, 0.85, 0.9, 0.95]\n",
    "        self.color_jitter_params = {'brightness':0.3,'contrast':0.3,'saturation':0.3,'hue':0.1}\n",
    "        self.translate_range = (0.1, 0.1)\n",
    "        self.blur_sigma = (0.1, 0.5)\n",
    "\n",
    "    def _get_deterministic_seed(self, image_or_hash):\n",
    "        if isinstance(image_or_hash, Image.Image):\n",
    "            img_bytes = image_or_hash.tobytes()\n",
    "            return int(hashlib.md5(img_bytes).hexdigest()[:8], 16)\n",
    "        return hash(str(image_or_hash)) & 0xFFFFFFFF\n",
    "\n",
    "    def horizontal_flip(self, image):\n",
    "        return F.hflip(image)\n",
    "\n",
    "    def rotation(self, image, angle):\n",
    "        return F.rotate(image, angle)\n",
    "\n",
    "    def center_crop(self, image, crop_ratio=0.9):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        return F.center_crop(image, [crop,crop])\n",
    "\n",
    "    def corner_crop(self, image, crop_ratio=0.9, position='tl'):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        if position=='tl': return F.crop(image, 0, 0, crop, crop)\n",
    "        if position=='tr': return F.crop(image, 0, w-crop, crop, crop)\n",
    "        if position=='bl': return F.crop(image, h-crop, 0, crop, crop)\n",
    "        if position=='br': return F.crop(image, h-crop, w-crop, crop, crop)\n",
    "        return image\n",
    "\n",
    "    def resized_crop(self, image, crop_ratio=0.85):\n",
    "        w,h = image.size\n",
    "        crop = int(min(w,h)*crop_ratio)\n",
    "        cropped = F.center_crop(image, [crop,crop])\n",
    "        return cropped.resize((self.image_size,self.image_size), Image.BILINEAR)\n",
    "\n",
    "    def color_jitter(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        b = 1.0 + np.random.uniform(-self.color_jitter_params['brightness'], self.color_jitter_params['brightness'])\n",
    "        c = 1.0 + np.random.uniform(-self.color_jitter_params['contrast'], self.color_jitter_params['contrast'])\n",
    "        s = 1.0 + np.random.uniform(-self.color_jitter_params['saturation'], self.color_jitter_params['saturation'])\n",
    "        h = np.random.uniform(-self.color_jitter_params['hue'], self.color_jitter_params['hue'])\n",
    "        img = F.adjust_brightness(image, b)\n",
    "        img = F.adjust_contrast(img, c)\n",
    "        img = F.adjust_saturation(img, s)\n",
    "        img = F.adjust_hue(img, h)\n",
    "        return img\n",
    "\n",
    "    def affine_transform(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        tx = np.random.uniform(-self.translate_range[0], self.translate_range[0])\n",
    "        ty = np.random.uniform(-self.translate_range[1], self.translate_range[1])\n",
    "        return F.affine(image, angle=0, translate=(tx*image.width, ty*image.height), scale=1.0, shear=0.0)\n",
    "\n",
    "    def gaussian_blur(self, image, seed_val):\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        sigma = np.random.uniform(self.blur_sigma[0], self.blur_sigma[1])\n",
    "        return F.gaussian_blur(image, kernel_size=3, sigma=[sigma,sigma])\n",
    "\n",
    "    def get_augmentations(self, image, num_augmentations=10, seed_source=None):\n",
    "        if seed_source is None:\n",
    "            seed_val = self._get_deterministic_seed(image)\n",
    "        elif isinstance(seed_source, str):\n",
    "            # Convert string (e.g., image path) to integer seed\n",
    "            seed_val = self._get_deterministic_seed(seed_source)\n",
    "        else:\n",
    "            # Already an integer\n",
    "            seed_val = int(seed_source)\n",
    "        augs = []\n",
    "        augs.append(image.resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        augs.append(self.horizontal_flip(image).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        for angle in self.rotation_angles[:max(0, min(4, num_augmentations-len(augs)))]:\n",
    "            augs.append(self.rotation(image, angle).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        corners = ['tl','tr','bl','br']\n",
    "        for cpos in corners[:max(0, min(4, num_augmentations-len(augs)))]:\n",
    "            augs.append(self.corner_crop(image, 0.9, cpos).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.center_crop(image, 0.9).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.resized_crop(image, 0.85))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.color_jitter(image, seed_val).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.affine_transform(image, seed_val+1).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        if len(augs) < num_augmentations:\n",
    "            augs.append(self.gaussian_blur(image, seed_val+2).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
    "        return augs[:num_augmentations]\n",
    "    \n",
    "    def apply_training_augmentation(self, image, seed_source=None):\n",
    "        \"\"\"\n",
    "        Apply deterministic training augmentation (same methods as TTA).\n",
    "        Deterministic based on image path/index for reproducibility.\n",
    "        \"\"\"\n",
    "        if seed_source is None:\n",
    "            seed_val = self._get_deterministic_seed(image)\n",
    "        elif isinstance(seed_source, str):\n",
    "            # Convert string (e.g., image path) to integer seed\n",
    "            seed_val = self._get_deterministic_seed(seed_source)\n",
    "        else:\n",
    "            # Already an integer\n",
    "            seed_val = int(seed_source)\n",
    "        \n",
    "        # Use seed to deterministically select augmentations\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        \n",
    "        # Horizontal flip (50% probability, deterministic)\n",
    "        if (seed_val % 2 == 0):\n",
    "            image = self.horizontal_flip(image)\n",
    "        \n",
    "        # Rotation (deterministic angle selection)\n",
    "        angle_idx = (seed_val // 2) % len(self.rotation_angles)\n",
    "        angle = self.rotation_angles[angle_idx]\n",
    "        image = self.rotation(image, angle)\n",
    "        \n",
    "        # Crop (deterministic crop ratio)\n",
    "        crop_idx = (seed_val // 10) % len(self.crop_ratios)\n",
    "        crop_ratio = self.crop_ratios[crop_idx]\n",
    "        w, h = image.size\n",
    "        crop_size = int(min(w, h) * crop_ratio)\n",
    "        image = F.center_crop(image, [crop_size, crop_size])\n",
    "        \n",
    "        # Color jitter (deterministic)\n",
    "        image = self.color_jitter(image, seed_val)\n",
    "        \n",
    "        # Affine transform (50% probability, deterministic)\n",
    "        if (seed_val // 3) % 2 == 0:\n",
    "            image = self.affine_transform(image, seed_val + 1)\n",
    "        \n",
    "        # Gaussian blur (20% probability, deterministic)\n",
    "        if (seed_val // 5) % 5 == 0:\n",
    "            image = self.gaussian_blur(image, seed_val + 2)\n",
    "        \n",
    "        # Resize to final size\n",
    "        image = image.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        \n",
    "        return image\n",
    "\n",
    "tta_aug = DeterministicAugmentation(image_size=224, seed=42)\n",
    "print('Deterministic augmentation ready.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading (CSV labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_from_csv(train_dir, csv_path):\n",
    "    train_dir = Path(train_dir); csv_path = Path(csv_path)\n",
    "    if not train_dir.exists() or not csv_path.exists():\n",
    "        raise FileNotFoundError(f'Missing train_dir or csv: {train_dir} / {csv_path}')\n",
    "    df = pd.read_csv(csv_path)\n",
    "    label_map = {v: VENDOR_TO_IDX[v] for v in VENDOR_CLASSES}\n",
    "    img_paths=[]; labels=[]\n",
    "    missing=0; unmapped=0\n",
    "    for _, r in df.iterrows():\n",
    "        img_id = str(r['Id']).zfill(5)\n",
    "        lab = str(r['Label']).lower()\n",
    "        if lab not in label_map:\n",
    "            unmapped += 1\n",
    "            continue\n",
    "        found = None\n",
    "        for ext in ('.png','.jpg','.jpeg','.PNG','.JPG','.JPEG'):\n",
    "            p = train_dir / f'{img_id}{ext}'\n",
    "            if p.exists():\n",
    "                found = str(p)\n",
    "                break\n",
    "        if found is None:\n",
    "            missing += 1\n",
    "            continue\n",
    "        img_paths.append(found)\n",
    "        labels.append(int(label_map[lab]))\n",
    "    print('Loaded:', len(img_paths), 'images')\n",
    "    print('Unmapped labels skipped:', unmapped, 'Missing files skipped:', missing)\n",
    "    if labels:\n",
    "        print('Label distribution:', np.bincount(np.array(labels), minlength=len(VENDOR_CLASSES)))\n",
    "    return img_paths, labels\n",
    "\n",
    "all_paths, all_labels = prepare_dataset_from_csv(SECOND_DATASET_TRAIN_DIR, SECOND_DATASET_CSV_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split (stratified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = np.bincount(np.array(all_labels), minlength=len(VENDOR_CLASSES)).min()\n",
    "can_stratify = (min_count >= 2)\n",
    "print('Min class count:', int(min_count), 'Stratify:', can_stratify)\n",
    "train_paths, val_paths, train_y, val_y = train_test_split(\n",
    "    all_paths, all_labels, test_size=VAL_SIZE, random_state=RANDOM_STATE,\n",
    "    stratify=all_labels if can_stratify else None\n",
    ")\n",
    "print('Train:', len(train_paths), 'Val:', len(val_paths))\n",
    "print('Train dist:', np.bincount(np.array(train_y), minlength=len(VENDOR_CLASSES)))\n",
    "print('Val   dist:', np.bincount(np.array(val_y), minlength=len(VENDOR_CLASSES)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_rgb(path):\n",
    "    img = Image.open(path)\n",
    "    if img.mode == 'P':\n",
    "        img = img.convert('RGBA')\n",
    "    if img.mode == 'RGBA':\n",
    "        bg = Image.new('RGB', img.size, (255,255,255))\n",
    "        bg.paste(img, mask=img.split()[3])\n",
    "        img = bg\n",
    "    elif img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    return img\n",
    "\n",
    "class EmojiDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, processor, use_augmentation=False):\n",
    "        self.image_paths = list(image_paths)\n",
    "        self.labels = list(labels)\n",
    "        self.processor = processor\n",
    "        self.use_augmentation = use_augmentation\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        p = self.image_paths[idx]\n",
    "        y = int(self.labels[idx])\n",
    "        img = load_image_rgb(p)\n",
    "        \n",
    "        # Apply training augmentation if enabled (deterministic based on image path)\n",
    "        if self.use_augmentation:\n",
    "            img = tta_aug.apply_training_augmentation(img, seed_source=str(p))\n",
    "        \n",
    "        inputs = self.processor(img, return_tensors='pt')\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "        y = int(max(0, min(y, len(VENDOR_CLASSES)-1)))\n",
    "        return {'pixel_values': pixel_values, 'labels': torch.tensor(y, dtype=torch.long)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model wrapper (DINOv2 backbone → 7 classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DINOv2ForEmojiClassification(nn.Module):\n",
    "#     def __init__(self, base_model, num_labels):\n",
    "#         super().__init__()\n",
    "#         self.base_model = base_model\n",
    "#         self.num_labels = num_labels\n",
    "#         hidden = getattr(base_model.config, 'hidden_size', 1024)\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.LayerNorm(hidden),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(hidden, hidden//2),\n",
    "#             nn.GELU(),\n",
    "#             nn.LayerNorm(hidden//2),\n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(hidden//2, num_labels)\n",
    "#         )\n",
    "#     def forward(self, pixel_values, labels=None):\n",
    "#         out = self.base_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "#         if hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
    "#             pooled = out.pooler_output\n",
    "#         else:\n",
    "#             pooled = out.hidden_states[-1][:,0,:]\n",
    "#         logits = self.classifier(pooled)\n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "#             loss = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "#         return ImageClassifierOutput(loss=loss, logits=logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtV2ForEmojiClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_labels = num_labels\n",
    "        # ConvNeXtV2 hidden size is in config.hidden_sizes\n",
    "        hidden = getattr(getattr(base_model, 'config', None), 'hidden_sizes', [1024])[-1]\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        # ConvNeXtV2 backbone feature map\n",
    "        out = self.base_model.convnextv2(pixel_values)\n",
    "        feats = out.last_hidden_state\n",
    "        if len(feats.shape) == 4:\n",
    "            pooled = feats.mean(dim=[2,3])\n",
    "        else:\n",
    "            pooled = feats\n",
    "        logits = self.classifier(pooled)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "            loss = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv2ForEmojiClassification(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_labels = num_labels\n",
    "        hidden = getattr(base_model.config, 'hidden_size', 1024)\n",
    "        \n",
    "        # Simpler head: less capacity, less overfitting risk\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Dropout(0.1),  # Lower dropout\n",
    "            nn.Linear(hidden, num_labels)  # Direct projection\n",
    "        )\n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        out = self.base_model(pixel_values=pixel_values, output_hidden_states=True)\n",
    "        if hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
    "            pooled = out.pooler_output\n",
    "        else:\n",
    "            pooled = out.hidden_states[-1][:,0,:]\n",
    "        logits = self.classifier(pooled)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = torch.clamp(labels, 0, self.num_labels-1)\n",
    "            loss = nn.CrossEntropyLoss(label_smoothing=0.1)(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        return ImageClassifierOutput(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Validate loops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    import random\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def make_loaders(processor):\n",
    "    train_ds = EmojiDataset(train_paths, train_y, processor, use_augmentation=True)  # Enable augmentation for training\n",
    "    val_ds = EmojiDataset(val_paths, val_y, processor, use_augmentation=False)  # No augmentation for validation\n",
    "    batch_size = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_epoch(model, loader, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss=0.0; correct=0; total=0\n",
    "    use_amp = (device.type=='cuda')\n",
    "    for batch in tqdm(loader, desc='Training', mininterval=TQDM_MININTERVAL):\n",
    "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        y = batch['labels'].to(device, non_blocking=True)\n",
    "        y = torch.clamp(y, 0, model.num_labels-1)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(pixel_values=x, labels=y)\n",
    "            loss = out.loss\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += float(loss.item())\n",
    "        pred = torch.argmax(out.logits, dim=1)\n",
    "        correct += int((pred==y).sum().item())\n",
    "        total += int(y.size(0))\n",
    "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss=0.0; correct=0; total=0\n",
    "    preds=[]; labels=[]\n",
    "    use_amp = (device.type=='cuda')\n",
    "    for batch in tqdm(loader, desc='Validation', mininterval=TQDM_MININTERVAL):\n",
    "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        y = batch['labels'].to(device, non_blocking=True)\n",
    "        y = torch.clamp(y, 0, model.num_labels-1)\n",
    "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
    "            out = model(pixel_values=x, labels=y)\n",
    "            loss = out.loss\n",
    "        total_loss += float(loss.item())\n",
    "        pred = torch.argmax(out.logits, dim=1)\n",
    "        pred = torch.clamp(pred, 0, model.num_labels-1)\n",
    "        correct += int((pred==y).sum().item())\n",
    "        total += int(y.size(0))\n",
    "        preds.extend(pred.cpu().numpy().tolist())\n",
    "        labels.extend(y.cpu().numpy().tolist())\n",
    "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total), preds, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ensemble members\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maybe_stratified_split(paths, labels, val_size, seed):\n",
    "    labels_arr = np.array(labels)\n",
    "    min_count = np.bincount(labels_arr, minlength=len(VENDOR_CLASSES)).min() if len(labels_arr) else 0\n",
    "    can_stratify = (min_count >= 2)\n",
    "    return train_test_split(\n",
    "        list(paths), list(labels),\n",
    "        test_size=val_size, random_state=seed,\n",
    "        stratify=list(labels) if can_stratify else None\n",
    "    )\n",
    "\n",
    "def _get_hard_examples(model, processor, paths, labels):\n",
    "    ds = EmojiDataset(paths, labels, processor, use_augmentation=False)\n",
    "    bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
    "    dl = DataLoader(ds, batch_size=bs, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    _, _, preds, trues = validate(model, dl, device)\n",
    "    hard_idx = [i for i,(p,t) in enumerate(zip(preds, trues)) if int(p)!=int(t)]\n",
    "    hard_paths = [paths[i] for i in hard_idx]\n",
    "    hard_labels = [labels[i] for i in hard_idx]\n",
    "    return hard_paths, hard_labels\n",
    "\n",
    "def _build_stage2_trainset(train_paths, train_y, hard_paths, hard_y):\n",
    "    if len(hard_paths) < HARD_MIN_SAMPLES:\n",
    "        print('Hard set too small (', len(hard_paths), '), using full train for stage2')\n",
    "        return list(train_paths), list(train_y)\n",
    "    # sample easy examples to avoid catastrophic forgetting\n",
    "    hard_set = set(hard_paths)\n",
    "    easy = [(p,y) for p,y in zip(train_paths, train_y) if p not in hard_set]\n",
    "    n_easy = int(min(len(easy), max(1, round(len(hard_paths) * HARD_EASY_RATIO))))\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    if n_easy > 0:\n",
    "        idx = rng.choice(len(easy), size=n_easy, replace=False)\n",
    "        easy_s = [easy[i] for i in idx]\n",
    "    else:\n",
    "        easy_s = []\n",
    "    new_paths = list(hard_paths) + [p for p,_ in easy_s]\n",
    "    new_y = list(hard_y) + [y for _,y in easy_s]\n",
    "    print('Stage2 train size:', len(new_paths), '(hard:', len(hard_paths), 'easy:', len(easy_s), ')')\n",
    "    return new_paths, new_y\n",
    "\n",
    "def train_stage(model_kind, model_id, seed, stage_tag, train_paths_s, train_y_s, val_paths_s, val_y_s):\n",
    "    seed_everything(seed)\n",
    "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "    backbone = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
    "    if model_kind == 'dino':\n",
    "        model = DINOv2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
    "    elif model_kind == 'cnn':\n",
    "        model = ConvNeXtV2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
    "    else:\n",
    "        raise ValueError('Unknown model_kind: ' + str(model_kind))\n",
    "\n",
    "    train_ds = EmojiDataset(train_paths_s, train_y_s, processor, use_augmentation=True)\n",
    "    val_ds = EmojiDataset(val_paths_s, val_y_s, processor, use_augmentation=False)\n",
    "    bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
    "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "    val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, min_lr=1e-7, cooldown=1)\n",
    "    scaler = None\n",
    "    if torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()):\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    best_acc = -1.0\n",
    "    best_path = f'best_{stage_tag}.pt'\n",
    "    bad = 0\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f'\\n[{stage_tag}] epoch {epoch+1}/{NUM_EPOCHS}')\n",
    "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, device, scaler)\n",
    "        va_loss, va_acc, va_pred, va_true = validate(model, val_loader, device)\n",
    "        scheduler.step(va_acc)\n",
    "        print(f'Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% | Val: loss={va_loss:.4f} acc={va_acc:.2f}%')\n",
    "        if va_acc > best_acc + 1e-6:\n",
    "            best_acc = va_acc\n",
    "            bad = 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print('✓ saved', best_path)\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= EARLY_STOPPING_PATIENCE:\n",
    "                print('Early stopping: no improvement for', EARLY_STOPPING_PATIENCE, 'epochs')\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    return model, processor, best_path, best_acc\n",
    "\n",
    "print('=== Stage training: DINOv2 (2 stages) then ConvNeXtV2 (2 stages) ===')\n",
    "\n",
    "# Stage 1 splits (already created above): train_paths/val_paths + train_y/val_y\n",
    "dino1, dino1_proc, dino1_ckpt, dino1_best = train_stage('dino', DINO_MODEL_ID, SEED, 'dino_stage1', train_paths, train_y, val_paths, val_y)\n",
    "hard_p, hard_y = _get_hard_examples(dino1, dino1_proc, train_paths, train_y)\n",
    "stage2_train_p, stage2_train_y = _build_stage2_trainset(train_paths, train_y, hard_p, hard_y)\n",
    "if REGENERATE_VAL_FOR_STAGE2:\n",
    "    stage2_train_p, stage2_val_p, stage2_train_y, stage2_val_y = _maybe_stratified_split(stage2_train_p, stage2_train_y, VAL_SIZE, RANDOM_STATE)\n",
    "else:\n",
    "    stage2_val_p, stage2_val_y = val_paths, val_y\n",
    "dino2, dino2_proc, dino2_ckpt, dino2_best = train_stage('dino', DINO_MODEL_ID, SEED, 'dino_stage2', stage2_train_p, stage2_train_y, stage2_val_p, stage2_val_y)\n",
    "\n",
    "cnn1, cnn1_proc, cnn1_ckpt, cnn1_best = train_stage('cnn', CNN_MODEL_ID, SEED, 'cnn_stage1', train_paths, train_y, val_paths, val_y)\n",
    "hard_p2, hard_y2 = _get_hard_examples(cnn1, cnn1_proc, train_paths, train_y)\n",
    "stage2_train_p2, stage2_train_y2 = _build_stage2_trainset(train_paths, train_y, hard_p2, hard_y2)\n",
    "if REGENERATE_VAL_FOR_STAGE2:\n",
    "    stage2_train_p2, stage2_val_p2, stage2_train_y2, stage2_val_y2 = _maybe_stratified_split(stage2_train_p2, stage2_train_y2, VAL_SIZE, RANDOM_STATE)\n",
    "else:\n",
    "    stage2_val_p2, stage2_val_y2 = val_paths, val_y\n",
    "cnn2, cnn2_proc, cnn2_ckpt, cnn2_best = train_stage('cnn', CNN_MODEL_ID, SEED, 'cnn_stage2', stage2_train_p2, stage2_train_y2, stage2_val_p2, stage2_val_y2)\n",
    "\n",
    "# Keep models on CPU when not actively used (prevents VRAM spikes during feature extraction)\n",
    "if device.type == 'cuda':\n",
    "    dino1 = dino1.to('cpu')\n",
    "    dino2 = dino2.to('cpu')\n",
    "    cnn1 = cnn1.to('cpu')\n",
    "    cnn2 = cnn2.to('cpu')\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "trained_members = [\n",
    "    (dino1, dino1_proc, 'dino1'),\n",
    "    (dino2, dino2_proc, 'dino2'),\n",
    "    (cnn1,  cnn1_proc,  'cnn1'),\n",
    "    (cnn2,  cnn2_proc,  'cnn2'),\n",
    "]\n",
    "print('✓ Trained members:', [t for _,_,t in trained_members])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load member function (for inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_member(model_kind, model_id, ckpt_path):\n",
    "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
    "    backbone = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
    "    if model_kind == 'dino':\n",
    "        model = DINOv2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
    "    else:\n",
    "        model = ConvNeXtV2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
    "    model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA probabilities per augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_proba_tta_per_aug(model, processor, image, num_augmentations, device):\n",
    "    \"\"\"Batched TTA prediction - processes all augmentations at once for speed\"\"\"\n",
    "    model.eval()\n",
    "    augs = tta_aug.get_augmentations(image, num_augmentations=num_augmentations)\n",
    "    # Batch process all augmentations at once (much faster!)\n",
    "    inputs = processor(augs, return_tensors='pt')\n",
    "    x = inputs['pixel_values'].to(device)\n",
    "    out = model(pixel_values=x)\n",
    "    probs = torch.softmax(out.logits, dim=-1)  # Shape: (num_augmentations, num_classes)\n",
    "    return probs.detach().cpu().numpy()  # (A, C)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature matrix: stats + (A×C) prob vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prob_cols_for_members(members, num_augmentations):\n",
    "    cols = []\n",
    "    for _, _, tag in members:\n",
    "        for i in range(num_augmentations):\n",
    "            for c in range(len(VENDOR_CLASSES)):\n",
    "                cols.append(f'prob_{tag}_aug{i}_cls{c}')\n",
    "    return cols\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_features_batched(image_paths, members, num_augmentations, batch_size=None):\n",
    "    \"\"\"\n",
    "    Batched feature extraction.\n",
    "    For each image: stats + per-model (A×C) probabilities.\n",
    "    \"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = FEATURE_BATCH_SIZE\n",
    "\n",
    "    all_rows = []\n",
    "    num_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
    "    for start in tqdm(range(0, len(image_paths), batch_size),\n",
    "                      desc='Building features (batched)',\n",
    "                      mininterval=TQDM_MININTERVAL,\n",
    "                      disable=(not SHOW_PROGRESS),\n",
    "                      total=num_batches):\n",
    "        batch_paths = image_paths[start:start+batch_size]\n",
    "        B = len(batch_paths)\n",
    "        batch_stats = [extract_image_properties(p) for p in batch_paths]\n",
    "        batch_imgs = [load_image_rgb(p) for p in batch_paths]\n",
    "\n",
    "        model_probs = {}  # tag -> (B, A, C)\n",
    "        for m, proc, tag in members:\n",
    "            # Move model to the active device just-in-time to reduce VRAM pressure\n",
    "            m.to(device)\n",
    "            batch_augs = []\n",
    "            for img in batch_imgs:\n",
    "                batch_augs.extend(tta_aug.get_augmentations(img, num_augmentations=num_augmentations))\n",
    "            inputs = proc(batch_augs, return_tensors='pt')\n",
    "            x = inputs['pixel_values'].to(device)\n",
    "            m.eval()\n",
    "            out = m(pixel_values=x)\n",
    "            probs = torch.softmax(out.logits, dim=-1)\n",
    "            probs = probs.view(B, num_augmentations, -1).detach().cpu().numpy()\n",
    "            model_probs[tag] = probs\n",
    "            # Move back to CPU after use to keep memory stable across 4 models\n",
    "            if device.type == 'cuda':\n",
    "                m.to('cpu')\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        for i, stats in enumerate(batch_stats):\n",
    "            row = dict(stats)\n",
    "            for tag, probs in model_probs.items():\n",
    "                for a in range(num_augmentations):\n",
    "                    for c in range(len(VENDOR_CLASSES)):\n",
    "                        row[f'prob_{tag}_aug{a}_cls{c}'] = float(probs[i, a, c])\n",
    "            all_rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    prob_cols = _prob_cols_for_members(members, num_augmentations)\n",
    "    all_cols = STAT_COLS + prob_cols\n",
    "    for col in all_cols:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0.0\n",
    "    return df[all_cols]\n",
    "\n",
    "print('Total features:', len(STAT_COLS) + len(_prob_cols_for_members([('','','dino1'),('','','dino2'),('','','cnn1'),('','','cnn2')], NUM_TTA_AUGS)))\n",
    "print('Using batched feature extraction with batch_size:', FEATURE_BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train meta-model (LightGBM preferred, XGBoost fallback)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained_members list from the stage training cell\n",
    "members = trained_members\n",
    "print('Members for features:', [t for _,_,t in members])\n",
    "\n",
    "X_train = build_features_batched(train_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "X_val = build_features_batched(val_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "y_train = np.array(train_y)\n",
    "y_val = np.array(val_y)\n",
    "print('X_train:', X_train.shape, 'X_val:', X_val.shape)\n",
    "\n",
    "if USE_LIGHTGBM and HAS_LGB:\n",
    "    meta = lgb.LGBMClassifier(**LGB_PARAMS)\n",
    "    meta.fit(X_train, y_train)\n",
    "    val_pred = meta.predict(X_val)\n",
    "    print('Meta(LGB) val acc:', accuracy_score(y_val, val_pred)*100.0)\n",
    "    print(classification_report(y_val, val_pred, target_names=VENDOR_CLASSES))\n",
    "    meta.booster_.save_model('meta_lgb_v11.txt')\n",
    "    print('Saved meta_lgb_v11.txt')\n",
    "else:\n",
    "    raise RuntimeError('LightGBM not available in this environment. Please install lightgbm or switch USE_LIGHTGBM=False')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final fit on Train+Val and predict Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members = trained_members\n",
    "combined_paths = train_paths + val_paths\n",
    "combined_y = train_y + val_y\n",
    "\n",
    "X_all = build_features_batched(combined_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
    "y_all = np.array(combined_y)\n",
    "\n",
    "if not (USE_LIGHTGBM and HAS_LGB):\n",
    "    raise RuntimeError('LightGBM not available for final fit')\n",
    "meta_final = lgb.LGBMClassifier(**LGB_PARAMS)\n",
    "meta_final.fit(X_all, y_all)\n",
    "meta_final.booster_.save_model('meta_lgb_v11_final.txt')\n",
    "print('Saved meta_lgb_v11_final.txt')\n",
    "\n",
    "test_dir = SECOND_DATASET_TEST_DIR\n",
    "if not test_dir.exists():\n",
    "    raise FileNotFoundError(f'Missing test dir: {test_dir}')\n",
    "test_paths = []\n",
    "for ext in ('.png','.jpg','.jpeg','.PNG','.JPG','.JPEG'):\n",
    "    test_paths += [str(p) for p in test_dir.rglob(f'*{ext}')]\n",
    "test_paths = sorted(set(test_paths))\n",
    "print('Found test images:', len(test_paths))\n",
    "\n",
    "pred_ids = []\n",
    "pred_labels = []\n",
    "test_batch_size = FEATURE_BATCH_SIZE\n",
    "num_test_batches = (len(test_paths) + test_batch_size - 1) // test_batch_size\n",
    "for start in tqdm(range(0, len(test_paths), test_batch_size), desc='Predicting test (batched)', mininterval=TQDM_MININTERVAL, disable=(not SHOW_PROGRESS), total=num_test_batches):\n",
    "    batch_paths = test_paths[start:start+test_batch_size]\n",
    "    batch_ids = [Path(p).stem for p in batch_paths]\n",
    "    Xp_batch = build_features_batched(batch_paths, members, NUM_TTA_AUGS, batch_size=test_batch_size)\n",
    "    preds = meta_final.predict(Xp_batch)\n",
    "    for img_id, pred in zip(batch_ids, preds):\n",
    "        pred = int(pred)\n",
    "        pred = max(0, min(pred, len(VENDOR_CLASSES)-1))\n",
    "        pred_ids.append(img_id)\n",
    "        pred_labels.append(IDX_TO_VENDOR[pred])\n",
    "\n",
    "out_path = Path(PREDICTIONS_OUTPUT_FILE)\n",
    "with out_path.open('w') as f:\n",
    "    f.write('Id,Label\\n')\n",
    "    for i,l in zip(pred_ids, pred_labels):\n",
    "        f.write(f'{str(i).strip()},{l}\\n')\n",
    "print('Wrote:', out_path, 'rows:', len(pred_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
