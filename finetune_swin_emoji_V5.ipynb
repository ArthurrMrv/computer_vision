{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning ConvNeXtV2 for Emoji Vendor Classification\n",
    "\n",
    "Using deterministic augmentation and TTA for improved performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q kagglehub transformers torch torchvision pillow datasets accelerate pandas scikit-learn\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import json\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import pandas as pd\n",
    "import random\n",
    "import hashlib\n",
    "\n",
    "# GPU Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. Training will be slow on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic Augmentation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified Deterministic Augmentation System\n",
    "# Same augmentation methods used for both training and TTA inference\n",
    "# Deterministic based on image hash/index for reproducibility\n",
    "\n",
    "import hashlib\n",
    "\n",
    "class DeterministicAugmentation:\n",
    "    \"\"\"\n",
    "    Unified deterministic augmentation system for both training and TTA.\n",
    "    Uses the same augmentation methods, but deterministic based on image content.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=224, seed=42):\n",
    "        self.image_size = image_size\n",
    "        self.seed = seed\n",
    "        \n",
    "        # Define augmentation parameters (same for training and TTA)\n",
    "        self.rotation_angles = [-10, -5, 5, 10]  # Fixed rotation angles\n",
    "        self.crop_ratios = [0.75, 0.85, 0.9, 0.95]  # Fixed crop ratios\n",
    "        self.color_jitter_params = {\n",
    "            'brightness': 0.3,\n",
    "            'contrast': 0.3,\n",
    "            'saturation': 0.3,\n",
    "            'hue': 0.1\n",
    "        }\n",
    "        self.translate_range = (0.1, 0.1)\n",
    "        self.blur_sigma = (0.1, 0.5)\n",
    "        \n",
    "    def _get_deterministic_seed(self, image_or_hash):\n",
    "        \"\"\"Generate deterministic seed from image hash or index.\"\"\"\n",
    "        if isinstance(image_or_hash, Image.Image):\n",
    "            # Use image content hash\n",
    "            img_bytes = image_or_hash.tobytes()\n",
    "            hash_val = int(hashlib.md5(img_bytes).hexdigest()[:8], 16)\n",
    "        elif isinstance(image_or_hash, (str, int)):\n",
    "            # Use provided hash/index\n",
    "            hash_val = hash(str(image_or_hash)) & 0xFFFFFFFF\n",
    "        else:\n",
    "            hash_val = hash(str(image_or_hash)) & 0xFFFFFFFF\n",
    "        return hash_val\n",
    "    \n",
    "    def horizontal_flip(self, image, apply=True):\n",
    "        \"\"\"Deterministic horizontal flip.\"\"\"\n",
    "        if apply:\n",
    "            return F.hflip(image)\n",
    "        return image\n",
    "    \n",
    "    def rotation(self, image, angle):\n",
    "        \"\"\"Deterministic rotation.\"\"\"\n",
    "        return F.rotate(image, angle)\n",
    "    \n",
    "    def center_crop(self, image, crop_ratio=0.9):\n",
    "        \"\"\"Deterministic center crop.\"\"\"\n",
    "        w, h = image.size\n",
    "        crop_size = int(min(w, h) * crop_ratio)\n",
    "        return F.center_crop(image, [crop_size, crop_size])\n",
    "    \n",
    "    def corner_crop(self, image, crop_ratio=0.9, position='tl'):\n",
    "        \"\"\"Deterministic corner crop (top-left, top-right, bottom-left, bottom-right).\"\"\"\n",
    "        w, h = image.size\n",
    "        crop_size = int(min(w, h) * crop_ratio)\n",
    "        \n",
    "        if position == 'tl':  # Top-left\n",
    "            return F.crop(image, 0, 0, crop_size, crop_size)\n",
    "        elif position == 'tr':  # Top-right\n",
    "            return F.crop(image, 0, w - crop_size, crop_size, crop_size)\n",
    "        elif position == 'bl':  # Bottom-left\n",
    "            return F.crop(image, h - crop_size, 0, crop_size, crop_size)\n",
    "        elif position == 'br':  # Bottom-right\n",
    "            return F.crop(image, h - crop_size, w - crop_size, crop_size, crop_size)\n",
    "        return image\n",
    "    \n",
    "    def resized_crop(self, image, crop_ratio=0.85):\n",
    "        \"\"\"Deterministic resized crop (simulating RandomResizedCrop).\"\"\"\n",
    "        w, h = image.size\n",
    "        crop_size = int(min(w, h) * crop_ratio)\n",
    "        # Use center crop as deterministic version\n",
    "        cropped = F.center_crop(image, [crop_size, crop_size])\n",
    "        return cropped.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "    \n",
    "    def color_jitter(self, image, seed_val):\n",
    "        \"\"\"Deterministic color jitter based on seed.\"\"\"\n",
    "        # Use seed to deterministically select jitter parameters\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        brightness_factor = 1.0 + np.random.uniform(-self.color_jitter_params['brightness'], \n",
    "                                                      self.color_jitter_params['brightness'])\n",
    "        contrast_factor = 1.0 + np.random.uniform(-self.color_jitter_params['contrast'],\n",
    "                                                  self.color_jitter_params['contrast'])\n",
    "        saturation_factor = 1.0 + np.random.uniform(-self.color_jitter_params['saturation'],\n",
    "                                                     self.color_jitter_params['saturation'])\n",
    "        hue_factor = np.random.uniform(-self.color_jitter_params['hue'],\n",
    "                                      self.color_jitter_params['hue'])\n",
    "        \n",
    "        # Apply deterministic color jitter\n",
    "        img = F.adjust_brightness(image, brightness_factor)\n",
    "        img = F.adjust_contrast(img, contrast_factor)\n",
    "        img = F.adjust_saturation(img, saturation_factor)\n",
    "        img = F.adjust_hue(img, hue_factor)\n",
    "        return img\n",
    "    \n",
    "    def affine_transform(self, image, seed_val):\n",
    "        \"\"\"Deterministic affine transform (translation).\"\"\"\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        translate_x = np.random.uniform(-self.translate_range[0], self.translate_range[0])\n",
    "        translate_y = np.random.uniform(-self.translate_range[1], self.translate_range[1])\n",
    "        return F.affine(image, angle=0, translate=(translate_x * image.width, translate_y * image.height),\n",
    "                        scale=1.0, shear=0.0)\n",
    "    \n",
    "    def gaussian_blur(self, image, seed_val):\n",
    "        \"\"\"Deterministic Gaussian blur.\"\"\"\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        sigma = np.random.uniform(self.blur_sigma[0], self.blur_sigma[1])\n",
    "        return F.gaussian_blur(image, kernel_size=3, sigma=[sigma, sigma])\n",
    "    \n",
    "    def get_augmentations(self, image, num_augmentations=10, seed_source=None):\n",
    "        \"\"\"\n",
    "        Generate deterministic augmented versions for TTA.\n",
    "        Uses same augmentation methods as training.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            num_augmentations: Number of augmentations to generate\n",
    "            seed_source: Optional seed source (image hash, index, etc.) for determinism\n",
    "        \"\"\"\n",
    "        augmentations = []\n",
    "        \n",
    "        # Get deterministic seed\n",
    "        if seed_source is None:\n",
    "            seed_source = self._get_deterministic_seed(image)\n",
    "        seed_val = seed_source\n",
    "        \n",
    "        # Original (resized)\n",
    "        augmentations.append(image.resize((self.image_size, self.image_size), Image.BILINEAR))\n",
    "        \n",
    "        # Horizontal flip\n",
    "        flipped = self.horizontal_flip(image, apply=True)\n",
    "        augmentations.append(flipped.resize((self.image_size, self.image_size), Image.BILINEAR))\n",
    "        \n",
    "        # Rotations (deterministic angles)\n",
    "        for angle in self.rotation_angles[:min(4, num_augmentations - len(augmentations))]:\n",
    "            rotated = self.rotation(image, angle)\n",
    "            augmentations.append(rotated.resize((self.image_size, self.image_size), Image.BILINEAR))\n",
    "        \n",
    "        # Corner crops (4 corners)\n",
    "        corners = ['tl', 'tr', 'bl', 'br']\n",
    "        for corner in corners[:min(4, num_augmentations - len(augmentations))]:\n",
    "            cropped = self.corner_crop(image, crop_ratio=0.9, position=corner)\n",
    "            augmentations.append(cropped.resize((self.image_size, self.image_size), Image.BILINEAR))\n",
    "        \n",
    "        # Center crop\n",
    "        if len(augmentations) < num_augmentations:\n",
    "            center_cropped = self.center_crop(image, crop_ratio=0.9)\n",
    "            augmentations.append(center_cropped.resize((self.image_size, self.image_size), Image.BILINEAR))\n",
    "        \n",
    "        # Resized crop (simulating RandomResizedCrop)\n",
    "        if len(augmentations) < num_augmentations:\n",
    "            resized_cropped = self.resized_crop(image, crop_ratio=0.85)\n",
    "            augmentations.append(resized_cropped)\n",
    "        \n",
    "        # Color jitter\n",
    "        if len(augmentations) < num_augmentations:\n",
    "            jittered = self.color_jitter(image, seed_val)\n",
    "            augmentations.append(jittered.resize((self.image_size, self.image_size), Image.BILINEAR))\n",
    "        \n",
    "        # Affine transform\n",
    "        if len(augmentations) < num_augmentations:\n",
    "            affine_img = self.affine_transform(image, seed_val + 1)\n",
    "            augmentations.append(affine_img.resize((self.image_size, self.image_size), Image.BILINEAR))\n",
    "        \n",
    "        # Gaussian blur\n",
    "        if len(augmentations) < num_augmentations:\n",
    "            blurred = self.gaussian_blur(image, seed_val + 2)\n",
    "            augmentations.append(blurred.resize((self.image_size, self.image_size), Image.BILINEAR))\n",
    "        \n",
    "        return augmentations[:num_augmentations]\n",
    "    \n",
    "    def apply_training_augmentation(self, image, index=None):\n",
    "        \"\"\"\n",
    "        Apply deterministic training augmentation.\n",
    "        Uses same methods as TTA but applied once per training sample.\n",
    "        \n",
    "        Args:\n",
    "            image: PIL Image\n",
    "            index: Optional index for deterministic seed\n",
    "        \"\"\"\n",
    "        # Get deterministic seed from index or image\n",
    "        if index is not None:\n",
    "            seed_val = hash(str(index)) & 0xFFFFFFFF\n",
    "        else:\n",
    "            seed_val = self._get_deterministic_seed(image)\n",
    "        \n",
    "        # Apply augmentations deterministically based on seed\n",
    "        np.random.seed(seed_val % (2**32))\n",
    "        \n",
    "        # Horizontal flip (50% probability, but deterministic)\n",
    "        should_flip = (seed_val % 2 == 0)\n",
    "        if should_flip:\n",
    "            image = self.horizontal_flip(image, apply=True)\n",
    "        \n",
    "        # Rotation (deterministic angle selection)\n",
    "        angle_idx = (seed_val // 2) % len(self.rotation_angles)\n",
    "        angle = self.rotation_angles[angle_idx]\n",
    "        image = self.rotation(image, angle)\n",
    "        \n",
    "        # Resized crop (deterministic crop ratio)\n",
    "        crop_idx = (seed_val // 10) % len(self.crop_ratios)\n",
    "        crop_ratio = self.crop_ratios[crop_idx]\n",
    "        w, h = image.size\n",
    "        crop_size = int(min(w, h) * crop_ratio)\n",
    "        image = F.center_crop(image, [crop_size, crop_size])\n",
    "        \n",
    "        # Color jitter (deterministic)\n",
    "        image = self.color_jitter(image, seed_val)\n",
    "        \n",
    "        # Affine transform (deterministic, 50% probability)\n",
    "        if (seed_val // 3) % 2 == 0:\n",
    "            image = self.affine_transform(image, seed_val + 1)\n",
    "        \n",
    "        # Gaussian blur (deterministic, 20% probability)\n",
    "        if (seed_val // 5) % 5 == 0:\n",
    "            image = self.gaussian_blur(image, seed_val + 2)\n",
    "        \n",
    "        # Resize to final size\n",
    "        image = image.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Create global augmentation instance\n",
    "augmentation_system = DeterministicAugmentation(image_size=224, seed=42)\n",
    "\n",
    "# For backward compatibility\n",
    "EnhancedTTAAugmentation = DeterministicAugmentation\n",
    "TTAAugmentation = DeterministicAugmentation\n",
    "\n",
    "# Training transform using deterministic augmentation\n",
    "# This will be applied in EmojiDataset using the augmentation_system\n",
    "train_transform = None  # Will use augmentation_system.apply_training_augmentation instead\n",
    "\n",
    "print(\"Unified deterministic augmentation system defined!\")\n",
    "print(\"Same augmentation methods used for both training and TTA inference.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ConvNeXtV2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/convnextv2-tiny-22k-224\")\n",
    "base_model = AutoModelForImageClassification.from_pretrained(\"facebook/convnextv2-tiny-22k-224\")\n",
    "\n",
    "# Move model to GPU\n",
    "base_model = base_model.to(device)\n",
    "print(f\"ConvNeXtV2 model loaded and moved to {device}\")\n",
    "print(f\"Model config: {base_model.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Vendor Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VENDOR_CLASSES = [\n",
    "    \"Apple\", \"DoCoMo\", \"Facebook\", \"Gmail\", \"Google\", \"JoyPixels\",\n",
    "    \"KDDI\", \"Samsung\", \"SoftBank\", \"Twitter\", \"Windows\"\n",
    "]\n",
    "\n",
    "VENDOR_TO_IDX = {vendor: idx for idx, vendor in enumerate(VENDOR_CLASSES)}\n",
    "IDX_TO_VENDOR = {idx: vendor for vendor, idx in VENDOR_TO_IDX.items()}\n",
    "\n",
    "print(f\"Number of vendor classes: {len(VENDOR_CLASSES)}\")\n",
    "print(\"Vendor classes:\", VENDOR_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiDataset(Dataset):\n",
    "    \"\"\"Dataset class with support for deterministic training-time augmentation.\"\"\"\n",
    "    def __init__(self, image_paths, labels, processor, use_augmentation=False, augmentation_system=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.use_augmentation = use_augmentation\n",
    "        self.augmentation_system = augmentation_system if augmentation_system is not None else globals().get('augmentation_system', None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            image = Image.new('RGB', (224, 224), color='white')\n",
    "\n",
    "        if self.use_augmentation:\n",
    "            aug_system = globals().get('augmentation_system', None)\n",
    "            if aug_system is not None:\n",
    "                image = aug_system.apply_training_augmentation(image, index=idx)\n",
    "\n",
    "        inputs = self.processor(image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeXtV2ForEmojiClassification(nn.Module):    \n",
    "    def __init__(self, num_labels=len(VENDOR_CLASSES), hidden_size=None):        \n",
    "        super().__init__()        \n",
    "        self.base_model = base_model        \n",
    "        self.num_labels = num_labels        # Get the hidden size from the model config        \n",
    "        if hidden_size is None:            # ConvNeXtV2 uses hidden_sizes list            \n",
    "            if hasattr(base_model.config, 'hidden_sizes'):                \n",
    "                hidden_size = base_model.config.hidden_sizes[-1]            \n",
    "            elif hasattr(base_model.config, 'hidden_size'):                \n",
    "                hidden_size = base_model.config.hidden_size            \n",
    "            else:                # Default for tiny model                \n",
    "                hidden_size = 768        # Improved classification head with more capacity        \n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(hidden_size),\n",
    "        nn.Dropout(0.3),            \n",
    "        nn.Linear(hidden_size, hidden_size // 2),            \n",
    "        nn.GELU(),            \n",
    "        nn.LayerNorm(hidden_size // 2),            \n",
    "        nn.Dropout(0.2),            \n",
    "        nn.Linear(hidden_size // 2, num_labels))    \n",
    "        \n",
    "    def forward(self, pixel_values, labels=None):        # Get embeddings from ConvNeXtV2        # ConvNeXtV2 outputs ImageClassifierOutput        outputs = self.base_model(pixel_values=pixel_values, output_hidden_states=True)                # Extract pooled features        # For ConvNeXtV2, hidden_states contains feature maps        if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:            # Get the last hidden state (feature map)            feature_map = outputs.hidden_states[-1]  # Shape: (batch, channels, H, W)            # Global average pooling            pooled_output = feature_map.mean(dim=[2, 3])  # Shape: (batch, channels)        else:            # Fallback: use the model's pooler if available            # For some models, we can access the backbone directly            with torch.no_grad():                # Get features before classifier                backbone_output = self.base_model.convnextv2(pixel_values)                if len(backbone_output.shape) == 4:                    pooled_output = backbone_output.mean(dim=[2, 3])                else:                    pooled_output = backbone_output        # Classification        logits = self.classifier(pooled_output)        loss = None        if labels is not None:            # Add label smoothing for better generalization            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1)            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))        return ImageClassifierOutput(            loss=loss,            logits=logits        )# Create the modelclassification_model = ConvNeXtV2ForEmojiClassification(num_labels=len(VENDOR_CLASSES))classification_model = classification_model.to(device)print(\"ConvNeXtV2 model created and moved to device\")\n",
    "        # Get features from ConvNeXtV2 backbone\n",
    "        # Access the convnextv2 backbone directly\n",
    "        features = self.base_model.convnextv2(pixel_values)\n",
    "        # Global average pooling: (B, C, H, W) -> (B, C)\n",
    "        if len(features.shape) == 4:\n",
    "            pooled_output = features.mean(dim=[2, 3])\n",
    "        else:\n",
    "            pooled_output = features\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return ImageClassifierOutput(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device, scaler=None):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    is_cuda_available = (device.type == 'cuda')\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        labels = batch['labels'].to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=is_cuda_available):\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / len(train_loader), 100 * correct / total\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    is_cuda_available = (device.type == 'cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=is_cuda_available):\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return total_loss / len(val_loader), accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA Inference Functions (for final tests only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_weighted_tta(model, image, processor, tta_aug, num_augmentations=10, device='cuda'):\n",
    "    \"\"\"Predict using Test-Time Augmentation with weighted averaging.\"\"\"\n",
    "    model.eval()\n",
    "    augmented_images = tta_aug.get_augmentations(image, num_augmentations=num_augmentations)\n",
    "    all_logits = []\n",
    "    weights = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, aug_image in enumerate(augmented_images):\n",
    "            inputs = processor(aug_image, return_tensors=\"pt\")\n",
    "            pixel_values = inputs['pixel_values'].to(device)\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            all_logits.append(logits)\n",
    "            weight = 2.0 if i == 0 else 1.0\n",
    "            weights.append(weight)\n",
    "    \n",
    "    weights = torch.tensor(weights, device=device).view(-1, 1, 1)\n",
    "    weighted_logits = torch.stack(all_logits) * weights\n",
    "    averaged_logits = weighted_logits.sum(dim=0) / weights.sum()\n",
    "    probabilities = torch.softmax(averaged_logits, dim=-1)\n",
    "    predicted_class = torch.argmax(averaged_logits, dim=-1)\n",
    "    return averaged_logits, predicted_class, probabilities\n",
    "\n",
    "def predict_with_tta(model, image, processor, tta_aug, num_augmentations=10, device='cuda'):\n",
    "    \"\"\"Predict using Test-Time Augmentation.\"\"\"\n",
    "    return predict_with_weighted_tta(model, image, processor, tta_aug, num_augmentations, device)\n",
    "\n",
    "tta_aug = augmentation_system\n",
    "print(\"TTA inference functions defined!\")\n",
    "print(\"Using unified deterministic augmentation system for TTA (same as training).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_path):\n",
    "    \"\"\"Prepare dataset by finding all images and their corresponding vendor labels.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    dataset_path = Path(dataset_path)\n",
    "    image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
    "\n",
    "    for vendor in VENDOR_CLASSES:\n",
    "        vendor_dir = dataset_path / vendor\n",
    "        if vendor_dir.exists() and vendor_dir.is_dir():\n",
    "            for ext in image_extensions:\n",
    "                images = list(vendor_dir.glob(f\"*{ext}\"))\n",
    "                for img_path in images:\n",
    "                    image_paths.append(str(img_path))\n",
    "                    labels.append(VENDOR_TO_IDX[vendor])\n",
    "\n",
    "    if len(image_paths) == 0:\n",
    "        for ext in image_extensions:\n",
    "            all_images = list(dataset_path.rglob(f\"*{ext}\"))\n",
    "            for img_path in all_images:\n",
    "                filename = img_path.name.lower()\n",
    "                for vendor in VENDOR_CLASSES:\n",
    "                    if vendor.lower() in filename or vendor.lower() in str(img_path.parent).lower():\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(VENDOR_TO_IDX[vendor])\n",
    "                        break\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "def prepare_dataset_from_csv(train_dir, csv_path):\n",
    "    \"\"\"Prepare dataset by loading images and labels from CSV file.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    train_dir = Path(train_dir)\n",
    "    csv_path = Path(csv_path)\n",
    "\n",
    "    if not csv_path.exists() or not train_dir.exists():\n",
    "        print(f\"WARNING: CSV or train directory not found\")\n",
    "        return image_paths, labels\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "    explicit_mapping = {'messenger': 'Facebook', 'whatsapp': 'Facebook', 'mozilla': 'Google'}\n",
    "    unique_labels = df['Label'].str.lower().unique()\n",
    "    label_mapping = {}\n",
    "    \n",
    "    for csv_label in unique_labels:\n",
    "        matched = False\n",
    "        if csv_label in explicit_mapping:\n",
    "            mapped_vendor = explicit_mapping[csv_label]\n",
    "            for idx, vendor in enumerate(VENDOR_CLASSES):\n",
    "                if vendor.lower() == mapped_vendor.lower():\n",
    "                    label_mapping[csv_label.lower()] = idx\n",
    "                    matched = True\n",
    "                    break\n",
    "        if not matched:\n",
    "            for idx, vendor in enumerate(VENDOR_CLASSES):\n",
    "                if csv_label == vendor.lower():\n",
    "                    label_mapping[csv_label.lower()] = idx\n",
    "                    matched = True\n",
    "                    break\n",
    "        if not matched:\n",
    "            for idx, vendor in enumerate(VENDOR_CLASSES):\n",
    "                if csv_label in vendor.lower() or vendor.lower() in csv_label:\n",
    "                    label_mapping[csv_label.lower()] = idx\n",
    "                    matched = True\n",
    "                    break\n",
    "        if not matched:\n",
    "            print(f\"WARNING: Label '{csv_label}' not found, skipping\")\n",
    "\n",
    "    skipped_count = 0\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = str(row['Id']).zfill(5)\n",
    "        label_str = str(row['Label']).lower()\n",
    "        \n",
    "        if label_str not in label_mapping:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "        \n",
    "        image_found = False\n",
    "        for ext in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']:\n",
    "            image_path = train_dir / f\"{image_id}{ext}\"\n",
    "            if image_path.exists():\n",
    "                image_paths.append(str(image_path))\n",
    "                labels.append(label_mapping[label_str])\n",
    "                image_found = True\n",
    "                break\n",
    "        \n",
    "        if not image_found:\n",
    "            print(f\"WARNING: Image not found for ID {image_id}\")\n",
    "    \n",
    "    if skipped_count > 0:\n",
    "        print(f\"WARNING: Skipped {skipped_count} images due to unmapped labels\")\n",
    "    \n",
    "    print(f\"Loaded {len(image_paths)} images with {len(set(labels))} unique classes\")\n",
    "    if len(labels) > 0:\n",
    "        label_counts = np.bincount(labels)\n",
    "        print(f\"Label distribution: {label_counts}\")\n",
    "    \n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: First Dataset - Download and Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download first dataset\n",
    "path = kagglehub.dataset_download(\"subinium/emojiimage-dataset\")\n",
    "print(f\"Path to dataset files: {path}\")\n",
    "\n",
    "# Prepare dataset\n",
    "image_paths, labels = prepare_dataset(path)\n",
    "\n",
    "print(f\"\\nFound {len(image_paths)} images\")\n",
    "if len(labels) > 0:\n",
    "    print(f\"Labels distribution: {np.bincount(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Split First Dataset and Create Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split first dataset into train and test\n",
    "if len(image_paths) > 0:\n",
    "    train_paths, test_paths, train_labels, test_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    train_dataset = EmojiDataset(train_paths, train_labels, processor, use_augmentation=True)\n",
    "    test_dataset = EmojiDataset(test_paths, test_labels, processor, use_augmentation=False)\n",
    "\n",
    "    batch_size = 16 if torch.cuda.is_available() else 8\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=4 if torch.cuda.is_available() else 2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False,\n",
    "        num_workers=4 if torch.cuda.is_available() else 2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(\"Training augmentation: ENABLED\")\n",
    "else:\n",
    "    print(\"ERROR: No images found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 15\n",
    "learning_rate = 1e-5\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    classification_model.parameters(), lr=learning_rate, weight_decay=0.01\n",
    ")\n",
    "\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=3, verbose=True, min_lr=1e-7, cooldown=2\n",
    ")\n",
    "\n",
    "scaler = None\n",
    "if torch.cuda.is_available():\n",
    "    model_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32\n",
    "    if model_dtype == torch.float16:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        print(\"Mixed precision training: Enabled (float16 with GradScaler)\")\n",
    "    elif model_dtype == torch.bfloat16:\n",
    "        print(\"Mixed precision training: Enabled (bfloat16 without GradScaler)\")\n",
    "    else:\n",
    "        print(\"Mixed precision training: Disabled\")\n",
    "else:\n",
    "    print(\"Mixed precision training: Disabled (CPU)\")\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Train on First Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training on first dataset\n",
    "if len(image_paths) > 0:\n",
    "    print(\"Starting training on first dataset...\")\n",
    "    best_val_acc = 0\n",
    "    early_stopping_patience = 5\n",
    "    early_stopping_counter = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        train_loss, train_acc = train_epoch(classification_model, train_loader, optimizer, device, scaler)\n",
    "        val_loss, val_acc, val_preds, val_labels = validate(classification_model, test_loader, device)\n",
    "\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Test Loss: {val_loss:.4f}, Test Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB / {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(classification_model.state_dict(), 'best_model_phase1.pt')\n",
    "            print(f\"✓ Saved best model with test accuracy: {best_val_acc:.2f}%\")\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            print(f\"No improvement. Patience: {early_stopping_counter}/{early_stopping_patience}\")\n",
    "        \n",
    "        if early_stopping_counter >= early_stopping_patience:\n",
    "            print(f\"\\nEarly stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nPhase 1 training completed! Best test accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")\n",
    "    \n",
    "    # Print detailed scores\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Phase 1 Final Scores\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Best Test Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(val_labels, val_preds, target_names=VENDOR_CLASSES))\n",
    "else:\n",
    "    print(\"ERROR: Cannot train without data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Fine-tune on Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and test splits for final training (no data leakage)\n",
    "if len(test_paths) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Training on Combined Dataset (Train + Test)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load best model from phase 1\n",
    "    if os.path.exists('best_model_phase1.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_model_phase1.pt', map_location=device))\n",
    "        print(\"Loaded best model from phase 1\")\n",
    "    \n",
    "    # Combine train and test splits\n",
    "    combined_train_paths = train_paths + test_paths\n",
    "    combined_train_labels = train_labels + test_labels\n",
    "    \n",
    "    print(f\"Combined dataset: {len(combined_train_paths)} samples\")\n",
    "    print(f\"  - Original train: {len(train_paths)} samples\")\n",
    "    print(f\"  - Original test: {len(test_paths)} samples\")\n",
    "    print(f\"Combined label distribution: {np.bincount(combined_train_labels)}\")\n",
    "    \n",
    "    # Create combined dataset with augmentation\n",
    "    combined_train_dataset = EmojiDataset(\n",
    "        combined_train_paths, combined_train_labels, processor, use_augmentation=True\n",
    "    )\n",
    "    \n",
    "    combined_train_loader = DataLoader(\n",
    "        combined_train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=4 if torch.cuda.is_available() else 2,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    # Final training parameters (lower learning rate for fine-tuning)\n",
    "    final_epochs = 5\n",
    "    final_lr = 5e-6\n",
    "    \n",
    "    final_optimizer = torch.optim.AdamW(\n",
    "        classification_model.parameters(), lr=final_lr, weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    final_scheduler = ReduceLROnPlateau(\n",
    "        final_optimizer, mode='max', factor=0.5, patience=2, verbose=True, min_lr=1e-7, cooldown=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining on combined dataset for {final_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(final_epochs):\n",
    "        print(f\"\\nFinal Training Epoch {epoch + 1}/{final_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(\n",
    "            classification_model, combined_train_loader, final_optimizer, device, scaler\n",
    "        )\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        final_scheduler.step(train_acc)\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(classification_model.state_dict(), 'best_model_phase1_final.pt')\n",
    "    print(\"\\n✓ Final training completed! Saved best_model_phase1_final.pt\")\n",
    "else:\n",
    "    print(\"No test split available for final training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Second Dataset - Load and Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load second dataset (single path, no alternatives)\n",
    "dataset_base = Path(\"/content/drive/MyDrive/vision/2-computer-vision-2025-b-sc-aidams-final-proj\")\n",
    "train_dir = dataset_base / \"train\"\n",
    "csv_path = dataset_base / \"train_labels.csv\"\n",
    "\n",
    "if not train_dir.exists():\n",
    "    print(f\"ERROR: Train directory not found at {train_dir}\")\n",
    "    print(\"Please ensure the dataset is in the correct location.\")\n",
    "    second_dataset_paths = []\n",
    "    second_dataset_labels = []\n",
    "else:\n",
    "    print(f\"Found train directory at: {train_dir}\")\n",
    "    print(f\"Found CSV file at: {csv_path}\")\n",
    "    \n",
    "    second_dataset_paths, second_dataset_labels = prepare_dataset_from_csv(train_dir, csv_path)\n",
    "    print(f\"\\nFound {len(second_dataset_paths)} labeled images from second dataset\")\n",
    "    \n",
    "    if len(second_dataset_paths) > 0:\n",
    "        print(f\"Label distribution: {np.bincount(second_dataset_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Split Second Dataset and Re-fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split second dataset into train and test\n",
    "if len(second_dataset_paths) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Re-fine-tuning on Second Dataset\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if os.path.exists('best_model_phase1_final.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_model_phase1_final.pt', map_location=device))\n",
    "        print(\"Loaded best final model from phase 1 (trained on combined data)\")\n",
    "    elif os.path.exists('best_model_phase1.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_model_phase1.pt', map_location=device))\n",
    "        print(\"Loaded best model from phase 1\")\n",
    "    \n",
    "    if len(second_dataset_paths) > 100:\n",
    "        unique_labels, label_counts = np.unique(second_dataset_labels, return_counts=True)\n",
    "        min_class_count = label_counts.min()\n",
    "        can_stratify = min_class_count >= 2\n",
    "        \n",
    "        if can_stratify:\n",
    "            try:\n",
    "                second_train_paths, second_val_paths, second_train_labels, second_val_labels = train_test_split(\n",
    "                    second_dataset_paths, second_dataset_labels, test_size=0.1, random_state=42, stratify=second_dataset_labels\n",
    "                )\n",
    "                print(f\"✓ Split with STRATIFIED sampling: {len(second_train_paths)} train, {len(second_val_paths)} validation\")\n",
    "            except ValueError as e:\n",
    "                print(f\"⚠️ Stratification failed: {e}\")\n",
    "                second_train_paths, second_val_paths, second_train_labels, second_val_labels = train_test_split(\n",
    "                    second_dataset_paths, second_dataset_labels, test_size=0.1, random_state=42\n",
    "                )\n",
    "                print(f\"Split without stratification: {len(second_train_paths)} train, {len(second_val_paths)} validation\")\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Cannot stratify (min class count: {min_class_count} < 2)\")\n",
    "            second_train_paths, second_val_paths, second_train_labels, second_val_labels = train_test_split(\n",
    "                second_dataset_paths, second_dataset_labels, test_size=0.1, random_state=42\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nTrain label distribution: {np.bincount(second_train_labels)}\")\n",
    "        print(f\"Validation label distribution: {np.bincount(second_val_labels)}\")\n",
    "    else:\n",
    "        second_train_paths, second_val_paths = second_dataset_paths, []\n",
    "        second_train_labels, second_val_labels = second_dataset_labels, []\n",
    "        print(\"Dataset too small for validation split, using all for training\")\n",
    "    \n",
    "    second_train_dataset = EmojiDataset(second_train_paths, second_train_labels, processor, use_augmentation=True)\n",
    "    \n",
    "    if len(second_val_paths) > 0:\n",
    "        second_val_dataset = EmojiDataset(second_val_paths, second_val_labels, processor, use_augmentation=False)\n",
    "        second_val_loader = DataLoader(\n",
    "            second_val_dataset, batch_size=batch_size, shuffle=False,\n",
    "            num_workers=4 if torch.cuda.is_available() else 2, pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "    else:\n",
    "        second_val_loader = None\n",
    "    \n",
    "    second_train_loader = DataLoader(\n",
    "        second_train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=4 if torch.cuda.is_available() else 2, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(f\"Re-fine-tuning samples: {len(second_train_dataset)}\")\n",
    "    if second_val_loader:\n",
    "        print(f\"Validation samples: {len(second_val_dataset)}\")\n",
    "    \n",
    "    refinetune_epochs = 7\n",
    "    refinetune_lr = 1e-5\n",
    "    refinetune_optimizer = torch.optim.AdamW(classification_model.parameters(), lr=refinetune_lr, weight_decay=0.01)\n",
    "    refinetune_scheduler = ReduceLROnPlateau(refinetune_optimizer, mode='max', factor=0.5, patience=2, verbose=True, min_lr=1e-7, cooldown=1)\n",
    "    \n",
    "    best_refinetune_acc = 0\n",
    "    refinetune_early_stopping_patience = 3\n",
    "    refinetune_early_stopping_counter = 0\n",
    "    best_refinetune_epoch = 0\n",
    "    \n",
    "    for epoch in range(refinetune_epochs):\n",
    "        print(f\"\\nRe-fine-tuning Epoch {epoch + 1}/{refinetune_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        train_loss, train_acc = train_epoch(classification_model, second_train_loader, refinetune_optimizer, device, scaler)\n",
    "\n",
    "        if second_val_loader:\n",
    "            val_loss, val_acc, val_preds, val_labels = validate(classification_model, second_val_loader, device)\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "            refinetune_scheduler.step(val_acc)\n",
    "\n",
    "            if val_acc > best_refinetune_acc:\n",
    "                best_refinetune_acc = val_acc\n",
    "                best_refinetune_epoch = epoch + 1\n",
    "                refinetune_early_stopping_counter = 0\n",
    "                torch.save(classification_model.state_dict(), 'best_model_phase2.pt')\n",
    "                print(f\"✓ Saved best re-fine-tuned model with validation accuracy: {best_refinetune_acc:.2f}%\")\n",
    "            else:\n",
    "                refinetune_early_stopping_counter += 1\n",
    "                print(f\"No improvement. Patience: {refinetune_early_stopping_counter}/{refinetune_early_stopping_patience}\")\n",
    "            \n",
    "            if refinetune_early_stopping_counter >= refinetune_early_stopping_patience:\n",
    "                print(f\"\\nEarly stopping triggered!\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            refinetune_scheduler.step(train_acc)\n",
    "            torch.save(classification_model.state_dict(), 'best_model_phase2.pt')\n",
    "\n",
    "    print(f\"\\nPhase 2 re-fine-tuning completed! Best validation accuracy: {best_refinetune_acc:.2f}% at epoch {best_refinetune_epoch}\")\n",
    "    \n",
    "    if second_val_loader:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"Phase 2 Final Scores\")\n",
    "        print(\"=\"*50)\n",
    "        print(f\"Best Validation Accuracy: {best_refinetune_acc:.2f}%\")\n",
    "        print(f\"\\nClassification Report:\")\n",
    "        print(classification_report(val_labels, val_preds, target_names=VENDOR_CLASSES))\n",
    "else:\n",
    "    print(\"Skipping re-fine-tuning (no second dataset found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "if second_val_loader:\n",
    "    cm = confusion_matrix(val_labels, val_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=VENDOR_CLASSES, yticklabels=VENDOR_CLASSES)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Final Training on Combined Train+Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine train and validation splits for final training (no data leakage)\n",
    "if len(second_val_paths) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Training on Combined Second Dataset (Train + Validation)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load best model from phase 2\n",
    "    if os.path.exists('best_model_phase2.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_model_phase2.pt', map_location=device))\n",
    "        print(\"Loaded best model from phase 2\")\n",
    "    \n",
    "    # Combine train and validation splits\n",
    "    combined_second_train_paths = second_train_paths + second_val_paths\n",
    "    combined_second_train_labels = second_train_labels + second_val_labels\n",
    "    \n",
    "    print(f\"Combined second dataset: {len(combined_second_train_paths)} samples\")\n",
    "    print(f\"  - Original train: {len(second_train_paths)} samples\")\n",
    "    print(f\"  - Original validation: {len(second_val_paths)} samples\")\n",
    "    print(f\"Combined label distribution: {np.bincount(combined_second_train_labels)}\")\n",
    "    \n",
    "    # Create combined dataset with augmentation\n",
    "    combined_second_train_dataset = EmojiDataset(\n",
    "        combined_second_train_paths, combined_second_train_labels, processor, use_augmentation=True\n",
    "    )\n",
    "    \n",
    "    combined_second_train_loader = DataLoader(\n",
    "        combined_second_train_dataset, batch_size=batch_size, shuffle=True,\n",
    "        num_workers=4 if torch.cuda.is_available() else 2, pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    # Final training parameters\n",
    "    final_second_epochs = 3\n",
    "    final_second_lr = 5e-6\n",
    "    \n",
    "    final_second_optimizer = torch.optim.AdamW(\n",
    "        classification_model.parameters(), lr=final_second_lr, weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    final_second_scheduler = ReduceLROnPlateau(\n",
    "        final_second_optimizer, mode='max', factor=0.5, patience=2, verbose=True, min_lr=1e-7, cooldown=1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining on combined second dataset for {final_second_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(final_second_epochs):\n",
    "        print(f\"\\nFinal Training Epoch {epoch + 1}/{final_second_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(\n",
    "            classification_model, combined_second_train_loader, final_second_optimizer, device, scaler\n",
    "        )\n",
    "        \n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        final_second_scheduler.step(train_acc)\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(classification_model.state_dict(), 'best_model_final.pt')\n",
    "    print(\"\\n✓ Final training completed! Saved best_model_final.pt\")\n",
    "else:\n",
    "    print(\"No validation split available for final training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions on Original Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original test data and generate predictions\n",
    "test_dataset_path = dataset_base / \"test\"\n",
    "\n",
    "if test_dataset_path.exists():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Generating Predictions on Original Test Data\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load best final model (trained on combined data)\n",
    "    if os.path.exists('best_model_final.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_model_final.pt', map_location=device))\n",
    "        print(\"Using best final model (trained on combined data)\")\n",
    "    elif os.path.exists('best_model_phase2.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_model_phase2.pt', map_location=device))\n",
    "        print(\"Using best phase 2 model\")\n",
    "    elif os.path.exists('best_model_phase1_final.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_model_phase1_final.pt', map_location=device))\n",
    "        print(\"Using best phase 1 final model\")\n",
    "    \n",
    "    classification_model.eval()\n",
    "    \n",
    "    test_image_paths = []\n",
    "    image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
    "    for ext in image_extensions:\n",
    "        test_image_paths.extend(list(test_dataset_path.rglob(f\"*{ext}\")))\n",
    "    test_image_paths = [str(p) for p in test_image_paths]\n",
    "    test_image_paths.sort()\n",
    "    \n",
    "    print(f\"Found {len(test_image_paths)} test images\")\n",
    "    \n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "    \n",
    "    print(f\"Processing {len(test_image_paths)} test images with TTA...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image_path in tqdm(test_image_paths, desc=\"Generating predictions\"):\n",
    "            try:\n",
    "                image_id = Path(image_path).stem\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                \n",
    "                _, predicted_class, probabilities = predict_with_tta(\n",
    "                    classification_model, image, processor, tta_aug,\n",
    "                    num_augmentations=10, device=device\n",
    "                )\n",
    "                \n",
    "                predicted_idx = predicted_class.item()\n",
    "                if predicted_idx >= len(VENDOR_CLASSES):\n",
    "                    print(f\"WARNING: Invalid prediction index {predicted_idx}, using first class\")\n",
    "                    predicted_idx = 0\n",
    "                \n",
    "                predicted_label = IDX_TO_VENDOR[predicted_idx]\n",
    "                predictions.append(predicted_label)\n",
    "                image_ids.append(image_id)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                predictions.append(VENDOR_CLASSES[0])\n",
    "                image_ids.append(Path(image_path).stem)\n",
    "    \n",
    "    predictions_file = \"predictions.csv\"\n",
    "    with open(predictions_file, 'w') as f:\n",
    "        f.write(\"Id,Label\\n\")\n",
    "        for img_id, pred_label in zip(image_ids, predictions):\n",
    "            clean_id = str(img_id).strip()\n",
    "            f.write(f\"{clean_id},{pred_label}\\n\")\n",
    "    \n",
    "    print(f\"\\nPredictions saved to {predictions_file}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"Unique image IDs: {len(set(image_ids))}\")\n",
    "    \n",
    "    from collections import Counter\n",
    "    label_counts = Counter(predictions)\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        percentage = 100 * count / len(predictions)\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "    verification_df = pd.read_csv(predictions_file)\n",
    "    print(f\"\\nVerification - Loaded {len(verification_df)} rows from {predictions_file}\")\n",
    "    print(f\"Columns: {verification_df.columns.tolist()}\")\n",
    "    print(f\"\\nSample predictions:\")\n",
    "    print(verification_df.head(10))\n",
    "    \n",
    "    print(\"\\n✓ Predictions generation completed!\")\n",
    "else:\n",
    "    print(f\"Test directory not found at {test_dataset_path}\")\n",
    "    print(\"Cannot generate predictions.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
