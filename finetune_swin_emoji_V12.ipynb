{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# V12 — Pre-trained DINOv2 + ConvNeXtV2 with HuggingFace Dataset + Meta-Model\n",
        "\n",
        "**Goal:** improve accuracy by pre-training models on HuggingFace dataset, then fine-tuning on target dataset, and stacking in LightGBM.\n",
        "\n",
        "**Pipeline:**\n",
        "- Download HuggingFace dataset (`subinium/emojiimage-dataset`)\n",
        "- Map 11 vendor classes to 7 target classes\n",
        "- **Pre-train DINOv2** on HuggingFace dataset (seed=42)\n",
        "- **Pre-train ConvNeXtV2** on HuggingFace dataset (seed=42)\n",
        "- Split target dataset train/val (stratified if possible)\n",
        "- **Fine-tune DINOv2** on target dataset (seed=42)\n",
        "- **Fine-tune ConvNeXtV2** on target dataset (seed=42)\n",
        "- For each image: compute deterministic TTA prob-vectors for **both models** + statistical features\n",
        "- Train **LightGBM** meta-model on top\n",
        "\n",
        "**Classes:** apple, google, whatsapp, facebook, samsung, mozilla, messenger\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import hashlib\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
        "from transformers.modeling_outputs import ImageClassifierOutput\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    HAS_LGB = True\n",
        "except Exception as e:\n",
        "    HAS_LGB = False\n",
        "    print('LightGBM import failed:', e)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HYPERPARAMETERS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data\n",
        "SECOND_DATASET_BASE_PATH = '.'\n",
        "SECOND_DATASET_TRAIN_DIR = Path(SECOND_DATASET_BASE_PATH) / 'train'\n",
        "SECOND_DATASET_CSV_PATH = Path(SECOND_DATASET_BASE_PATH) / 'train_labels.csv'\n",
        "SECOND_DATASET_TEST_DIR = Path(SECOND_DATASET_BASE_PATH) / 'test'\n",
        "\n",
        "# HuggingFace Dataset\n",
        "HF_DATASET_ID = 'subinium/emojiimage-dataset'\n",
        "\n",
        "# Models\n",
        "DINO_MODEL_ID = 'facebook/dinov2-base'\n",
        "CNN_MODEL_ID  = 'facebook/convnextv2-base-22k-224'\n",
        "SEED = 42\n",
        "\n",
        "# Train\n",
        "VAL_SIZE = 0.10\n",
        "RANDOM_STATE = 42\n",
        "NUM_EPOCHS = 20\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "LEARNING_RATE = 1e-5\n",
        "BATCH_SIZE_CUDA = 16\n",
        "BATCH_SIZE_CPU = 4\n",
        "NUM_WORKERS = 2\n",
        "LABEL_SMOOTHING = 0.05\n",
        "\n",
        "# TTA\n",
        "NUM_TTA_AUGS = 10\n",
        "TQDM_MININTERVAL = 10\n",
        "SHOW_PROGRESS = True  # set False to reduce output\n",
        "FEATURE_BATCH_SIZE = 16  # images per batch for feature extraction\n",
        "\n",
        "# Meta-model\n",
        "USE_LIGHTGBM = True\n",
        "LGB_PARAMS = {\n",
        "    'n_estimators': 1200,\n",
        "    'learning_rate': 0.03,\n",
        "    'num_leaves': 63,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'random_state': RANDOM_STATE\n",
        "}\n",
        "\n",
        "# Output\n",
        "PREDICTIONS_OUTPUT_FILE = 'predictions_V12.csv'\n",
        "\n",
        "print('DINO_MODEL_ID:', DINO_MODEL_ID)\n",
        "print('CNN_MODEL_ID :', CNN_MODEL_ID)\n",
        "print('SEED:', SEED)\n",
        "print('NUM_EPOCHS:', NUM_EPOCHS, 'patience:', EARLY_STOPPING_PATIENCE)\n",
        "print('LABEL_SMOOTHING:', LABEL_SMOOTHING)\n",
        "print('NUM_TTA_AUGS:', NUM_TTA_AUGS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VENDOR_CLASSES = ['apple','google','whatsapp','facebook','samsung','mozilla','messenger']\n",
        "VENDOR_TO_IDX = {v:i for i,v in enumerate(VENDOR_CLASSES)}\n",
        "IDX_TO_VENDOR = {i:v for v,i in VENDOR_TO_IDX.items()}\n",
        "\n",
        "# Label mapping from HuggingFace dataset (11 classes) to target dataset (7 classes)\n",
        "HF_TO_V11_MAPPING = {\n",
        "    'Apple': 'apple',\n",
        "    'Google': 'google', 'Gmail': 'google', 'Mozilla': 'google',\n",
        "    'Facebook': 'facebook',\n",
        "    'Samsung': 'samsung',\n",
        "    'WhatsApp': 'whatsapp',  # if exists in HF dataset\n",
        "    'Messenger': 'messenger',  # if exists in HF dataset\n",
        "    'DoCoMo': 'apple', 'JoyPixels': 'apple', 'KDDI': 'apple', 'SoftBank': 'apple',\n",
        "    'Twitter': 'google', 'Windows': 'google'\n",
        "}\n",
        "\n",
        "print('VENDOR_CLASSES:', VENDOR_CLASSES)\n",
        "print('HF_TO_V11_MAPPING:', HF_TO_V11_MAPPING)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deterministic Augmentation (Predictable TTA)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistical features (incl. original_mode)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_image_properties(image_path):\n",
        "    try:\n",
        "        img = Image.open(image_path)\n",
        "        mode_mapping = {'L':0,'LA':1,'P':2,'RGB':3,'RGBA':4}\n",
        "        original_mode = float(mode_mapping.get(img.mode, 3))\n",
        "        # Normalize image to RGB for pixel stats\n",
        "        if img.mode == 'P':\n",
        "            img = img.convert('RGBA')\n",
        "        if img.mode == 'RGBA':\n",
        "            bg = Image.new('RGB', img.size, (255,255,255))\n",
        "            bg.paste(img, mask=img.split()[3])\n",
        "            img = bg\n",
        "        elif img.mode != 'RGB':\n",
        "            img = img.convert('RGB')\n",
        "        w,h = img.size\n",
        "        ar = w / h if h else 0.0\n",
        "        pix = float(w*h)\n",
        "        arr = np.array(img)\n",
        "        mean_r = float(arr[:,:,0].mean()); mean_g = float(arr[:,:,1].mean()); mean_b = float(arr[:,:,2].mean())\n",
        "        std_r = float(arr[:,:,0].std());  std_g  = float(arr[:,:,1].std());  std_b  = float(arr[:,:,2].std())\n",
        "        brightness = float((mean_r+mean_g+mean_b)/3.0)\n",
        "        is_mostly_white = float(brightness > 200)\n",
        "        return {\n",
        "            'width': float(w), 'height': float(h), 'aspect_ratio': float(ar), 'pixel_count': pix,\n",
        "            'mean_r': mean_r, 'mean_g': mean_g, 'mean_b': mean_b,\n",
        "            'std_r': std_r, 'std_g': std_g, 'std_b': std_b,\n",
        "            'brightness': brightness, 'is_mostly_white': is_mostly_white,\n",
        "            'original_mode': original_mode\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'width':224.0,'height':224.0,'aspect_ratio':1.0,'pixel_count':50176.0,\n",
        "            'mean_r':128.0,'mean_g':128.0,'mean_b':128.0,\n",
        "            'std_r':50.0,'std_g':50.0,'std_b':50.0,\n",
        "            'brightness':128.0,'is_mostly_white':0.0,'original_mode':3.0\n",
        "        }\n",
        "\n",
        "STAT_COLS = ['width','height','aspect_ratio','pixel_count','mean_r','mean_g','mean_b','std_r','std_g','std_b','brightness','is_mostly_white','original_mode']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DeterministicAugmentation:\n",
        "    def __init__(self, image_size=224, seed=42):\n",
        "        self.image_size = image_size\n",
        "        self.seed = seed\n",
        "        self.rotation_angles = [-10, -5, 5, 10]\n",
        "        self.crop_ratios = [0.75, 0.85, 0.9, 0.95]\n",
        "        self.color_jitter_params = {'brightness':0.3,'contrast':0.3,'saturation':0.3,'hue':0.1}\n",
        "        self.translate_range = (0.1, 0.1)\n",
        "        self.blur_sigma = (0.1, 0.5)\n",
        "\n",
        "    def _get_deterministic_seed(self, image_or_hash):\n",
        "        if isinstance(image_or_hash, Image.Image):\n",
        "            img_bytes = image_or_hash.tobytes()\n",
        "            return int(hashlib.md5(img_bytes).hexdigest()[:8], 16)\n",
        "        return hash(str(image_or_hash)) & 0xFFFFFFFF\n",
        "\n",
        "    def horizontal_flip(self, image):\n",
        "        return F.hflip(image)\n",
        "\n",
        "    def rotation(self, image, angle):\n",
        "        return F.rotate(image, angle)\n",
        "\n",
        "    def center_crop(self, image, crop_ratio=0.9):\n",
        "        w,h = image.size\n",
        "        crop = int(min(w,h)*crop_ratio)\n",
        "        return F.center_crop(image, [crop,crop])\n",
        "\n",
        "    def corner_crop(self, image, crop_ratio=0.9, position='tl'):\n",
        "        w,h = image.size\n",
        "        crop = int(min(w,h)*crop_ratio)\n",
        "        if position=='tl': return F.crop(image, 0, 0, crop, crop)\n",
        "        if position=='tr': return F.crop(image, 0, w-crop, crop, crop)\n",
        "        if position=='bl': return F.crop(image, h-crop, 0, crop, crop)\n",
        "        if position=='br': return F.crop(image, h-crop, w-crop, crop, crop)\n",
        "        return image\n",
        "\n",
        "    def resized_crop(self, image, crop_ratio=0.85):\n",
        "        w,h = image.size\n",
        "        crop = int(min(w,h)*crop_ratio)\n",
        "        cropped = F.center_crop(image, [crop,crop])\n",
        "        return cropped.resize((self.image_size,self.image_size), Image.BILINEAR)\n",
        "\n",
        "    def color_jitter(self, image, seed_val):\n",
        "        np.random.seed(seed_val % (2**32))\n",
        "        b = 1.0 + np.random.uniform(-self.color_jitter_params['brightness'], self.color_jitter_params['brightness'])\n",
        "        c = 1.0 + np.random.uniform(-self.color_jitter_params['contrast'], self.color_jitter_params['contrast'])\n",
        "        s = 1.0 + np.random.uniform(-self.color_jitter_params['saturation'], self.color_jitter_params['saturation'])\n",
        "        h = np.random.uniform(-self.color_jitter_params['hue'], self.color_jitter_params['hue'])\n",
        "        img = F.adjust_brightness(image, b)\n",
        "        img = F.adjust_contrast(img, c)\n",
        "        img = F.adjust_saturation(img, s)\n",
        "        img = F.adjust_hue(img, h)\n",
        "        return img\n",
        "\n",
        "    def affine_transform(self, image, seed_val):\n",
        "        np.random.seed(seed_val % (2**32))\n",
        "        tx = np.random.uniform(-self.translate_range[0], self.translate_range[0])\n",
        "        ty = np.random.uniform(-self.translate_range[1], self.translate_range[1])\n",
        "        return F.affine(image, angle=0, translate=(tx*image.width, ty*image.height), scale=1.0, shear=0.0)\n",
        "\n",
        "    def gaussian_blur(self, image, seed_val):\n",
        "        np.random.seed(seed_val % (2**32))\n",
        "        sigma = np.random.uniform(self.blur_sigma[0], self.blur_sigma[1])\n",
        "        return F.gaussian_blur(image, kernel_size=3, sigma=[sigma,sigma])\n",
        "\n",
        "    def get_augmentations(self, image, num_augmentations=10, seed_source=None):\n",
        "        if seed_source is None:\n",
        "            seed_val = self._get_deterministic_seed(image)\n",
        "        elif isinstance(seed_source, str):\n",
        "            seed_val = self._get_deterministic_seed(seed_source)\n",
        "        else:\n",
        "            seed_val = int(seed_source)\n",
        "        augs = []\n",
        "        augs.append(image.resize((self.image_size,self.image_size), Image.BILINEAR))\n",
        "        augs.append(self.horizontal_flip(image).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
        "        for angle in self.rotation_angles[:max(0, min(4, num_augmentations-len(augs)))]:\n",
        "            augs.append(self.rotation(image, angle).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
        "        corners = ['tl','tr','bl','br']\n",
        "        for cpos in corners[:max(0, min(4, num_augmentations-len(augs)))]:\n",
        "            augs.append(self.corner_crop(image, 0.9, cpos).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
        "        if len(augs) < num_augmentations:\n",
        "            augs.append(self.center_crop(image, 0.9).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
        "        if len(augs) < num_augmentations:\n",
        "            augs.append(self.resized_crop(image, 0.85))\n",
        "        if len(augs) < num_augmentations:\n",
        "            augs.append(self.color_jitter(image, seed_val).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
        "        if len(augs) < num_augmentations:\n",
        "            augs.append(self.affine_transform(image, seed_val+1).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
        "        if len(augs) < num_augmentations:\n",
        "            augs.append(self.gaussian_blur(image, seed_val+2).resize((self.image_size,self.image_size), Image.BILINEAR))\n",
        "        return augs[:num_augmentations]\n",
        "    \n",
        "    def apply_training_augmentation(self, image, seed_source=None):\n",
        "        if seed_source is None:\n",
        "            seed_val = self._get_deterministic_seed(image)\n",
        "        elif isinstance(seed_source, str):\n",
        "            seed_val = self._get_deterministic_seed(seed_source)\n",
        "        else:\n",
        "            seed_val = int(seed_source)\n",
        "        \n",
        "        np.random.seed(seed_val % (2**32))\n",
        "        \n",
        "        if (seed_val % 2 == 0):\n",
        "            image = self.horizontal_flip(image)\n",
        "        \n",
        "        angle_idx = (seed_val // 2) % len(self.rotation_angles)\n",
        "        angle = self.rotation_angles[angle_idx]\n",
        "        image = self.rotation(image, angle)\n",
        "        \n",
        "        crop_idx = (seed_val // 10) % len(self.crop_ratios)\n",
        "        crop_ratio = self.crop_ratios[crop_idx]\n",
        "        w, h = image.size\n",
        "        crop_size = int(min(w, h) * crop_ratio)\n",
        "        image = F.center_crop(image, [crop_size, crop_size])\n",
        "        \n",
        "        image = self.color_jitter(image, seed_val)\n",
        "        \n",
        "        if (seed_val // 3) % 2 == 0:\n",
        "            image = self.affine_transform(image, seed_val + 1)\n",
        "        \n",
        "        if (seed_val // 5) % 5 == 0:\n",
        "            image = self.gaussian_blur(image, seed_val + 2)\n",
        "        \n",
        "        image = image.resize((self.image_size, self.image_size), Image.BILINEAR)\n",
        "        \n",
        "        return image\n",
        "\n",
        "tta_aug = DeterministicAugmentation(image_size=224, seed=42)\n",
        "print('Deterministic augmentation ready.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HuggingFace Dataset Loading (with label mapping)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_hf_dataset_with_mapping(dataset_path):\n",
        "    \"\"\"\n",
        "    Prepare HuggingFace dataset by finding all images and mapping vendor labels.\n",
        "    Maps 11 HF classes to 7 target classes using HF_TO_V11_MAPPING.\n",
        "    \"\"\"\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    dataset_path = Path(dataset_path)\n",
        "    image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
        "    \n",
        "    # HF dataset has vendor folders (Apple, Google, Facebook, etc.)\n",
        "    # Scan each vendor folder and map to target classes\n",
        "    for hf_vendor, target_vendor in HF_TO_V11_MAPPING.items():\n",
        "        if target_vendor not in VENDOR_TO_IDX:\n",
        "            continue  # Skip if target vendor not in our classes\n",
        "        \n",
        "        vendor_dir = dataset_path / hf_vendor\n",
        "        if vendor_dir.exists() and vendor_dir.is_dir():\n",
        "            for ext in image_extensions:\n",
        "                images = list(vendor_dir.glob(f\"*{ext}\"))\n",
        "                for img_path in images:\n",
        "                    image_paths.append(str(img_path))\n",
        "                    labels.append(VENDOR_TO_IDX[target_vendor])\n",
        "    \n",
        "    # Fallback: if no images found via vendor folders, try scanning all images\n",
        "    if len(image_paths) == 0:\n",
        "        for ext in image_extensions:\n",
        "            all_images = list(dataset_path.rglob(f\"*{ext}\"))\n",
        "            for img_path in all_images:\n",
        "                filename = img_path.name.lower()\n",
        "                parent_dir = img_path.parent.name\n",
        "                # Try to match vendor from filename or parent directory\n",
        "                for hf_vendor, target_vendor in HF_TO_V11_MAPPING.items():\n",
        "                    if target_vendor not in VENDOR_TO_IDX:\n",
        "                        continue\n",
        "                    if hf_vendor.lower() in filename or hf_vendor.lower() in parent_dir.lower():\n",
        "                        image_paths.append(str(img_path))\n",
        "                        labels.append(VENDOR_TO_IDX[target_vendor])\n",
        "                        break\n",
        "    \n",
        "    print(f'Loaded {len(image_paths)} images from HuggingFace dataset')\n",
        "    if len(labels) > 0:\n",
        "        label_counts = np.bincount(np.array(labels), minlength=len(VENDOR_CLASSES))\n",
        "        print(f'Label distribution: {label_counts}')\n",
        "        print(f'Label names: {[VENDOR_CLASSES[i] for i in range(len(VENDOR_CLASSES))]}')\n",
        "    \n",
        "    return image_paths, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data loading (CSV labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dataset_from_csv(train_dir, csv_path):\n",
        "    train_dir = Path(train_dir); csv_path = Path(csv_path)\n",
        "    if not train_dir.exists() or not csv_path.exists():\n",
        "        raise FileNotFoundError(f'Missing train_dir or csv: {train_dir} / {csv_path}')\n",
        "    df = pd.read_csv(csv_path)\n",
        "    label_map = {v: VENDOR_TO_IDX[v] for v in VENDOR_CLASSES}\n",
        "    img_paths=[]; labels=[]\n",
        "    missing=0; unmapped=0\n",
        "    for _, r in df.iterrows():\n",
        "        img_id = str(r['Id']).zfill(5)\n",
        "        lab = str(r['Label']).lower()\n",
        "        if lab not in label_map:\n",
        "            unmapped += 1\n",
        "            continue\n",
        "        found = None\n",
        "        for ext in ('.png','.jpg','.jpeg','.PNG','.JPG','.JPEG'):\n",
        "            p = train_dir / f'{img_id}{ext}'\n",
        "            if p.exists():\n",
        "                found = str(p)\n",
        "                break\n",
        "        if found is None:\n",
        "            missing += 1\n",
        "            continue\n",
        "        img_paths.append(found)\n",
        "        labels.append(int(label_map[lab]))\n",
        "    print('Loaded:', len(img_paths), 'images')\n",
        "    print('Unmapped labels skipped:', unmapped, 'Missing files skipped:', missing)\n",
        "    if labels:\n",
        "        print('Label distribution:', np.bincount(np.array(labels), minlength=len(VENDOR_CLASSES)))\n",
        "    return img_paths, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split (stratified)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_image_rgb(path):\n",
        "    img = Image.open(path)\n",
        "    if img.mode == 'P':\n",
        "        img = img.convert('RGBA')\n",
        "    if img.mode == 'RGBA':\n",
        "        bg = Image.new('RGB', img.size, (255,255,255))\n",
        "        bg.paste(img, mask=img.split()[3])\n",
        "        img = bg\n",
        "    elif img.mode != 'RGB':\n",
        "        img = img.convert('RGB')\n",
        "    return img\n",
        "\n",
        "class EmojiDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, processor, use_augmentation=False):\n",
        "        self.image_paths = list(image_paths)\n",
        "        self.labels = list(labels)\n",
        "        self.processor = processor\n",
        "        self.use_augmentation = use_augmentation\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    def __getitem__(self, idx):\n",
        "        p = self.image_paths[idx]\n",
        "        y = int(self.labels[idx])\n",
        "        img = load_image_rgb(p)\n",
        "        \n",
        "        # Apply training augmentation if enabled (deterministic based on image path)\n",
        "        if self.use_augmentation:\n",
        "            img = tta_aug.apply_training_augmentation(img, seed_source=str(p))\n",
        "        \n",
        "        inputs = self.processor(img, return_tensors='pt')\n",
        "        pixel_values = inputs['pixel_values'].squeeze(0)\n",
        "        y = int(max(0, min(y, len(VENDOR_CLASSES)-1)))\n",
        "        return {'pixel_values': pixel_values, 'labels': torch.tensor(y, dtype=torch.long)}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model wrapper (DINOv2 and ConvNeXtV2 backbones → 7 classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ConvNeXtV2ForEmojiClassification(nn.Module):\n",
        "    def __init__(self, base_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.num_labels = num_labels\n",
        "        # ConvNeXtV2 hidden size is in config.hidden_sizes\n",
        "        hidden = getattr(getattr(base_model, 'config', None), 'hidden_sizes', [1024])[-1]\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(hidden, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        # ConvNeXtV2 backbone feature map\n",
        "        out = self.base_model.convnextv2(pixel_values)\n",
        "        feats = out.last_hidden_state\n",
        "        if len(feats.shape) == 4:\n",
        "            pooled = feats.mean(dim=[2,3])\n",
        "        else:\n",
        "            pooled = feats\n",
        "        logits = self.classifier(pooled)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = torch.clamp(labels, 0, self.num_labels-1)\n",
        "            loss = nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        return ImageClassifierOutput(loss=loss, logits=logits)\n",
        "\n",
        "class DINOv2ForEmojiClassification(nn.Module):\n",
        "    def __init__(self, base_model, num_labels):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.num_labels = num_labels\n",
        "        hidden = getattr(base_model.config, 'hidden_size', 1024)\n",
        "        \n",
        "        # Simpler head: less capacity, less overfitting risk\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(hidden),\n",
        "            nn.Dropout(0.1),  # Lower dropout\n",
        "            nn.Linear(hidden, num_labels)  # Direct projection\n",
        "        )\n",
        "    \n",
        "    def forward(self, pixel_values, labels=None):\n",
        "        out = self.base_model(pixel_values=pixel_values, output_hidden_states=True)\n",
        "        if hasattr(out, 'pooler_output') and out.pooler_output is not None:\n",
        "            pooled = out.pooler_output\n",
        "        else:\n",
        "            pooled = out.hidden_states[-1][:,0,:]\n",
        "        logits = self.classifier(pooled)\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            labels = torch.clamp(labels, 0, self.num_labels-1)\n",
        "            loss = nn.CrossEntropyLoss(label_smoothing=0.1)(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        return ImageClassifierOutput(loss=loss, logits=logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train / Validate loops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def seed_everything(seed):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, scaler=None):\n",
        "    model.train()\n",
        "    total_loss=0.0; correct=0; total=0\n",
        "    use_amp = (device.type=='cuda')\n",
        "    for batch in tqdm(loader, desc='Training', mininterval=TQDM_MININTERVAL):\n",
        "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
        "        y = batch['labels'].to(device, non_blocking=True)\n",
        "        y = torch.clamp(y, 0, model.num_labels-1)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "            out = model(pixel_values=x, labels=y)\n",
        "            loss = out.loss\n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        total_loss += float(loss.item())\n",
        "        pred = torch.argmax(out.logits, dim=1)\n",
        "        correct += int((pred==y).sum().item())\n",
        "        total += int(y.size(0))\n",
        "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    total_loss=0.0; correct=0; total=0\n",
        "    preds=[]; labels=[]\n",
        "    use_amp = (device.type=='cuda')\n",
        "    for batch in tqdm(loader, desc='Validation', mininterval=TQDM_MININTERVAL):\n",
        "        x = batch['pixel_values'].to(device, non_blocking=True)\n",
        "        y = batch['labels'].to(device, non_blocking=True)\n",
        "        y = torch.clamp(y, 0, model.num_labels-1)\n",
        "        with torch.amp.autocast('cuda', enabled=use_amp):\n",
        "            out = model(pixel_values=x, labels=y)\n",
        "            loss = out.loss\n",
        "        total_loss += float(loss.item())\n",
        "        pred = torch.argmax(out.logits, dim=1)\n",
        "        pred = torch.clamp(pred, 0, model.num_labels-1)\n",
        "        correct += int((pred==y).sum().item())\n",
        "        total += int(y.size(0))\n",
        "        preds.extend(pred.cpu().numpy().tolist())\n",
        "        labels.extend(y.cpu().numpy().tolist())\n",
        "    return total_loss/max(1,len(loader)), 100.0*correct/max(1,total), preds, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pre-training on HuggingFace Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pretrain_on_hf_dataset(model_kind, model_id, seed):\n",
        "    \"\"\"\n",
        "    Pre-train model on HuggingFace dataset with label mapping.\n",
        "    Returns model, processor, checkpoint_path, best_acc\n",
        "    \"\"\"\n",
        "    seed_everything(seed)\n",
        "    \n",
        "    # Download and load HuggingFace dataset\n",
        "    print(f'\\n=== Pre-training {model_kind.upper()} on HuggingFace dataset ===')\n",
        "    hf_path = kagglehub.dataset_download(HF_DATASET_ID)\n",
        "    print(f'HuggingFace dataset path: {hf_path}')\n",
        "    \n",
        "    hf_paths, hf_labels = prepare_hf_dataset_with_mapping(hf_path)\n",
        "    \n",
        "    if len(hf_paths) == 0:\n",
        "        raise ValueError('No images found in HuggingFace dataset')\n",
        "    \n",
        "    # Split HF dataset into train/val\n",
        "    labels_arr = np.array(hf_labels)\n",
        "    min_count = np.bincount(labels_arr, minlength=len(VENDOR_CLASSES)).min() if len(labels_arr) else 0\n",
        "    can_stratify = (min_count >= 2)\n",
        "    \n",
        "    hf_train_paths, hf_val_paths, hf_train_y, hf_val_y = train_test_split(\n",
        "        list(hf_paths), list(hf_labels),\n",
        "        test_size=VAL_SIZE, random_state=seed,\n",
        "        stratify=list(hf_labels) if can_stratify else None\n",
        "    )\n",
        "    \n",
        "    print(f'HF Train: {len(hf_train_paths)}, HF Val: {len(hf_val_paths)}')\n",
        "    \n",
        "    # Load model and processor\n",
        "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "    backbone = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
        "    if model_kind == 'dino':\n",
        "        model = DINOv2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
        "    elif model_kind == 'cnn':\n",
        "        model = ConvNeXtV2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
        "    else:\n",
        "        raise ValueError('Unknown model_kind: ' + str(model_kind))\n",
        "    \n",
        "    # Create datasets and loaders\n",
        "    hf_train_ds = EmojiDataset(hf_train_paths, hf_train_y, processor, use_augmentation=True)\n",
        "    hf_val_ds = EmojiDataset(hf_val_paths, hf_val_y, processor, use_augmentation=False)\n",
        "    bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
        "    hf_train_loader = DataLoader(hf_train_ds, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
        "    hf_val_loader = DataLoader(hf_val_ds, batch_size=bs, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
        "    \n",
        "    # Training setup\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, min_lr=1e-7, cooldown=1)\n",
        "    scaler = None\n",
        "    if torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()):\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "    \n",
        "    # Training loop\n",
        "    best_acc = -1.0\n",
        "    best_path = f'pretrained_{model_kind}_hf.pt'\n",
        "    bad = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f'\\n[HF Pre-train {model_kind}] epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "        tr_loss, tr_acc = train_epoch(model, hf_train_loader, optimizer, device, scaler)\n",
        "        va_loss, va_acc, va_pred, va_true = validate(model, hf_val_loader, device)\n",
        "        scheduler.step(va_acc)\n",
        "        print(f'Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% | Val: loss={va_loss:.4f} acc={va_acc:.2f}%')\n",
        "        if va_acc > best_acc + 1e-6:\n",
        "            best_acc = va_acc\n",
        "            bad = 0\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print('✓ saved', best_path)\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= EARLY_STOPPING_PATIENCE:\n",
        "                print('Early stopping: no improvement for', EARLY_STOPPING_PATIENCE, 'epochs')\n",
        "                break\n",
        "    \n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    print(f'✓ Pre-training completed! Best HF validation accuracy: {best_acc:.2f}%')\n",
        "    return model, processor, best_path, best_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuning on Target Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def finetune_model(model_kind, model_id, seed, stage_tag, train_paths_s, train_y_s, val_paths_s, val_y_s, pretrained_checkpoint=None):\n",
        "    \"\"\"\n",
        "    Fine-tune model on target dataset.\n",
        "    If pretrained_checkpoint is provided, loads weights from pre-training.\n",
        "    \"\"\"\n",
        "    seed_everything(seed)\n",
        "    processor = AutoImageProcessor.from_pretrained(model_id)\n",
        "    backbone = AutoModelForImageClassification.from_pretrained(model_id).to(device)\n",
        "    if model_kind == 'dino':\n",
        "        model = DINOv2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
        "    elif model_kind == 'cnn':\n",
        "        model = ConvNeXtV2ForEmojiClassification(backbone, num_labels=len(VENDOR_CLASSES)).to(device)\n",
        "    else:\n",
        "        raise ValueError('Unknown model_kind: ' + str(model_kind))\n",
        "    \n",
        "    # Load pre-trained weights if provided\n",
        "    if pretrained_checkpoint is not None and os.path.exists(pretrained_checkpoint):\n",
        "        model.load_state_dict(torch.load(pretrained_checkpoint, map_location=device))\n",
        "        print(f'✓ Loaded pre-trained weights from {pretrained_checkpoint}')\n",
        "    else:\n",
        "        print('Starting from scratch (no pre-trained checkpoint)')\n",
        "\n",
        "    train_ds = EmojiDataset(train_paths_s, train_y_s, processor, use_augmentation=True)\n",
        "    val_ds = EmojiDataset(val_paths_s, val_y_s, processor, use_augmentation=False)\n",
        "    bs = BATCH_SIZE_CUDA if torch.cuda.is_available() else BATCH_SIZE_CPU\n",
        "    train_loader = DataLoader(train_ds, batch_size=bs, shuffle=True, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
        "    val_loader = DataLoader(val_ds, batch_size=bs, shuffle=False, num_workers=NUM_WORKERS, pin_memory=torch.cuda.is_available())\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, min_lr=1e-7, cooldown=1)\n",
        "    scaler = None\n",
        "    if torch.cuda.is_available() and (not torch.cuda.is_bf16_supported()):\n",
        "        scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    best_acc = -1.0\n",
        "    best_path = f'best_{stage_tag}.pt'\n",
        "    bad = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        print(f'\\n[{stage_tag}] epoch {epoch+1}/{NUM_EPOCHS}')\n",
        "        tr_loss, tr_acc = train_epoch(model, train_loader, optimizer, device, scaler)\n",
        "        va_loss, va_acc, va_pred, va_true = validate(model, val_loader, device)\n",
        "        scheduler.step(va_acc)\n",
        "        print(f'Train: loss={tr_loss:.4f} acc={tr_acc:.2f}% | Val: loss={va_loss:.4f} acc={va_acc:.2f}%')\n",
        "        if va_acc > best_acc + 1e-6:\n",
        "            best_acc = va_acc\n",
        "            bad = 0\n",
        "            torch.save(model.state_dict(), best_path)\n",
        "            print('✓ saved', best_path)\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= EARLY_STOPPING_PATIENCE:\n",
        "                print('Early stopping: no improvement for', EARLY_STOPPING_PATIENCE, 'epochs')\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "    return model, processor, best_path, best_acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Target Dataset and Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load target dataset\n",
        "all_paths, all_labels = prepare_dataset_from_csv(SECOND_DATASET_TRAIN_DIR, SECOND_DATASET_CSV_PATH)\n",
        "\n",
        "# Split target dataset\n",
        "min_count = np.bincount(np.array(all_labels), minlength=len(VENDOR_CLASSES)).min()\n",
        "can_stratify = (min_count >= 2)\n",
        "print('Min class count:', int(min_count), 'Stratify:', can_stratify)\n",
        "train_paths, val_paths, train_y, val_y = train_test_split(\n",
        "    all_paths, all_labels, test_size=VAL_SIZE, random_state=RANDOM_STATE,\n",
        "    stratify=all_labels if can_stratify else None\n",
        ")\n",
        "print('Train:', len(train_paths), 'Val:', len(val_paths))\n",
        "print('Train dist:', np.bincount(np.array(train_y), minlength=len(VENDOR_CLASSES)))\n",
        "print('Val   dist:', np.bincount(np.array(val_y), minlength=len(VENDOR_CLASSES)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Pipeline: Pre-train + Fine-tune\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('=== Pre-training on HuggingFace dataset ===')\n",
        "\n",
        "# Pre-train DINOv2 on HuggingFace dataset\n",
        "dino_pretrained, dino_proc, dino_hf_ckpt, dino_hf_best = pretrain_on_hf_dataset('dino', DINO_MODEL_ID, SEED)\n",
        "\n",
        "# Pre-train ConvNeXtV2 on HuggingFace dataset\n",
        "cnn_pretrained, cnn_proc, cnn_hf_ckpt, cnn_hf_best = pretrain_on_hf_dataset('cnn', CNN_MODEL_ID, SEED)\n",
        "\n",
        "# Move pre-trained models to CPU to save memory\n",
        "if device.type == 'cuda':\n",
        "    dino_pretrained = dino_pretrained.to('cpu')\n",
        "    cnn_pretrained = cnn_pretrained.to('cpu')\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print('\\n=== Fine-tuning on target dataset ===')\n",
        "\n",
        "# Fine-tune DINOv2 on target dataset\n",
        "dino_final, dino_proc, dino_ckpt, dino_best = finetune_model(\n",
        "    'dino', DINO_MODEL_ID, SEED, 'dino_finetuned',\n",
        "    train_paths, train_y, val_paths, val_y,\n",
        "    pretrained_checkpoint=dino_hf_ckpt\n",
        ")\n",
        "\n",
        "# Fine-tune ConvNeXtV2 on target dataset\n",
        "cnn_final, cnn_proc, cnn_ckpt, cnn_best = finetune_model(\n",
        "    'cnn', CNN_MODEL_ID, SEED, 'cnn_finetuned',\n",
        "    train_paths, train_y, val_paths, val_y,\n",
        "    pretrained_checkpoint=cnn_hf_ckpt\n",
        ")\n",
        "\n",
        "# Keep models on CPU when not actively used\n",
        "if device.type == 'cuda':\n",
        "    dino_final = dino_final.to('cpu')\n",
        "    cnn_final = cnn_final.to('cpu')\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "trained_members = [\n",
        "    (dino_final, dino_proc, 'dino'),\n",
        "    (cnn_final,  cnn_proc,  'cnn'),\n",
        "]\n",
        "print('✓ Trained members:', [t for _,_,t in trained_members])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature matrix: stats + (A×C) prob vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _prob_cols_for_members(members, num_augmentations):\n",
        "    cols = []\n",
        "    for _, _, tag in members:\n",
        "        for i in range(num_augmentations):\n",
        "            for c in range(len(VENDOR_CLASSES)):\n",
        "                cols.append(f'prob_{tag}_aug{i}_cls{c}')\n",
        "    return cols\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_features_batched(image_paths, members, num_augmentations, batch_size=None):\n",
        "    \"\"\"\n",
        "    Batched feature extraction optimized for GPU efficiency.\n",
        "    Processes all batches for each model before switching to the next model.\n",
        "    For each image: stats + per-model (A×C) probabilities.\n",
        "    \"\"\"\n",
        "    if batch_size is None:\n",
        "        batch_size = FEATURE_BATCH_SIZE\n",
        "\n",
        "    num_batches = (len(image_paths) + batch_size - 1) // batch_size\n",
        "    \n",
        "    # Step 1: Extract image stats for all images (model-independent)\n",
        "    all_stats = []\n",
        "    all_images = []\n",
        "    for start in tqdm(range(0, len(image_paths), batch_size),\n",
        "                      desc='Extracting image stats',\n",
        "                      mininterval=TQDM_MININTERVAL,\n",
        "                      disable=(not SHOW_PROGRESS),\n",
        "                      total=num_batches):\n",
        "        batch_paths = image_paths[start:start+batch_size]\n",
        "        batch_stats = [extract_image_properties(p) for p in batch_paths]\n",
        "        batch_imgs = [load_image_rgb(p) for p in batch_paths]\n",
        "        all_stats.extend(batch_stats)\n",
        "        all_images.extend(batch_imgs)\n",
        "    \n",
        "    # Step 2: Process all batches for each model (minimize GPU model loading/unloading)\n",
        "    # Structure: model_probs[tag][batch_idx][aug_idx][class_idx]\n",
        "    model_probs = {}  # tag -> list of (B, A, C) arrays, one per batch\n",
        "    \n",
        "    for m, proc, tag in tqdm(members, desc='Processing models', disable=(not SHOW_PROGRESS)):\n",
        "        # Load model to GPU once for all batches\n",
        "        m.to(device)\n",
        "        m.eval()\n",
        "        \n",
        "        model_batch_probs = []  # Store probabilities for all batches of this model\n",
        "        \n",
        "        for start in tqdm(range(0, len(image_paths), batch_size),\n",
        "                          desc=f'  {tag} batches',\n",
        "                          mininterval=TQDM_MININTERVAL,\n",
        "                          disable=(not SHOW_PROGRESS),\n",
        "                          total=num_batches,\n",
        "                          leave=False):\n",
        "            batch_paths = image_paths[start:start+batch_size]\n",
        "            B = len(batch_paths)\n",
        "            batch_start_idx = start\n",
        "            batch_end_idx = min(start + batch_size, len(image_paths))\n",
        "            batch_imgs = all_images[batch_start_idx:batch_end_idx]\n",
        "            \n",
        "            # Generate augmentations for this batch\n",
        "            batch_augs = []\n",
        "            for img in batch_imgs:\n",
        "                batch_augs.extend(tta_aug.get_augmentations(img, num_augmentations=num_augmentations))\n",
        "            \n",
        "            # Process batch\n",
        "            inputs = proc(batch_augs, return_tensors='pt')\n",
        "            x = inputs['pixel_values'].to(device)\n",
        "            out = m(pixel_values=x)\n",
        "            probs = torch.softmax(out.logits, dim=-1)\n",
        "            probs = probs.view(B, num_augmentations, -1).detach().cpu().numpy()\n",
        "            model_batch_probs.append(probs)\n",
        "        \n",
        "        model_probs[tag] = model_batch_probs\n",
        "        \n",
        "        # Move model back to CPU after processing all batches\n",
        "        if device.type == 'cuda':\n",
        "            m.to('cpu')\n",
        "            torch.cuda.empty_cache()\n",
        "    \n",
        "    # Step 3: Combine stats and probabilities into final feature matrix\n",
        "    all_rows = []\n",
        "    for img_idx, stats in enumerate(all_stats):\n",
        "        row = dict(stats)\n",
        "        \n",
        "        # Find which batch this image belongs to\n",
        "        batch_idx = img_idx // batch_size\n",
        "        batch_local_idx = img_idx % batch_size\n",
        "        \n",
        "        # Extract probabilities for this image from all models\n",
        "        for tag, batch_probs_list in model_probs.items():\n",
        "            batch_probs = batch_probs_list[batch_idx]  # (B, A, C)\n",
        "            for a in range(num_augmentations):\n",
        "                for c in range(len(VENDOR_CLASSES)):\n",
        "                    row[f'prob_{tag}_aug{a}_cls{c}'] = float(batch_probs[batch_local_idx, a, c])\n",
        "        \n",
        "        all_rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(all_rows)\n",
        "    prob_cols = _prob_cols_for_members(members, num_augmentations)\n",
        "    all_cols = STAT_COLS + prob_cols\n",
        "    for col in all_cols:\n",
        "        if col not in df.columns:\n",
        "            df[col] = 0.0\n",
        "    return df[all_cols]\n",
        "\n",
        "print('Total features:', len(STAT_COLS) + len(_prob_cols_for_members([('','','dino'),('','','cnn')], NUM_TTA_AUGS)))\n",
        "print('Using batched feature extraction with batch_size:', FEATURE_BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the trained_members list from the training pipeline\n",
        "members = trained_members\n",
        "print('Members for features:', [t for _,_,t in members])\n",
        "\n",
        "X_train = build_features_batched(train_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
        "X_val = build_features_batched(val_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
        "y_train = np.array(train_y)\n",
        "y_val = np.array(val_y)\n",
        "print('X_train:', X_train.shape, 'X_val:', X_val.shape)\n",
        "\n",
        "if USE_LIGHTGBM and HAS_LGB:\n",
        "    meta = lgb.LGBMClassifier(**LGB_PARAMS)\n",
        "    meta.fit(X_train, y_train)\n",
        "    val_pred = meta.predict(X_val)\n",
        "    print('Meta(LGB) val acc:', accuracy_score(y_val, val_pred)*100.0)\n",
        "    print(classification_report(y_val, val_pred, target_names=VENDOR_CLASSES))\n",
        "    meta.booster_.save_model('meta_lgb_v12.txt')\n",
        "    print('Saved meta_lgb_v12.txt')\n",
        "else:\n",
        "    raise RuntimeError('LightGBM not available in this environment. Please install lightgbm or switch USE_LIGHTGBM=False')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final fit on Train+Val and predict Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "members = trained_members\n",
        "combined_paths = train_paths + val_paths\n",
        "combined_y = train_y + val_y\n",
        "\n",
        "X_all = build_features_batched(combined_paths, members, NUM_TTA_AUGS, batch_size=FEATURE_BATCH_SIZE)\n",
        "y_all = np.array(combined_y)\n",
        "\n",
        "if not (USE_LIGHTGBM and HAS_LGB):\n",
        "    raise RuntimeError('LightGBM not available for final fit')\n",
        "meta_final = lgb.LGBMClassifier(**LGB_PARAMS)\n",
        "meta_final.fit(X_all, y_all)\n",
        "meta_final.booster_.save_model('meta_lgb_v12_final.txt')\n",
        "print('Saved meta_lgb_v12_final.txt')\n",
        "\n",
        "test_dir = SECOND_DATASET_TEST_DIR\n",
        "if not test_dir.exists():\n",
        "    raise FileNotFoundError(f'Missing test dir: {test_dir}')\n",
        "test_paths = []\n",
        "for ext in ('.png','.jpg','.jpeg','.PNG','.JPG','.JPEG'):\n",
        "    test_paths += [str(p) for p in test_dir.rglob(f'*{ext}')]\n",
        "test_paths = sorted(set(test_paths))\n",
        "print('Found test images:', len(test_paths))\n",
        "\n",
        "pred_ids = []\n",
        "pred_labels = []\n",
        "test_batch_size = FEATURE_BATCH_SIZE\n",
        "num_test_batches = (len(test_paths) + test_batch_size - 1) // test_batch_size\n",
        "for start in tqdm(range(0, len(test_paths), test_batch_size), desc='Predicting test (batched)', mininterval=TQDM_MININTERVAL, disable=(not SHOW_PROGRESS), total=num_test_batches):\n",
        "    batch_paths = test_paths[start:start+test_batch_size]\n",
        "    batch_ids = [Path(p).stem for p in batch_paths]\n",
        "    Xp_batch = build_features_batched(batch_paths, members, NUM_TTA_AUGS, batch_size=test_batch_size)\n",
        "    preds = meta_final.predict(Xp_batch)\n",
        "    for img_id, pred in zip(batch_ids, preds):\n",
        "        pred = int(pred)\n",
        "        pred = max(0, min(pred, len(VENDOR_CLASSES)-1))\n",
        "        pred_ids.append(img_id)\n",
        "        pred_labels.append(IDX_TO_VENDOR[pred])\n",
        "\n",
        "out_path = Path(PREDICTIONS_OUTPUT_FILE)\n",
        "with out_path.open('w') as f:\n",
        "    f.write('Id,Label\\n')\n",
        "    for i,l in zip(pred_ids, pred_labels):\n",
        "        f.write(f'{str(i).strip()},{l}\\n')\n",
        "print('Wrote:', out_path, 'rows:', len(pred_labels))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
