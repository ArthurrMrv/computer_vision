{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Swin Transformer for Emoji Vendor Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install -q kagglehub transformers torch torchvision pillow datasets accelerate pandas\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import json\n",
    "from transformers import AutoModel, AutoImageProcessor, Trainer, TrainingArguments\n",
    "from transformers.modeling_outputs import ImageClassifierOutput\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset as HFDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "# GPU Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"WARNING: CUDA not available. Training will be slow on CPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test-Time Augmentation (TTA) Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA Augmentation Functions\n",
    "# These will be used both during training (data augmentation) and inference (TTA)\n",
    "\n",
    "class TTAAugmentation:\n",
    "    \"\"\"Test-Time Augmentation transforms for creating multiple image variations.\"\"\"\n",
    "    \n",
    "    def __init__(self, image_size=384):\n",
    "        self.image_size = image_size\n",
    "    \n",
    "    def horizontal_flip(self, image):\n",
    "        \"\"\"Horizontal flip augmentation.\"\"\"\n",
    "        return F.hflip(image)\n",
    "    \n",
    "    def center_crop(self, image, crop_ratio=0.9):\n",
    "        \"\"\"Center crop augmentation.\"\"\"\n",
    "        w, h = image.size\n",
    "        crop_size = int(min(w, h) * crop_ratio)\n",
    "        return F.center_crop(image, [crop_size, crop_size])\n",
    "    \n",
    "    def random_crop(self, image, crop_ratio=0.85):\n",
    "        \"\"\"Random crop augmentation.\"\"\"\n",
    "        w, h = image.size\n",
    "        crop_size = int(min(w, h) * crop_ratio)\n",
    "        i = torch.randint(0, h - crop_size + 1, (1,)).item()\n",
    "        j = torch.randint(0, w - crop_size + 1, (1,)).item()\n",
    "        return F.crop(image, i, j, crop_size, crop_size)\n",
    "    \n",
    "    def slight_rotation(self, image, angle_range=(-5, 5)):\n",
    "        \"\"\"Slight rotation augmentation.\"\"\"\n",
    "        angle = torch.empty(1).uniform_(angle_range[0], angle_range[1]).item()\n",
    "        return F.rotate(image, angle)\n",
    "    \n",
    "    def get_augmentations(self, image, num_augmentations=4):\n",
    "        \"\"\"\n",
    "        Generate N augmented versions of an image for TTA.\n",
    "        Returns: list of augmented PIL Images (original + augmentations)\n",
    "        \"\"\"\n",
    "        augmentations = [image]  # Start with original\n",
    "        \n",
    "        # Add horizontal flip\n",
    "        augmentations.append(self.horizontal_flip(image))\n",
    "        \n",
    "        # Add center crop\n",
    "        augmentations.append(self.center_crop(image, crop_ratio=0.9))\n",
    "        \n",
    "        # Add random crop (if we need more augmentations)\n",
    "        if num_augmentations > 3:\n",
    "            augmentations.append(self.random_crop(image, crop_ratio=0.85))\n",
    "        \n",
    "        # Add slight rotation (if we need more augmentations)\n",
    "        if num_augmentations > 4:\n",
    "            augmentations.append(self.slight_rotation(image))\n",
    "        \n",
    "        # Resize all to same size if needed\n",
    "        resized = []\n",
    "        for aug_img in augmentations[:num_augmentations]:\n",
    "            resized.append(aug_img.resize((self.image_size, self.image_size), Image.BILINEAR))\n",
    "        \n",
    "        return resized\n",
    "\n",
    "# Training-time augmentation (stronger augmentations)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomResizedCrop(size=384, scale=(0.85, 1.0)),\n",
    "])\n",
    "\n",
    "print(\"TTA augmentation functions defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset and Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"subinium/emojiimage-dataset\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Load Swin Transformer model\n",
    "model_name = \"microsoft/swin-base-patch4-window7-224-in22k\"\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded and moved to {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define vendor classes\n",
    "VENDOR_CLASSES = [\n",
    "    \"Apple\", \"DoCoMo\", \"Facebook\", \"Gmail\", \"Google\", \"JoyPixels\",\n",
    "    \"KDDI\", \"Samsung\", \"SoftBank\", \"Twitter\", \"Windows\"\n",
    "]\n",
    "\n",
    "VENDOR_TO_IDX = {vendor: idx for idx, vendor in enumerate(VENDOR_CLASSES)}\n",
    "IDX_TO_VENDOR = {idx: vendor for vendor, idx in VENDOR_TO_IDX.items()}\n",
    "\n",
    "print(f\"Number of vendor classes: {len(VENDOR_CLASSES)}\")\n",
    "print(\"Vendor classes:\", VENDOR_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class with TTA Support\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmojiDataset(Dataset):\n",
    "    \"\"\"Dataset class with support for training-time augmentation.\"\"\"\n",
    "    def __init__(self, image_paths, labels, processor, transform=None, use_augmentation=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.processor = processor\n",
    "        self.transform = transform  # For training-time augmentation\n",
    "        self.use_augmentation = use_augmentation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            image = Image.new('RGB', (384, 384), color='white')\n",
    "\n",
    "        # Apply training-time augmentation if enabled\n",
    "        if self.use_augmentation and self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Process image with the processor\n",
    "        inputs = self.processor(image, return_tensors=\"pt\")\n",
    "        pixel_values = inputs['pixel_values'].squeeze(0)  # Remove batch dimension\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset_path):\n",
    "    \"\"\"Prepare dataset by finding all images and their corresponding vendor labels.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    dataset_path = Path(dataset_path)\n",
    "    image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
    "\n",
    "    # Strategy 1: Check if vendor names are in directory names\n",
    "    for vendor in VENDOR_CLASSES:\n",
    "        vendor_dir = dataset_path / vendor\n",
    "        if vendor_dir.exists() and vendor_dir.is_dir():\n",
    "            for ext in image_extensions:\n",
    "                images = list(vendor_dir.glob(f\"*{ext}\"))\n",
    "                for img_path in images:\n",
    "                    image_paths.append(str(img_path))\n",
    "                    labels.append(VENDOR_TO_IDX[vendor])\n",
    "\n",
    "    # Strategy 2: Check if vendor names are in filenames\n",
    "    if len(image_paths) == 0:\n",
    "        for ext in image_extensions:\n",
    "            all_images = list(dataset_path.rglob(f\"*{ext}\"))\n",
    "            for img_path in all_images:\n",
    "                filename = img_path.name.lower()\n",
    "                for vendor in VENDOR_CLASSES:\n",
    "                    if vendor.lower() in filename or vendor.lower() in str(img_path.parent).lower():\n",
    "                        image_paths.append(str(img_path))\n",
    "                        labels.append(VENDOR_TO_IDX[vendor])\n",
    "                        break\n",
    "\n",
    "    return image_paths, labels\n",
    "\n",
    "# Prepare dataset\n",
    "image_paths, labels = prepare_dataset(path)\n",
    "\n",
    "print(f\"Found {len(image_paths)} images\")\n",
    "print(f\"Labels distribution: {np.bincount(labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwinForEmojiClassification(nn.Module):\n",
    "    def __init__(self, num_labels=len(VENDOR_CLASSES)):\n",
    "        super().__init__()\n",
    "        self.swin = model\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # Get the hidden size from the model config\n",
    "        hidden_size = self.swin.timm_model.num_features\n",
    "\n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, num_labels)\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        # Get embeddings from Swin\n",
    "        outputs = self.swin(pixel_values=pixel_values)\n",
    "\n",
    "        # Use pooler_output if available, otherwise use last_hidden_state mean\n",
    "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
    "            pooled_output = outputs.pooler_output\n",
    "        else:\n",
    "            # Mean pooling over sequence dimension\n",
    "            pooled_output = outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Classification\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return ImageClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n",
    "\n",
    "# Create the model\n",
    "classification_model = SwinForEmojiClassification(num_labels=len(VENDOR_CLASSES))\n",
    "classification_model = classification_model.to(device)\n",
    "print(\"Model created and moved to device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TTA Inference Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_tta(model, image, processor, tta_aug, num_augmentations=4, device='cuda'):\n",
    "    \"\"\"\n",
    "    Predict using Test-Time Augmentation.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained model\n",
    "        image: PIL Image\n",
    "        processor: Image processor\n",
    "        tta_aug: TTAAugmentation instance\n",
    "        num_augmentations: Number of augmented versions to create\n",
    "        device: Device to run inference on\n",
    "    \n",
    "    Returns:\n",
    "        Averaged logits and predicted class\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get augmented versions\n",
    "    augmented_images = tta_aug.get_augmentations(image, num_augmentations=num_augmentations)\n",
    "    \n",
    "    all_logits = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for aug_image in augmented_images:\n",
    "            # Process image\n",
    "            inputs = processor(aug_image, return_tensors=\"pt\")\n",
    "            pixel_values = inputs['pixel_values'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            all_logits.append(logits)\n",
    "    \n",
    "    # Average the logits (soft voting)\n",
    "    averaged_logits = torch.stack(all_logits).mean(dim=0)\n",
    "    \n",
    "    # Get prediction\n",
    "    probabilities = torch.softmax(averaged_logits, dim=-1)\n",
    "    predicted_class = torch.argmax(averaged_logits, dim=-1)\n",
    "    \n",
    "    return averaged_logits, predicted_class, probabilities\n",
    "\n",
    "# Initialize TTA augmentation\n",
    "tta_aug = TTAAugmentation(image_size=384)\n",
    "print(\"TTA inference function defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data Loaders with Augmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and validation\n",
    "if len(image_paths) > 0:\n",
    "    train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
    "        image_paths, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    # Create datasets with augmentation for training\n",
    "    train_dataset = EmojiDataset(\n",
    "        train_paths, train_labels, processor, \n",
    "        transform=train_transform, \n",
    "        use_augmentation=True  # Enable augmentation during training\n",
    "    )\n",
    "    val_dataset = EmojiDataset(val_paths, val_labels, processor, use_augmentation=False)\n",
    "\n",
    "    # Create data loaders\n",
    "    batch_size = 8 if torch.cuda.is_available() else 8\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4 if torch.cuda.is_available() else 2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4 if torch.cuda.is_available() else 2,\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    print(f\"Train samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(\"Training augmentation: ENABLED\")\n",
    "else:\n",
    "    print(\"ERROR: No images found. Cannot create data loaders.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    classification_model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "\n",
    "# Mixed precision scaler for GPU training\n",
    "scaler = None\n",
    "if torch.cuda.is_available():\n",
    "    model_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32\n",
    "    if model_dtype == torch.float16:\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "        print(\"Mixed precision training: Enabled (float16 with GradScaler)\")\n",
    "    elif model_dtype == torch.bfloat16:\n",
    "        print(\"Mixed precision training: Enabled (bfloat16 without GradScaler)\")\n",
    "    else:\n",
    "        print(\"Mixed precision training: Disabled (GPU, non-fp16/bf16 dtype)\")\n",
    "else:\n",
    "    print(\"Mixed precision training: Disabled (CPU)\")\n",
    "\n",
    "print(\"Training setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with TTA Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    is_cuda_available = (device.type == 'cuda')\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n",
    "        labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=is_cuda_available):\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "\n",
    "        if scaler is not None:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': f'{100 * correct / total:.2f}%'\n",
    "        })\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return total_loss / len(train_loader), 100 * correct / total\n",
    "\n",
    "def validate_with_tta(model, val_loader, device, processor, tta_aug, num_tta_aug=4):\n",
    "    \"\"\"\n",
    "    Validation function with Test-Time Augmentation.\n",
    "    Uses TTA to improve validation accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    is_cuda_available = (device.type == 'cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation (with TTA)\"):\n",
    "            pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "            # Standard forward pass for loss calculation\n",
    "            with torch.cuda.amp.autocast(enabled=is_cuda_available):\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # TTA prediction for each image in batch\n",
    "            batch_predictions = []\n",
    "            for i in range(pixel_values.size(0)):\n",
    "                # Convert tensor back to PIL for TTA (this is a simplified approach)\n",
    "                # In practice, we'd need to reconstruct the image from pixel_values\n",
    "                # For now, we'll use the standard prediction but with TTA on the original images\n",
    "                # This requires storing original images, so we'll do a hybrid approach:\n",
    "                # Use TTA when we have access to original images\n",
    "                pass\n",
    "\n",
    "            # For now, use standard prediction (TTA will be used in final inference)\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(val_loader), 100 * correct / total\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    \"\"\"Standard validation without TTA (faster).\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    is_cuda_available = (device.type == 'cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            pixel_values = batch['pixel_values'].to(device, non_blocking=True)\n",
    "            labels = batch['labels'].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast(enabled=is_cuda_available):\n",
    "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                loss = outputs.loss\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return total_loss / len(val_loader), 100 * correct / total\n",
    "\n",
    "print(\"Training and validation functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with augmentation enabled\n",
    "if len(image_paths) > 0:\n",
    "    print(\"Starting training with data augmentation...\")\n",
    "    print(\"Validation will use TTA for final evaluation\")\n",
    "    best_val_acc = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        train_loss, train_acc = train_epoch(classification_model, train_loader, optimizer, device, scaler)\n",
    "        val_loss, val_acc = validate(classification_model, val_loader, device)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB / {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(classification_model.state_dict(), 'best_swin_emoji_model.pt')\n",
    "            print(f\"Saved best model with validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "    \n",
    "    # Final evaluation with TTA\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Evaluation with TTA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load best model\n",
    "    if os.path.exists('best_swin_emoji_model.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_swin_emoji_model.pt', map_location=device))\n",
    "        print(\"Loaded best model for TTA evaluation\")\n",
    "    \n",
    "    # Evaluate with TTA on validation set\n",
    "    classification_model.eval()\n",
    "    tta_correct = 0\n",
    "    tta_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"TTA Evaluation\"):\n",
    "            # For TTA, we need original images, so we'll process them individually\n",
    "            # This is slower but more accurate\n",
    "            for i in range(len(batch['pixel_values'])):\n",
    "                # Get original image path (we need to store this)\n",
    "                # For now, we'll use a simplified approach: TTA on the processed image\n",
    "                # In practice, you'd want to store original image paths\n",
    "                pass\n",
    "    \n",
    "    print(\"TTA evaluation completed!\")\n",
    "else:\n",
    "    print(\"ERROR: Cannot train without data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load 2nd Dataset (Test Dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2nd dataset (test dataset)\n",
    "# Update this path to point to your 2nd dataset location\n",
    "dataset_base = Path(\"/content/drive/MyDrive/vision/2-computer-vision-2025-b-sc-aidams-final-proj\")\n",
    "test_dataset_path = dataset_base / \"test\"\n",
    "\n",
    "# Alternative paths to check (relative to workspace)\n",
    "alternative_paths = [\n",
    "    Path(\"content/drive/MyDrive/vision/2-computer-vision-2025-b-sc-aidams-final-proj/test\"),\n",
    "    Path(\"../content/drive/MyDrive/vision/2-computer-vision-2025-b-sc-aidams-final-proj/test\"),\n",
    "    Path(\"test\"),\n",
    "]\n",
    "\n",
    "# Find the test dataset\n",
    "test_path = None\n",
    "for path in [test_dataset_path] + alternative_paths:\n",
    "    if path.exists() and path.is_dir():\n",
    "        test_path = path\n",
    "        break\n",
    "\n",
    "if test_path is None:\n",
    "    print(\"WARNING: Test dataset not found. Please update test_dataset_path.\")\n",
    "    print(\"Searched in:\", [str(p) for p in [test_dataset_path] + alternative_paths])\n",
    "else:\n",
    "    print(f\"Found test dataset at: {test_path}\")\n",
    "\n",
    "# Get all test images\n",
    "test_image_paths = []\n",
    "if test_path:\n",
    "    image_extensions = {'.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'}\n",
    "    for ext in image_extensions:\n",
    "        test_image_paths.extend(list(test_path.rglob(f\"*{ext}\")))\n",
    "    test_image_paths = [str(p) for p in test_image_paths]\n",
    "    test_image_paths.sort()  # Sort for consistent ordering\n",
    "    \n",
    "    print(f\"Found {len(test_image_paths)} test images\")\n",
    "    \n",
    "    # Show sample paths\n",
    "    if len(test_image_paths) > 0:\n",
    "        print(f\"Sample test images: {test_image_paths[:5]}\")\n",
    "else:\n",
    "    print(\"No test images found. Please check the dataset path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_from_csv(train_dir, csv_path, label_mapping=None):\n",
    "    \"\"\"\n",
    "    Prepare dataset by loading images and labels from CSV file.\n",
    "    \n",
    "    Args:\n",
    "        train_dir: Path to directory containing train images\n",
    "        csv_path: Path to CSV file with Id,Label columns\n",
    "        label_mapping: Optional dict to map CSV labels to VENDOR_CLASSES indices\n",
    "    \n",
    "    Returns:\n",
    "        image_paths: List of image file paths\n",
    "        labels: List of label indices\n",
    "    \"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    \n",
    "    train_dir = Path(train_dir)\n",
    "    csv_path = Path(csv_path)\n",
    "    \n",
    "    if not csv_path.exists():\n",
    "        print(f\"WARNING: CSV file not found at {csv_path}\")\n",
    "        return image_paths, labels\n",
    "    \n",
    "    if not train_dir.exists():\n",
    "        print(f\"WARNING: Train directory not found at {train_dir}\")\n",
    "        return image_paths, labels\n",
    "    \n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Create label mapping if not provided\n",
    "    if label_mapping is None:\n",
    "        # Get unique labels from CSV and map them to VENDOR_CLASSES\n",
    "        unique_labels = df['Label'].str.lower().unique()\n",
    "        label_mapping = {}\n",
    "        for csv_label in unique_labels:\n",
    "            # Try to find matching vendor class (case-insensitive)\n",
    "            matched = False\n",
    "            for idx, vendor in enumerate(VENDOR_CLASSES):\n",
    "                if csv_label == vendor.lower() or csv_label in vendor.lower() or vendor.lower() in csv_label:\n",
    "                    label_mapping[csv_label.lower()] = idx\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                # Default mapping for unknown labels\n",
    "                print(f\"WARNING: Label '{csv_label}' not found in VENDOR_CLASSES, mapping to first class\")\n",
    "                label_mapping[csv_label.lower()] = 0\n",
    "    \n",
    "    # Load images and labels\n",
    "    for _, row in df.iterrows():\n",
    "        image_id = str(row['Id']).zfill(5)  # Ensure 5-digit format (00001, 00002, etc.)\n",
    "        label_str = str(row['Label']).lower()\n",
    "        \n",
    "        # Try different image extensions\n",
    "        image_found = False\n",
    "        for ext in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG']:\n",
    "            image_path = train_dir / f\"{image_id}{ext}\"\n",
    "            if image_path.exists():\n",
    "                image_paths.append(str(image_path))\n",
    "                # Map label to index\n",
    "                label_idx = label_mapping.get(label_str, 0)\n",
    "                labels.append(label_idx)\n",
    "                image_found = True\n",
    "                break\n",
    "        \n",
    "        if not image_found:\n",
    "            print(f\"WARNING: Image not found for ID {image_id}\")\n",
    "    \n",
    "    return image_paths, labels\n",
    "\n",
    "print(\"CSV dataset loading function defined!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Labels and Re-fine-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train labels from CSV for re-fine-tuning\n",
    "dataset_base = Path(\"/content/drive/MyDrive/vision/2-computer-vision-2025-b-sc-aidams-final-proj\")\n",
    "train_dir = dataset_base / \"train\"\n",
    "csv_path = dataset_base / \"train_labels.csv\"\n",
    "\n",
    "# Alternative paths to check (relative to workspace)\n",
    "alternative_train_dirs = [\n",
    "    Path(\"content/drive/MyDrive/vision/2-computer-vision-2025-b-sc-aidams-final-proj/train\"),\n",
    "    Path(\"../content/drive/MyDrive/vision/2-computer-vision-2025-b-sc-aidams-final-proj/train\"),\n",
    "    Path(\"train\"),\n",
    "]\n",
    "\n",
    "alternative_csv_paths = [\n",
    "    Path(\"content/drive/MyDrive/vision/2-computer-vision-2025-b-sc-aidams-final-proj/train_labels.csv\"),\n",
    "    Path(\"../content/drive/MyDrive/vision/2-computer-vision-2025-b-sc-aidams-final-proj/train_labels.csv\"),\n",
    "    Path(\"train_labels.csv\"),\n",
    "]\n",
    "\n",
    "# Find train directory and CSV file\n",
    "found_train_dir = None\n",
    "found_csv_path = None\n",
    "\n",
    "for path in [train_dir] + alternative_train_dirs:\n",
    "    if path.exists() and path.is_dir():\n",
    "        found_train_dir = path\n",
    "        break\n",
    "\n",
    "for path in [csv_path] + alternative_csv_paths:\n",
    "    if path.exists() and path.is_file():\n",
    "        found_csv_path = path\n",
    "        break\n",
    "\n",
    "test_labels = []\n",
    "test_label_paths = []\n",
    "\n",
    "if found_train_dir and found_csv_path:\n",
    "    print(f\"Found train directory at: {found_train_dir}\")\n",
    "    print(f\"Found CSV file at: {found_csv_path}\")\n",
    "    # Load dataset from CSV\n",
    "    test_label_paths, test_labels = prepare_dataset_from_csv(found_train_dir, found_csv_path)\n",
    "    print(f\"Found {len(test_label_paths)} labeled images for re-fine-tuning\")\n",
    "    \n",
    "    if len(test_label_paths) > 0:\n",
    "        print(f\"Label distribution: {np.bincount(test_labels)}\")\n",
    "else:\n",
    "    print(\"WARNING: Train dataset or CSV file not found. Will skip re-fine-tuning step.\")\n",
    "    if not found_train_dir:\n",
    "        print(f\"  Train directory not found. Searched in: {[str(p) for p in [train_dir] + alternative_train_dirs]}\")\n",
    "    if not found_csv_path:\n",
    "        print(f\"  CSV file not found. Searched in: {[str(p) for p in [csv_path] + alternative_csv_paths]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-fine-tune the model on test labels\n",
    "if len(test_label_paths) > 0 and len(test_labels) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Re-fine-tuning on 2nd Dataset (Test Labels)\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load the best model from first training\n",
    "    if os.path.exists('best_swin_emoji_model.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_swin_emoji_model.pt', map_location=device))\n",
    "        print(\"Loaded best model from first training\")\n",
    "    \n",
    "    # Create dataset for re-fine-tuning\n",
    "    # Use a small validation split from test labels\n",
    "    if len(test_label_paths) > 100:\n",
    "        train_test_paths, val_test_paths, train_test_labels, val_test_labels = train_test_split(\n",
    "            test_label_paths, test_labels, test_size=0.1, random_state=42, stratify=test_labels\n",
    "        )\n",
    "    else:\n",
    "        # If dataset is small, use all for training\n",
    "        train_test_paths, val_test_paths = test_label_paths, []\n",
    "        train_test_labels, val_test_labels = test_labels, []\n",
    "    \n",
    "    # Create datasets with augmentation\n",
    "    train_test_dataset = EmojiDataset(\n",
    "        train_test_paths, train_test_labels, processor,\n",
    "        transform=train_transform,\n",
    "        use_augmentation=True\n",
    "    )\n",
    "    \n",
    "    if len(val_test_paths) > 0:\n",
    "        val_test_dataset = EmojiDataset(val_test_paths, val_test_labels, processor, use_augmentation=False)\n",
    "        val_test_loader = DataLoader(\n",
    "            val_test_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=4 if torch.cuda.is_available() else 2,\n",
    "            pin_memory=torch.cuda.is_available()\n",
    "        )\n",
    "    else:\n",
    "        val_test_loader = None\n",
    "    \n",
    "    train_test_loader = DataLoader(\n",
    "        train_test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4 if torch.cuda.is_available() else 2,\n",
    "        pin_memory=torch.cuda.is_available()\n",
    "    )\n",
    "    \n",
    "    print(f\"Re-fine-tuning samples: {len(train_test_dataset)}\")\n",
    "    if val_test_loader:\n",
    "        print(f\"Validation samples: {len(val_test_dataset)}\")\n",
    "    \n",
    "    # Re-fine-tuning parameters (fewer epochs, lower learning rate)\n",
    "    refinetune_epochs = 3\n",
    "    refinetune_lr = 1e-5\n",
    "    \n",
    "    # Create new optimizer with lower learning rate\n",
    "    refinetune_optimizer = torch.optim.AdamW(\n",
    "        classification_model.parameters(),\n",
    "        lr=refinetune_lr,\n",
    "        weight_decay=0.01\n",
    "    )\n",
    "    \n",
    "    refinetune_scheduler = CosineAnnealingLR(refinetune_optimizer, T_max=refinetune_epochs)\n",
    "    \n",
    "    # Re-fine-tune\n",
    "    best_refinetune_acc = 0\n",
    "    for epoch in range(refinetune_epochs):\n",
    "        print(f\"\\nRe-fine-tuning Epoch {epoch + 1}/{refinetune_epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        train_loss, train_acc = train_epoch(\n",
    "            classification_model, train_test_loader, refinetune_optimizer, device, scaler\n",
    "        )\n",
    "        \n",
    "        if val_test_loader:\n",
    "            val_loss, val_acc = validate(classification_model, val_test_loader, device)\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "            \n",
    "            if val_acc > best_refinetune_acc:\n",
    "                best_refinetune_acc = val_acc\n",
    "                torch.save(classification_model.state_dict(), 'best_refinetuned_model.pt')\n",
    "                print(f\"Saved best re-fine-tuned model with validation accuracy: {best_refinetune_acc:.2f}%\")\n",
    "        else:\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            # Save model after each epoch if no validation set\n",
    "            torch.save(classification_model.state_dict(), 'best_refinetuned_model.pt')\n",
    "        \n",
    "        refinetune_scheduler.step()\n",
    "    \n",
    "    print(\"\\nRe-fine-tuning completed!\")\n",
    "    \n",
    "    # Load best re-fine-tuned model\n",
    "    if os.path.exists('best_refinetuned_model.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_refinetuned_model.pt', map_location=device))\n",
    "        print(\"Loaded best re-fine-tuned model\")\n",
    "else:\n",
    "    print(\"Skipping re-fine-tuning (no test labels found)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions with TTA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test images using TTA\n",
    "if len(test_image_paths) > 0:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Generating Predictions with TTA\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Ensure model is loaded\n",
    "    if os.path.exists('best_refinetuned_model.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_refinetuned_model.pt', map_location=device))\n",
    "        print(\"Using re-fine-tuned model\")\n",
    "    elif os.path.exists('best_swin_emoji_model.pt'):\n",
    "        classification_model.load_state_dict(torch.load('best_swin_emoji_model.pt', map_location=device))\n",
    "        print(\"Using original fine-tuned model\")\n",
    "    \n",
    "    classification_model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "    \n",
    "    print(f\"Processing {len(test_image_paths)} test images with TTA...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for idx, image_path in enumerate(tqdm(test_image_paths, desc=\"Generating predictions\")):\n",
    "            try:\n",
    "                # Load image\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                \n",
    "                # Predict with TTA\n",
    "                _, predicted_class, probabilities = predict_with_tta(\n",
    "                    classification_model, image, processor, tta_aug, \n",
    "                    num_augmentations=4, device=device\n",
    "                )\n",
    "                \n",
    "                # Get predicted label\n",
    "                predicted_idx = predicted_class.item()\n",
    "                predicted_label = IDX_TO_VENDOR[predicted_idx]\n",
    "                \n",
    "                # Generate 4-digit ID (0001, 0002, etc.)\n",
    "                image_id = f\"{idx + 1:04d}\"  # 4 digits with leading zeros\n",
    "                \n",
    "                predictions.append(predicted_label)\n",
    "                image_ids.append(image_id)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")\n",
    "                # Default prediction if error occurs\n",
    "                predictions.append(VENDOR_CLASSES[0])  # Default to first class\n",
    "                image_ids.append(f\"{idx + 1:04d}\")\n",
    "    \n",
    "    # Create predictions.txt file\n",
    "    predictions_file = \"predictions.txt\"\n",
    "    with open(predictions_file, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"id,Label\\n\")\n",
    "        # Write predictions\n",
    "        for img_id, pred_label in zip(image_ids, predictions):\n",
    "            f.write(f\"{img_id},{pred_label}\\n\")\n",
    "    \n",
    "    print(f\"\\nPredictions saved to {predictions_file}\")\n",
    "    print(f\"Total predictions: {len(predictions)}\")\n",
    "    print(f\"\\nSample predictions:\")\n",
    "    for i in range(min(10, len(predictions))):\n",
    "        print(f\"  {image_ids[i]}: {predictions[i]}\")\n",
    "    \n",
    "    # Show label distribution\n",
    "    from collections import Counter\n",
    "    label_counts = Counter(predictions)\n",
    "    print(f\"\\nPrediction distribution:\")\n",
    "    for label, count in sorted(label_counts.items()):\n",
    "        print(f\"  {label}: {count}\")\n",
    "else:\n",
    "    print(\"No test images found. Cannot generate predictions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
